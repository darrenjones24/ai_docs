{"Summarize this text in 50 words: \n\n # A script to crawl files from a given directory and its subdirectories.\n# It uses the `os` module to walk through the directory structure and collects file paths based on specified include and exclude patterns.\n# files that are not in the exclude patterns and are in the include patterns are collected.\nimport os\nimport typing\nimport logging\nfrom pathlib import Path\nfrom settings import DEFAULT_EXCLUDE_SUFFIXES, DEFAULT_INCLUDE_SUFFIXES, DEFAULT_EXCLUDE_DIRS\n\n\n\ndef crawl_files(\n    directory: str,\n    include_patterns: typing.Set[str] = DEFAULT_INCLUDE_SUFFIXES,\n    exclude_suffixes: typing.Set[str] = DEFAULT_EXCLUDE_SUFFIXES,\n    exclude_dirs: typing.Set[str] = DEFAULT_EXCLUDE_DIRS,\n    ) -> typing.List[str]:\n    try:\n        files = {\n            str(item.relative_to(directory))  # Store the relative path string\n            for item in Path(directory).rglob(\"*\") # Iterate through all items recursively\n            if item.is_file() # Consider only files\n            if item.suffix in include_patterns # Check 0: Suffix is included\n            and item.suffix not in exclude_suffixes # Check 1: Suffix is not excluded\n            and not any(part in exclude_dirs for part in item.relative_to(directory).parts) # Check 2: No path part is excluded\n        }\n\n    except ValueError as e:\n        logging.error(f\"Error calculating relative paths: {e}\")\n        logging.error(f\"Ensure the search directory '{directory.resolve()}' is within or is the \"\n            f\"current working directory '{directory.resolve()}'\")\n\n    logging.info(f\"Found {len(files)} files in {directory}\")\n    logging.debug(f\"Files found: {files}\")\n\n    return files\n\n\ndef main() -> None:\n    print(\"Crawling files...\")\n    directory = os.getcwd()  \n    print(f\"Files crawled from {directory}:\")\n    print (crawl_files(directory))\n\nif __name__ == \"__main__\":\n    main()": "This script crawls a directory and its subdirectories to find files. It uses `os` to traverse the directory structure and `pathlib` to construct paths. Files are collected based on include and exclude patterns for suffixes and directories, returning a list of relative file paths. A main method demonstrates use of crawl_files from the current directory.\n", "Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\n\nCode:\n```python\n# A script to crawl files from a given directory and its subdirectories.\n# It uses the `os` module to walk through the directory structure and collects file paths based on specified include and exclude patterns.\n# files that are not in the exclude patterns and are in the include patterns are collected.\nimport os\nimport typing\nimport logging\nfrom pathlib import Path\nfrom settings import DEFAULT_EXCLUDE_SUFFIXES, DEFAULT_INCLUDE_SUFFIXES, DEFAULT_EXCLUDE_DIRS\n\n\n\ndef crawl_files(\n    directory: str,\n    include_patterns: typing.Set[str] = DEFAULT_INCLUDE_SUFFIXES,\n    exclude_suffixes: typing.Set[str] = DEFAULT_EXCLUDE_SUFFIXES,\n    exclude_dirs: typing.Set[str] = DEFAULT_EXCLUDE_DIRS,\n    ) -> typing.List[str]:\n    try:\n        files = {\n            str(item.relative_to(directory))  # Store the relative path string\n            for item in Path(directory).rglob(\"*\") # Iterate through all items recursively\n            if item.is_file() # Consider only files\n            if item.suffix in include_patterns # Check 0: Suffix is included\n            and item.suffix not in exclude_suffixes # Check 1: Suffix is not excluded\n            and not any(part in exclude_dirs for part in item.relative_to(directory).parts) # Check 2: No path part is excluded\n        }\n\n    except ValueError as e:\n        logging.error(f\"Error calculating relative paths: {e}\")\n        logging.error(f\"Ensure the search directory '{directory.resolve()}' is within or is the \"\n            f\"current working directory '{directory.resolve()}'\")\n\n    logging.info(f\"Found {len(files)} files in {directory}\")\n    logging.debug(f\"Files found: {files}\")\n\n    return files\n\n\ndef main() -> None:\n    print(\"Crawling files...\")\n    directory = os.getcwd()  \n    print(f\"Files crawled from {directory}:\")\n    print (crawl_files(directory))\n\nif __name__ == \"__main__\":\n    main()\n```": "```python\n# A script to crawl files from a given directory and its subdirectories.\n# It uses the `os` module to walk through the directory structure and collects file paths based on specified include and exclude patterns.\n# files that are not in the exclude patterns and are in the include patterns are collected.\nimport os\nimport typing\nimport logging\nfrom pathlib import Path\nfrom settings import DEFAULT_EXCLUDE_SUFFIXES, DEFAULT_INCLUDE_SUFFIXES, DEFAULT_EXCLUDE_DIRS\n\n\n\ndef crawl_files(\n    directory: str,\n    include_patterns: typing.Set[str] = DEFAULT_INCLUDE_SUFFIXES,\n    exclude_suffixes: typing.Set[str] = DEFAULT_EXCLUDE_SUFFIXES,\n    exclude_dirs: typing.Set[str] = DEFAULT_EXCLUDE_DIRS,\n    ) -> typing.List[str]:\n    \"\"\"\n    Crawls files from a given directory and its subdirectories based on specified include and exclude patterns.\n\n    Args:\n        directory (str): The directory to crawl.\n        include_patterns (typing.Set[str], optional): A set of file suffixes to include. Defaults to DEFAULT_INCLUDE_SUFFIXES.\n        exclude_suffixes (typing.Set[str], optional): A set of file suffixes to exclude. Defaults to DEFAULT_EXCLUDE_SUFFIXES.\n        exclude_dirs (typing.Set[str], optional): A set of directory names to exclude. Defaults to DEFAULT_EXCLUDE_DIRS.\n\n    Returns:\n        typing.List[str]: A list of file paths relative to the given directory.\n    \"\"\"\n    try:\n        files = {\n            str(item.relative_to(directory))  # Store the relative path string\n            for item in Path(directory).rglob(\"*\") # Iterate through all items recursively\n            if item.is_file() # Consider only files\n            if item.suffix in include_patterns # Check 0: Suffix is included\n            and item.suffix not in exclude_suffixes # Check 1: Suffix is not excluded\n            and not any(part in exclude_dirs for part in item.relative_to(directory).parts) # Check 2: No path part is excluded\n        }\n\n    except ValueError as e:\n        logging.error(f\"Error calculating relative paths: {e}\")\n        logging.error(f\"Ensure the search directory '{directory.resolve()}' is within or is the \"\n            f\"current working directory '{directory.resolve()}'\")\n\n    logging.info(f\"Found {len(files)} files in {directory}\")\n    logging.debug(f\"Files found: {files}\")\n\n    return files\n\n\ndef main() -> None:\n    \"\"\"\n    Main function to crawl files from the current working directory and print the results.\n    \"\"\"\n    print(\"Crawling files...\")\n    directory = os.getcwd()  \n    print(f\"Files crawled from {directory}:\")\n    print (crawl_files(directory))\n\nif __name__ == \"__main__\":\n    main()\n```\n\n**Algorithm and Logic Flow Analysis:**\n\n1.  **`crawl_files` Function:**\n    *   **Purpose:** This function is the core of the script. It recursively crawls a directory and its subdirectories to find files that match specific criteria (include and exclude patterns for file suffixes and directories).\n    *   **Input:**\n        *   `directory` (str): The path to the directory to crawl.\n        *   `include_patterns` (typing.Set\\[str]): A set of file suffixes to include (e.g., `'.txt'`, `'.py'`).\n        *   `exclude_suffixes` (typing.Set\\[str]): A set of file suffixes to exclude (e.g., `'.log'`, `'.tmp'`).\n        *   `exclude_dirs` (typing.Set\\[str]): A set of directory names to exclude (e.g., `'venv'`, `'__pycache__'` ).\n    *   **`Path(directory).rglob(\"*\")`**: This creates a `Path` object representing the input `directory`. The `rglob(\"*\")` method recursively finds all files and subdirectories within the directory.  It returns a generator that yields `Path` objects representing each item.\n    *   **Set Comprehension**:  The heart of the function uses a set comprehension.  For each `item` (a `Path` object) found by `rglob`:\n        *   **`item.is_file()`**: It checks if the item is a file. Only files are considered.\n        *   **`item.suffix in include_patterns`**:  It checks if the file's suffix (e.g., `'.txt'`) is present in the `include_patterns` set. This implements the \"include\" filter.\n        *   **`item.suffix not in exclude_suffixes`**: It checks if the file's suffix is *not* present in the `exclude_suffixes` set. This implements the \"exclude\" filter based on suffix.\n        *   **`not any(part in exclude_dirs for part in item.relative_to(directory).parts)`**:  This is the directory exclusion logic. `item.relative_to(directory)` calculates the relative path from the input directory to the current file.  `.parts` splits the relative path into its directory components.  The `any()` function then checks if any of these directory components are present in the `exclude_dirs` set.  If any part of the path is in `exclude_dirs`, the file is excluded.\n        *   **`str(item.relative_to(directory))`**: If all the conditions above are met (the item is a file, its suffix is included, its suffix is not excluded, and none of its directory components are excluded), the relative path of the file (as a string) is added to the `files` set.  Using a set ensures that the result contains no duplicates.\n    *   **Error Handling**: A `try...except` block catches `ValueError` exceptions, which can occur if `item` is not within the `directory`. A logging message is raised if this happens.\n    *   **Logging**:  The code uses the `logging` module to log the number of files found and the list of files (at the debug level).\n    *   **Output:** Returns a `typing.List[str]` containing the relative paths (strings) of all the files that meet the inclusion/exclusion criteria.\n\n2.  **`main` Function:**\n    *   **Purpose:** This function is the entry point of the script.\n    *   It gets the current working directory using `os.getcwd()`.\n    *   It calls the `crawl_files` function with the current working directory as input.\n    *   It prints the results returned by `crawl_files`.\n\n3.  **`if __name__ == \"__main__\":` Block:**\n    *   This standard Python idiom ensures that the `main` function is called only when the script is executed directly (not when it is imported as a module).\n\n**In Summary:**\n\nThe code's main algorithm is to recursively traverse a directory, filter files based on include and exclude patterns applied to file suffixes and directory names, and return a list of the relative paths of the matching files.  The set comprehension provides a concise way to combine the directory traversal, filtering, and path transformation into a single expression. The inclusion and exclusion mechanisms give fine-grained control over the files that are collected.  Error handling is included to address potential issues with relative path calculations. Logging provides information about the process and the results.\n", "Summarize this text in 50 words: \n\n from google import genai\nfrom google.genai import types\nimport os\nimport logging\nimport json\nfrom datetime import datetime\n\n\n# Configure logging\nlog_directory = os.getenv(\"LOG_DIR\", \"logs\")\nos.makedirs(log_directory, exist_ok=True)\nlog_file = os.path.join(\n    log_directory, f\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\"\n)\n\n# Set up logger\nlogger = logging.getLogger(\"llm_logger\")\nlogger.setLevel(logging.INFO)\nlogger.propagate = False  # Prevent propagation to root logger\nfile_handler = logging.FileHandler(log_file)\nfile_handler.setFormatter(\n    logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n)\nlogger.addHandler(file_handler)\n\n# Simple cache configuration\ncache_file = \"llm_cache.json\"\n\n\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\n    # Log the prompt\n    logger.info(f\"PROMPT: {prompt}\")\n\n    # Check cache if enabled\n    if use_cache:\n        # Load cache from disk\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, \"r\") as f:\n                    cache = json.load(f)\n            except:\n                logger.warning(f\"Failed to load cache, starting with empty cache\")\n\n        # Return from cache if exists\n        if prompt in cache:\n            logger.info(f\"RESPONSE: {cache[prompt]}\")\n            return cache[prompt]\n\n    # Call the LLM if not in cache or cache disabled\n    client = genai.Client(\n        vertexai=True,\n        project=os.getenv(\"GEMINI_PROJECT_ID\", \"ai-sre-dev-84b7\"),\n        location=os.getenv(\"GEMINI_LOCATION\", \"us-central1\"),\n        http_options=types.HttpOptions(api_version='v1')\n    )\n\n    model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.0-flash-exp\")\n    # for model in client.models.list():\n    #     print(model)\n    response = client.models.generate_content(model=model, contents=[prompt])\n    response_text = response.text\n\n    # Log the response\n    logger.info(f\"RESPONSE: {response_text}\")\n\n    # Update cache if enabled\n    if use_cache:\n        # Load cache again to avoid overwrites\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, \"r\") as f:\n                    cache = json.load(f)\n            except:\n                pass\n\n        # Add to cache and save\n        cache[prompt] = response_text\n        try:\n            with open(cache_file, \"w\") as f:\n                json.dump(cache, f)\n        except Exception as e:\n            logger.error(f\"Failed to save cache: {e}\")\n\n    return response_text\n\n\nif __name__ == \"__main__\":\n    test_prompt = \"Hello, how are you?\"\n\n    # First call - should hit the API\n    print(\"Making call...\")\n    response1 = call_llm(test_prompt, use_cache=False)\n    print(f\"Response: {response1}\")\n": "This code defines a function `call_llm` that interacts with a Google LLM (Gemini) API.  It logs prompts and responses, utilizes a JSON file for caching responses to avoid redundant API calls, and includes error handling for loading/saving the cache. The main block calls the function.\n", "Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\n\nCode:\n```python\nfrom google import genai\nfrom google.genai import types\nimport os\nimport logging\nimport json\nfrom datetime import datetime\n\n\n# Configure logging\nlog_directory = os.getenv(\"LOG_DIR\", \"logs\")\nos.makedirs(log_directory, exist_ok=True)\nlog_file = os.path.join(\n    log_directory, f\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\"\n)\n\n# Set up logger\nlogger = logging.getLogger(\"llm_logger\")\nlogger.setLevel(logging.INFO)\nlogger.propagate = False  # Prevent propagation to root logger\nfile_handler = logging.FileHandler(log_file)\nfile_handler.setFormatter(\n    logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n)\nlogger.addHandler(file_handler)\n\n# Simple cache configuration\ncache_file = \"llm_cache.json\"\n\n\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\n    # Log the prompt\n    logger.info(f\"PROMPT: {prompt}\")\n\n    # Check cache if enabled\n    if use_cache:\n        # Load cache from disk\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, \"r\") as f:\n                    cache = json.load(f)\n            except:\n                logger.warning(f\"Failed to load cache, starting with empty cache\")\n\n        # Return from cache if exists\n        if prompt in cache:\n            logger.info(f\"RESPONSE: {cache[prompt]}\")\n            return cache[prompt]\n\n    # Call the LLM if not in cache or cache disabled\n    client = genai.Client(\n        vertexai=True,\n        project=os.getenv(\"GEMINI_PROJECT_ID\", \"ai-sre-dev-84b7\"),\n        location=os.getenv(\"GEMINI_LOCATION\", \"us-central1\"),\n        http_options=types.HttpOptions(api_version='v1')\n    )\n\n    model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.0-flash-exp\")\n    # for model in client.models.list():\n    #     print(model)\n    response = client.models.generate_content(model=model, contents=[prompt])\n    response_text = response.text\n\n    # Log the response\n    logger.info(f\"RESPONSE: {response_text}\")\n\n    # Update cache if enabled\n    if use_cache:\n        # Load cache again to avoid overwrites\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, \"r\") as f:\n                    cache = json.load(f)\n            except:\n                pass\n\n        # Add to cache and save\n        cache[prompt] = response_text\n        try:\n            with open(cache_file, \"w\") as f:\n                json.dump(cache, f)\n        except Exception as e:\n            logger.error(f\"Failed to save cache: {e}\")\n\n    return response_text\n\n\nif __name__ == \"__main__\":\n    test_prompt = \"Hello, how are you?\"\n\n    # First call - should hit the API\n    print(\"Making call...\")\n    response1 = call_llm(test_prompt, use_cache=False)\n    print(f\"Response: {response1}\")\n\n```": "```python\nfrom google import genai\nfrom google.genai import types\nimport os\nimport logging\nimport json\nfrom datetime import datetime\n\n\n# Configure logging\nlog_directory = os.getenv(\"LOG_DIR\", \"logs\")\nos.makedirs(log_directory, exist_ok=True)\nlog_file = os.path.join(\n    log_directory, f\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\"\n)\n\n# Set up logger\nlogger = logging.getLogger(\"llm_logger\")\nlogger.setLevel(logging.INFO)\nlogger.propagate = False  # Prevent propagation to root logger\nfile_handler = logging.FileHandler(log_file)\nfile_handler.setFormatter(\n    logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n)\nlogger.addHandler(file_handler)\n\n# Simple cache configuration\ncache_file = \"llm_cache.json\"\n\n\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\n    # Log the prompt\n    logger.info(f\"PROMPT: {prompt}\")\n\n    # Check cache if enabled\n    if use_cache:\n        # Load cache from disk\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, \"r\") as f:\n                    cache = json.load(f)\n            except:\n                logger.warning(f\"Failed to load cache, starting with empty cache\")\n\n        # Return from cache if exists\n        if prompt in cache:\n            logger.info(f\"RESPONSE: {cache[prompt]}\")\n            return cache[prompt]\n\n    # Call the LLM if not in cache or cache disabled\n    client = genai.Client(\n        vertexai=True,\n        project=os.getenv(\"GEMINI_PROJECT_ID\", \"ai-sre-dev-84b7\"),\n        location=os.getenv(\"GEMINI_LOCATION\", \"us-central1\"),\n        http_options=types.HttpOptions(api_version='v1')\n    )\n\n    model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.0-flash-exp\")\n    # for model in client.models.list():\n    #     print(model)\n    response = client.models.generate_content(model=model, contents=[prompt])\n    response_text = response.text\n\n    # Log the response\n    logger.info(f\"RESPONSE: {response_text}\")\n\n    # Update cache if enabled\n    if use_cache:\n        # Load cache again to avoid overwrites\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, \"r\") as f:\n                    cache = json.load(f)\n            except:\n                pass\n\n        # Add to cache and save\n        cache[prompt] = response_text\n        try:\n            with open(cache_file, \"w\") as f:\n                json.dump(cache, f)\n        except Exception as e:\n            logger.error(f\"Failed to save cache: {e}\")\n\n    return response_text\n\n\nif __name__ == \"__main__\":\n    test_prompt = \"Hello, how are you?\"\n\n    # First call - should hit the API\n    print(\"Making call...\")\n    response1 = call_llm(test_prompt, use_cache=False)\n    print(f\"Response: {response1}\")\n```\n\n**Overall Purpose:**\n\nThe Python code defines a function `call_llm` that interacts with a Large Language Model (LLM) from Google's `genai` library. It provides functionality for caching responses to avoid redundant API calls.  The main goal is to simplify querying the LLM while incorporating logging and a basic caching mechanism.\n\n**Detailed Analysis of Logic Flow:**\n\n1. **Initialization:**\n   - Imports necessary libraries: `google.genai`, `os`, `logging`, `json`, and `datetime`.\n   - Configures logging:\n     - Sets up a directory for logs (defaulting to \"logs\").\n     - Creates a log file named with the current date (e.g., `llm_calls_20241027.log`).\n     - Configures a logger to write messages to the log file with timestamps and severity levels.  The logger is configured to *not* propagate to the root logger, meaning it operates independently.\n   - Defines the cache file name: `llm_cache.json`.\n\n2. **`call_llm(prompt: str, use_cache: bool = True) -> str` Function:**\n   - **Input:**\n     - `prompt` (str): The text prompt to send to the LLM.\n     - `use_cache` (bool):  A flag indicating whether to use the cache (defaults to `True`).\n   - **Output:**\n     - (str): The LLM's response to the prompt.\n   - **Logic:**\n     - **Logging:** Logs the incoming `prompt` to the configured logger.\n     - **Cache Check (if `use_cache` is True):**\n       - Loads the cache from `llm_cache.json`. If the file doesn't exist or if loading fails, an empty dictionary is used.  A warning is logged if the cache fails to load.\n       - Checks if the `prompt` already exists as a key in the `cache` dictionary.\n       - If the prompt is found in the cache, the corresponding cached response is logged and returned directly, avoiding an LLM call.\n     - **LLM Call (if not in cache or `use_cache` is False):**\n       - Initializes the `genai` client using environment variables for project ID and location. It uses Vertex AI as the backend.  It configures the `api_version` to 'v1' through `HttpOptions`.\n       - Sets the `model` name using an environment variable.\n       - Calls `client.models.generate_content(model=model, contents=[prompt])` to send the prompt to the LLM.\n       - Extracts the response text from the LLM's response.\n     - **Logging:** Logs the LLM's response to the logger.\n     - **Cache Update (if `use_cache` is True):**\n       - Reloads the cache from disk *again*. This is done to prevent overwriting any changes made to the cache by other processes since the initial load.\n       - Adds the `prompt` and the LLM's `response_text` to the `cache` dictionary.\n       - Saves the updated `cache` to `llm_cache.json`. If saving fails, an error is logged.\n     - **Return:** Returns the LLM's `response_text`.\n\n3. **`if __name__ == \"__main__\":` Block:**\n   - This block is executed only when the script is run directly (not when imported as a module).\n   - Defines a `test_prompt`.\n   - Calls the `call_llm` function with `use_cache=False` to force an API call to the LLM.\n   - Prints the returned response.\n\n**Control Structures:**\n\n- **`if use_cache:`:** Conditional statement to enable or disable the cache functionality.\n- **`if os.path.exists(cache_file):`:**  Conditional statement to check if the cache file exists before attempting to load it.\n- **`try...except:`:** Error handling blocks to catch exceptions during cache loading and saving.\n\n**Data Transformations:**\n\n- `datetime.now().strftime('%Y%m%d')`: Converts the current date and time into a string format (YYYYMMDD) for use in the log file name.\n- `json.load(f)`: Parses the JSON content of the cache file into a Python dictionary.\n- `json.dump(cache, f)`: Serializes the Python dictionary `cache` into JSON format and writes it to the cache file.\n\n**In summary, the code defines a `call_llm` function that intelligently queries a Large Language Model. It uses a JSON file as a simple cache to store and retrieve LLM responses based on the input prompt, minimizing redundant API calls.  It also includes comprehensive logging of prompts and responses for debugging and auditing purposes.**\n", "Summarize this text in 50 words: \n\n import logging\nfrom pathlib import Path\nfrom pocketflow import Node\nfrom call_llm import call_llm\nfrom settings import (\n    DEFAULT_EXCLUDE_SUFFIXES,\n    DEFAULT_INCLUDE_SUFFIXES,\n    DEFAULT_EXCLUDE_DIRS,\n)\nfrom settings import parse_args\nimport fetch_files\n\n\nclass Summarise(Node):\n    \"\"\"\n    This class identifies abstractions in the code.\n    \"\"\"\n\n    def prep(self,shared):\n        return file_contents[\"data\"]\n    \n    def exec(self, text):\n        prompt = f\"Summarize this text in 50 words: \\n\\n {text}\"\n        return call_llm(prompt)\n    \n    def post(self, shared, prep_res, exec_res):\n        # Store the result in the shared settings\n        file_contents[\"summaries\"] = exec_res\n        return \"default\"\n\n\n# New Node for Logic Flow Analysis\nclass AnalyzeLogicFlow(Node):\n    \"\"\"\n    This class analyzes the code to describe its main algorithm or logic flow.\n    \"\"\"\n\n    # prep and post might not be needed if main loop handles orchestration\n    # def prep(self, shared):\n    #     pass\n\n    def exec(self, text):\n        prompt = f\"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\\n\\nCode:\\n```python\\n{text}\\n```\"\n        # Assuming call_llm handles potential errors and returns a string\n        return call_llm(prompt)\n\n    # def post(self, shared, prep_res, exec_res):\n    #     pass\n\n\ndef main() -> None:\n    args = parse_args()\n\n    # Create a dictionary of settings to be used in the script\n    shared_settings = {\n        \"name\": args.name,\n        \"directory\": args.directory, # Target directory to analyze\n        \"exclude_patterns\": DEFAULT_EXCLUDE_SUFFIXES,\n        \"exclude_dirs\": DEFAULT_EXCLUDE_DIRS,\n        \"include_suffixes\": DEFAULT_INCLUDE_SUFFIXES,\n        \"additional_exclude_dirs\": args.exclude_dirs,\n    }\n    logging.debug(f\"Shared settings: {shared_settings}\")\n\n    # Get a list of files from the target directory\n    # Ensure paths are relative to the target directory for correct output structure\n    target_dir = Path(shared_settings[\"directory\"]).resolve()\n    logging.info(f\"Analyzing directory: {target_dir}\")\n\n    # fetch_files.crawl_files already returns relative paths if called correctly\n    # Let's ensure the base directory passed to crawl_files is the target directory\n    files_to_process = fetch_files.crawl_files(\n        directory=str(target_dir), # Pass absolute path to crawl\n        include_patterns=shared_settings[\"include_suffixes\"],\n        exclude_suffixes=shared_settings[\"exclude_patterns\"],\n        exclude_dirs=shared_settings[\"exclude_dirs\"].union(\n            shared_settings[\"additional_exclude_dirs\"]\n        ),\n    )\n    \n    # Instantiate nodes\n    summariser = Summarise()\n    logic_analyzer = AnalyzeLogicFlow()\n\n    results = {}\n    logging.info(f\"Processing {len(files_to_process)} files...\")\n\n    # Process each file\n    for relative_file_path in files_to_process:\n        absolute_file_path = target_dir / relative_file_path\n        logging.debug(f\"Processing file: {absolute_file_path}\")\n        print(f\"Processing: {relative_file_path}\") # User feedback\n        try:\n            with open(absolute_file_path, \"r\", encoding='utf-8') as f:\n                content = f.read()\n                if content.strip(): # Ensure content is not empty\n                    # Call LLM for summary\n                    summary = summariser.exec(content)\n                    # Call LLM for logic flow\n                    logic_flow = logic_analyzer.exec(content)\n                    results[relative_file_path] = {\"summary\": summary, \"logic_flow\": logic_flow}\n                else:\n                    logging.warning(f\"Skipped empty file: {absolute_file_path}\")\n                    print(f\"Skipped empty file: {relative_file_path}\")\n        except Exception as e:\n            logging.error(f\"Error processing file {absolute_file_path}: {e}\")\n            print(f\"Error processing file {relative_file_path}: {e}\")\n\n    # Generate Markdown output\n    output_base_dir = target_dir / \"docs\"\n    logging.info(f\"Generating documentation in: {output_base_dir}\")\n    try:\n        output_base_dir.mkdir(parents=True, exist_ok=True)\n    except Exception as e:\n        logging.error(f\"Failed to create output directory {output_base_dir}: {e}\")\n        print(f\"Error: Could not create output directory {output_base_dir}. Aborting.\")\n        return # Exit if we can't create the output dir\n\n    for relative_file_path, data in results.items():\n        md_filename = Path(relative_file_path).with_suffix(\".md\")\n        md_path = output_base_dir / md_filename\n        \n        try:\n            # Ensure subdirectory exists\n            md_path.parent.mkdir(parents=True, exist_ok=True)\n\n            # Format Markdown content\n            md_content = f\"\"\"# File: {relative_file_path}\n\n## Summary\n\n{data['summary']}\n\n## Logic Flow\n\n{data['logic_flow']}\n\"\"\"\n            # Write the Markdown file\n            with open(md_path, \"w\", encoding='utf-8') as f:\n                f.write(md_content)\n            logging.info(f\"Generated: {md_path}\")\n            print(f\"Generated: {md_path.relative_to(target_dir)}\") # Show relative path to user\n\n        except Exception as e:\n            logging.error(f\"Failed to write Markdown file {md_path}: {e}\")\n            print(f\"Error writing file {md_path.relative_to(target_dir)}: {e}\")\n\n    print(f\"\\nDocumentation generation complete. Files written to: {output_base_dir}\")\n\n\nif __name__ == \"__main__\":\n    main()\n": "This Python script analyzes code in a directory, generating summaries and logic flow descriptions for each file using a large language model (LLM). It uses `Summarise` and `AnalyzeLogicFlow` nodes, crawls files, processes them, and outputs results in Markdown format to a \"docs\" subdirectory within the target directory.\n", "Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\n\nCode:\n```python\nimport logging\nfrom pathlib import Path\nfrom pocketflow import Node\nfrom call_llm import call_llm\nfrom settings import (\n    DEFAULT_EXCLUDE_SUFFIXES,\n    DEFAULT_INCLUDE_SUFFIXES,\n    DEFAULT_EXCLUDE_DIRS,\n)\nfrom settings import parse_args\nimport fetch_files\n\n\nclass Summarise(Node):\n    \"\"\"\n    This class identifies abstractions in the code.\n    \"\"\"\n\n    def prep(self,shared):\n        return file_contents[\"data\"]\n    \n    def exec(self, text):\n        prompt = f\"Summarize this text in 50 words: \\n\\n {text}\"\n        return call_llm(prompt)\n    \n    def post(self, shared, prep_res, exec_res):\n        # Store the result in the shared settings\n        file_contents[\"summaries\"] = exec_res\n        return \"default\"\n\n\n# New Node for Logic Flow Analysis\nclass AnalyzeLogicFlow(Node):\n    \"\"\"\n    This class analyzes the code to describe its main algorithm or logic flow.\n    \"\"\"\n\n    # prep and post might not be needed if main loop handles orchestration\n    # def prep(self, shared):\n    #     pass\n\n    def exec(self, text):\n        prompt = f\"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\\n\\nCode:\\n```python\\n{text}\\n```\"\n        # Assuming call_llm handles potential errors and returns a string\n        return call_llm(prompt)\n\n    # def post(self, shared, prep_res, exec_res):\n    #     pass\n\n\ndef main() -> None:\n    args = parse_args()\n\n    # Create a dictionary of settings to be used in the script\n    shared_settings = {\n        \"name\": args.name,\n        \"directory\": args.directory, # Target directory to analyze\n        \"exclude_patterns\": DEFAULT_EXCLUDE_SUFFIXES,\n        \"exclude_dirs\": DEFAULT_EXCLUDE_DIRS,\n        \"include_suffixes\": DEFAULT_INCLUDE_SUFFIXES,\n        \"additional_exclude_dirs\": args.exclude_dirs,\n    }\n    logging.debug(f\"Shared settings: {shared_settings}\")\n\n    # Get a list of files from the target directory\n    # Ensure paths are relative to the target directory for correct output structure\n    target_dir = Path(shared_settings[\"directory\"]).resolve()\n    logging.info(f\"Analyzing directory: {target_dir}\")\n\n    # fetch_files.crawl_files already returns relative paths if called correctly\n    # Let's ensure the base directory passed to crawl_files is the target directory\n    files_to_process = fetch_files.crawl_files(\n        directory=str(target_dir), # Pass absolute path to crawl\n        include_patterns=shared_settings[\"include_suffixes\"],\n        exclude_suffixes=shared_settings[\"exclude_patterns\"],\n        exclude_dirs=shared_settings[\"exclude_dirs\"].union(\n            shared_settings[\"additional_exclude_dirs\"]\n        ),\n    )\n    \n    # Instantiate nodes\n    summariser = Summarise()\n    logic_analyzer = AnalyzeLogicFlow()\n\n    results = {}\n    logging.info(f\"Processing {len(files_to_process)} files...\")\n\n    # Process each file\n    for relative_file_path in files_to_process:\n        absolute_file_path = target_dir / relative_file_path\n        logging.debug(f\"Processing file: {absolute_file_path}\")\n        print(f\"Processing: {relative_file_path}\") # User feedback\n        try:\n            with open(absolute_file_path, \"r\", encoding='utf-8') as f:\n                content = f.read()\n                if content.strip(): # Ensure content is not empty\n                    # Call LLM for summary\n                    summary = summariser.exec(content)\n                    # Call LLM for logic flow\n                    logic_flow = logic_analyzer.exec(content)\n                    results[relative_file_path] = {\"summary\": summary, \"logic_flow\": logic_flow}\n                else:\n                    logging.warning(f\"Skipped empty file: {absolute_file_path}\")\n                    print(f\"Skipped empty file: {relative_file_path}\")\n        except Exception as e:\n            logging.error(f\"Error processing file {absolute_file_path}: {e}\")\n            print(f\"Error processing file {relative_file_path}: {e}\")\n\n    # Generate Markdown output\n    output_base_dir = target_dir / \"docs\"\n    logging.info(f\"Generating documentation in: {output_base_dir}\")\n    try:\n        output_base_dir.mkdir(parents=True, exist_ok=True)\n    except Exception as e:\n        logging.error(f\"Failed to create output directory {output_base_dir}: {e}\")\n        print(f\"Error: Could not create output directory {output_base_dir}. Aborting.\")\n        return # Exit if we can't create the output dir\n\n    for relative_file_path, data in results.items():\n        md_filename = Path(relative_file_path).with_suffix(\".md\")\n        md_path = output_base_dir / md_filename\n        \n        try:\n            # Ensure subdirectory exists\n            md_path.parent.mkdir(parents=True, exist_ok=True)\n\n            # Format Markdown content\n            md_content = f\"\"\"# File: {relative_file_path}\n\n## Summary\n\n{data['summary']}\n\n## Logic Flow\n\n{data['logic_flow']}\n\"\"\"\n            # Write the Markdown file\n            with open(md_path, \"w\", encoding='utf-8') as f:\n                f.write(md_content)\n            logging.info(f\"Generated: {md_path}\")\n            print(f\"Generated: {md_path.relative_to(target_dir)}\") # Show relative path to user\n\n        except Exception as e:\n            logging.error(f\"Failed to write Markdown file {md_path}: {e}\")\n            print(f\"Error writing file {md_path.relative_to(target_dir)}: {e}\")\n\n    print(f\"\\nDocumentation generation complete. Files written to: {output_base_dir}\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n```": "Okay, let's break down the Python code's algorithm and logic flow.\n\n**Purpose:**\n\nThe code aims to automatically generate documentation for Python code files in a given directory. It uses a Large Language Model (LLM) to create summaries and analyze the logic flow of each file.  The results are then saved as Markdown files within a `docs` subdirectory of the original directory.\n\n**Algorithm/Logic Flow:**\n\n1.  **Initialization and Argument Parsing:**\n    *   The `main()` function starts by parsing command-line arguments using `parse_args()`.  The expected arguments probably include the target directory (`--directory`) and potentially exclude directories (`--exclude_dirs`).\n    *   Default settings are defined from the `settings` module:\n        *   `DEFAULT_EXCLUDE_SUFFIXES`:  File extensions to exclude (e.g., `.pyc`, `.log`).\n        *   `DEFAULT_INCLUDE_SUFFIXES`: File extensions to include (e.g., `.py`).\n        *   `DEFAULT_EXCLUDE_DIRS`: Directories to exclude (e.g., `.git`, `__pycache__`).\n    *   A `shared_settings` dictionary is created, combining the command-line arguments and the default values. This dictionary configures the file processing.\n\n2.  **File Discovery:**\n    *   The code resolves the target directory path using `Path(shared_settings[\"directory\"]).resolve()`.\n    *   The `fetch_files.crawl_files()` function is called to generate a list of Python files to process.  This function recursively crawls the target directory, filtering files based on the include/exclude patterns defined in `shared_settings`. Crucially, `crawl_files` is expected to return *relative* paths of files, *relative to the target directory*.\n    * The code ensures that it passes the absolute path of the target directory to `crawl_files`.\n3.  **Node Instantiation:**\n    *   Two node objects `Summarise` and `AnalyzeLogicFlow` are created. These nodes leverage the `pocketflow` framework and encapsulate the LLM call logic.\n\n4.  **File Processing Loop:**\n    *   The code iterates through each file found by `crawl_files()`. The loop performs the core analysis for each file.\n    *   Inside the loop:\n        *   It constructs the absolute file path by combining the `target_dir` and the relative file path from `crawl_files()`.\n        *   It opens the file in read mode (`\"r\"`) with UTF-8 encoding to handle different character sets.\n        *   It reads the file content into the `content` variable.\n        *   A check `if content.strip()` ensures the file is not empty.\n        *   If the file contains content:\n            *   It calls the `exec` method of `summariser` object, passing in the `content` to generate a summary of the file.\n            *   It calls the `exec` method of `logic_analyzer` object, passing in the `content` to generate an analysis of the logic flow of the file.\n            *   The returned summary and logic flow are stored in the `results` dictionary using the relative path as the key.\n        *   If the file is empty, a warning message is logged, and the file is skipped.\n        *   A `try...except` block handles potential errors during file processing (e.g., file not found, permission errors).\n\n5.  **Markdown Output Generation:**\n    *   After processing all files, the code creates the output directory ( `target_dir / \"docs\"` ) if it doesn't exist.\n    *   It iterates through the `results` dictionary, which contains the summaries and logic flows for each file.\n    *   For each file:\n        *   It constructs the output Markdown file path in the \"docs\" subdirectory. The filename is derived from the original Python file's name, but with a `.md` extension.\n        *   It creates a Markdown string (`md_content`) with a header containing the filename, followed by sections for the summary and logic flow, which are populated from the `results` dictionary.\n        *   It writes the Markdown content to the output file using UTF-8 encoding.\n        *   A `try...except` block handles potential errors during file writing.\n        * The code ensures that the parent directories of the markdown file exist by calling `md_path.parent.mkdir(parents=True, exist_ok=True)`\n\n6.  **Completion Message:**\n    *   Finally, the code prints a message indicating that the documentation generation is complete and shows the location of the generated files.\n\n**Control Structures:**\n\n*   **`for` loop:** Iterates through the list of files to process.\n*   **`try...except` blocks:** Handle potential exceptions during file processing and markdown generation.\n*   **`if` statement:** Checks if the file content is empty before processing.\n\n**Function Calls:**\n\n*   `parse_args()`: Parses command-line arguments.\n*   `fetch_files.crawl_files()`:  Recursively finds files in a directory based on include/exclude rules.\n*   `open()`: Opens files for reading and writing.\n*   `file.read()`: Reads the content of a file.\n*   `Summarise.exec()`: Calls the LLM to generate a summary of the code.\n*   `AnalyzeLogicFlow.exec()`: Calls the LLM to analyze the logic flow of the code.\n*   `Path.mkdir()`: Creates a directory (and parent directories if necessary).\n*   `file.write()`: Writes content to a file.\n*   `logging.info()`, `logging.debug()`, `logging.error()`:  Logs informational, debugging, and error messages.\n\n**Data Transformations:**\n\n*   Command-line arguments are parsed and stored in the `shared_settings` dictionary.\n*   File paths are manipulated using the `Path` object.\n*   The `crawl_files()` function transforms a directory and include/exclude patterns into a list of file paths.\n*   The LLM functions (`Summarise.exec()`, `AnalyzeLogicFlow.exec()`) transform code content into summaries and logic flow analyses (strings).\n*   The output is formatted as Markdown.\n\n**Key Classes and Nodes:**\n\n*   `Summarise`: A `pocketflow` Node responsible for summarizing code using an LLM.  The `exec` method defines the LLM prompt.\n*   `AnalyzeLogicFlow`: A `pocketflow` Node responsible for analyzing the logic flow of code using an LLM.  The `exec` method defines the LLM prompt.\n*   `Node` (from `pocketflow`):  Likely a base class for defining processing nodes in a pipeline. The `prep`, `exec`, and `post` methods suggest a process-execute-postprocess workflow.\n\n**Assumptions:**\n\n*   The `call_llm()` function is responsible for interacting with the LLM, handling API requests, and potentially error handling.  It's expected to return a string containing the LLM's response.\n*   The `pocketflow` library provides the `Node` base class and handles the execution and coordination of the nodes. The prep, exec, and post methods provide the entry points to the nodes' functionality.\n*   The `fetch_files.crawl_files()` function is assumed to return relative file paths with respect to the target directory.\n* The logging level is assumed to be configured elsewhere.\n\n**Improvements/Observations:**\n\n*   **Error Handling:** The `try...except` blocks are good, but could be more specific (e.g., catching `FileNotFoundError` separately).\n*   **LLM Interaction:** The `call_llm()` function's error handling and retry logic are hidden.  Ideally, this would be exposed or handled more explicitly.\n*   **Output Formatting:** The Markdown output format is basic.  It could be made more configurable or provide options for different documentation styles.\n*   **Modularity:**  The code is fairly modular, with the `Node` classes separating the LLM interaction logic.  However, the Markdown generation could be extracted into a separate function or class.\n*   **Shared State:**  The use of `shared_settings` is a good practice for passing configuration data.\n* The nodes don't implement the prep and post functions, which suggests that they are not used to share state.\n", "Summarize this text in 50 words: \n\n import typing\nimport argparse\nimport os\n\n\nDEFAULT_INCLUDE_SUFFIXES = {\n    \".js\",\n    \".json\",\n    \".jsx\",\n    \".md\",\n    \".py\",\n    \".pyi\",\n    \".pyx\",\n    \".rst\",\n    \".tf\",\n    \".tfvars\",\n    \".ts\",\n    \".tsx\",\n    \".yaml\",\n    \".yml\",\n    \"Dockerfile\",\n    \"Jenkinsfil\",\n}\n\nDEFAULT_EXCLUDE_SUFFIXES = {}\n\nDEFAULT_EXCLUDE_DIRS = {\n    \".git\",\n    \".github\",\n    \".terraform\",\n    \"assets\",\n    \"build\",\n    \"dist\",\n    \"env\",\n    \"docs\",\n    \"env\",\n    \"images\",\n    \"temp\",\n    \"tests\",\n    \"tmp\",\n    \"venv\",\n}\n\n\ndef parse_args() -> typing.Any:\n    \"\"\"\n    Parse command line arguments.\n    :return: Parsed arguments\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Generate documentation for a project using AI.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    parser.add_argument(\n        \"-n\",\n        \"--name\",\n        help=\"local Repository name\",\n        default=\"repo_name\",\n        )\n\n    parser.add_argument(\n        \"-d\",\n        \"--directory\",\n        help=\"top level directory to scan, default is current working directory\",\n        default=os.getcwd(),\n        )\n\n    parser.add_argument(\n        \"-e\",\n        \"--exclude_dirs\",\n        nargs='*',\n        help=\"comma separated list of directories to exclude\",\n        default=\",\"\n        )\n\n    return parser.parse_args()\n\nmodel = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\nproject=os.getenv(\"GEMINI_PROJECT_ID\", \"sre-ai-dev\")": "This code defines default file suffixes and directories to include/exclude when generating documentation. It uses `argparse` to handle command-line arguments for specifying the repository name, target directory, and directories to exclude. It also retrieves the Gemini model and project ID from environment variables.\n", "Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\n\nCode:\n```python\nimport typing\nimport argparse\nimport os\n\n\nDEFAULT_INCLUDE_SUFFIXES = {\n    \".js\",\n    \".json\",\n    \".jsx\",\n    \".md\",\n    \".py\",\n    \".pyi\",\n    \".pyx\",\n    \".rst\",\n    \".tf\",\n    \".tfvars\",\n    \".ts\",\n    \".tsx\",\n    \".yaml\",\n    \".yml\",\n    \"Dockerfile\",\n    \"Jenkinsfil\",\n}\n\nDEFAULT_EXCLUDE_SUFFIXES = {}\n\nDEFAULT_EXCLUDE_DIRS = {\n    \".git\",\n    \".github\",\n    \".terraform\",\n    \"assets\",\n    \"build\",\n    \"dist\",\n    \"env\",\n    \"docs\",\n    \"env\",\n    \"images\",\n    \"temp\",\n    \"tests\",\n    \"tmp\",\n    \"venv\",\n}\n\n\ndef parse_args() -> typing.Any:\n    \"\"\"\n    Parse command line arguments.\n    :return: Parsed arguments\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Generate documentation for a project using AI.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    parser.add_argument(\n        \"-n\",\n        \"--name\",\n        help=\"local Repository name\",\n        default=\"repo_name\",\n        )\n\n    parser.add_argument(\n        \"-d\",\n        \"--directory\",\n        help=\"top level directory to scan, default is current working directory\",\n        default=os.getcwd(),\n        )\n\n    parser.add_argument(\n        \"-e\",\n        \"--exclude_dirs\",\n        nargs='*',\n        help=\"comma separated list of directories to exclude\",\n        default=\",\"\n        )\n\n    return parser.parse_args()\n\nmodel = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\nproject=os.getenv(\"GEMINI_PROJECT_ID\", \"sre-ai-dev\")\n```": "```python\nimport typing\nimport argparse\nimport os\n\n\nDEFAULT_INCLUDE_SUFFIXES = {\n    \".js\",\n    \".json\",\n    \".jsx\",\n    \".md\",\n    \".py\",\n    \".pyi\",\n    \".pyx\",\n    \".rst\",\n    \".tf\",\n    \".tfvars\",\n    \".ts\",\n    \".tsx\",\n    \".yaml\",\n    \".yml\",\n    \"Dockerfile\",\n    \"Jenkinsfil\",\n}\n\nDEFAULT_EXCLUDE_SUFFIXES = {}\n\nDEFAULT_EXCLUDE_DIRS = {\n    \".git\",\n    \".github\",\n    \".terraform\",\n    \"assets\",\n    \"build\",\n    \"dist\",\n    \"env\",\n    \"docs\",\n    \"env\",\n    \"images\",\n    \"temp\",\n    \"tests\",\n    \"tmp\",\n    \"venv\",\n}\n\n\ndef parse_args() -> typing.Any:\n    \"\"\"\n    Parse command line arguments.\n    :return: Parsed arguments\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Generate documentation for a project using AI.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    parser.add_argument(\n        \"-n\",\n        \"--name\",\n        help=\"local Repository name\",\n        default=\"repo_name\",\n        )\n\n    parser.add_argument(\n        \"-d\",\n        \"--directory\",\n        help=\"top level directory to scan, default is current working directory\",\n        default=os.getcwd(),\n        )\n\n    parser.add_argument(\n        \"-e\",\n        \"--exclude_dirs\",\n        nargs='*',\n        help=\"comma separated list of directories to exclude\",\n        default=\",\"\n        )\n\n    return parser.parse_args()\n\nmodel = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\nproject=os.getenv(\"GEMINI_PROJECT_ID\", \"sre-ai-dev\")\n```\n\n**Analysis of the Python Code:**\n\n**1. Imports:**\n\n*   `typing`: Used for type hinting, improving code readability and maintainability.\n*   `argparse`:  A standard library module used for parsing command-line arguments. It allows the program to receive input from the user when executed from the command line.\n*   `os`: Provides a way of using operating system-dependent functionality, such as getting the current working directory.\n\n**2. Default Configuration Sets:**\n\n*   `DEFAULT_INCLUDE_SUFFIXES`: A set containing file extensions that the program will consider for processing (e.g., \".js\", \".py\", \".md\").  This suggests the code will process files of these types.\n*   `DEFAULT_EXCLUDE_SUFFIXES`: An empty set, presumably designed to hold file extensions that should be excluded from processing.  Currently, it excludes none.\n*   `DEFAULT_EXCLUDE_DIRS`: A set of directory names that should be excluded from the scanning process (e.g., \".git\", \"venv\", \"tests\"). This is useful for ignoring irrelevant directories.\n\n**3. `parse_args()` Function:**\n\n*   **Purpose:** This function defines how the program accepts and processes command-line arguments.\n*   **`argparse.ArgumentParser`:** Creates an argument parser object. The `description` provides a brief explanation of the program's purpose, and `formatter_class` improves the help message format.\n*   **`parser.add_argument()`:**  This method is called multiple times to define the different command-line arguments that the program accepts.\n    *   `-n`, `--name`:  Defines an argument for the repository name. It has a default value of \"repo\\_name\".\n    *   `-d`, `--directory`: Defines an argument for the top-level directory to scan. Its default value is the current working directory obtained using `os.getcwd()`.\n    *   `-e`, `--exclude_dirs`:  Defines an argument for a list of directories to exclude. It accepts multiple values (`nargs='*'`) and has a default value of a single comma \",\". This means that if the user doesn't specify any excluded directories, the default value is a single comma, which likely requires further processing to handle correctly (e.g., splitting it into an empty list).\n*   **`parser.parse_args()`:**  Parses the command-line arguments and returns an object containing the values provided by the user (or the default values if no arguments are provided).\n*   **Return Value:**  The function returns an `argparse.Namespace` object (aliased as `typing.Any` for generic type hinting). This object contains the parsed arguments as attributes.\n\n**4. Environment Variables:**\n\n*   `model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")`: Reads the value of the environment variable `GEMINI_MODEL`. If the environment variable is not set, it defaults to \"gemini-2.5-pro-exp-03-25\".  This likely specifies an AI model to be used.\n*   `project=os.getenv(\"GEMINI_PROJECT_ID\", \"sre-ai-dev\")`: Reads the value of the environment variable `GEMINI_PROJECT_ID`. If the environment variable is not set, it defaults to \"sre-ai-dev\". This probably specifies the Google Cloud project ID for the AI model.\n\n**Overall Purpose and Logic Flow:**\n\nThis code snippet sets up the basic infrastructure for a program that generates documentation for a project using AI.  The core logic is as follows:\n\n1.  **Configuration:** Defines default settings for file suffixes to include, file suffixes to exclude, and directories to exclude.\n2.  **Argument Parsing:** Uses `argparse` to handle command-line arguments, allowing the user to specify:\n    *   The repository name.\n    *   The directory to scan.\n    *   A list of directories to exclude.\n3.  **Environment Variable Handling:** Reads the `GEMINI_MODEL` and `GEMINI_PROJECT_ID` environment variables (or uses default values if they are not set). These variables appear to be related to the AI model used for generating the documentation.\n\nThe next steps (not shown in the code snippet) would likely involve:\n\n1.  Retrieving the command-line arguments from the `parse_args()` function.\n2.  Walking the specified directory, excluding directories specified via command-line or the default exclude list.\n3.  Filtering files based on the included and excluded suffixes.\n4.  Processing the remaining files using the specified AI model (from the environment variable), presumably to generate documentation.\n5.  Outputting the generated documentation.\n", "Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Do not include the code in the output. Explain the purpose of the code in terms of its logic.\n\nCode:\n```python\nimport typing\nimport argparse\nimport os\n\n\nDEFAULT_INCLUDE_SUFFIXES = {\n    \".js\",\n    \".json\",\n    \".jsx\",\n    \".md\",\n    \".py\",\n    \".pyi\",\n    \".pyx\",\n    \".rst\",\n    \".tf\",\n    \".tfvars\",\n    \".ts\",\n    \".tsx\",\n    \".yaml\",\n    \".yml\",\n    \"Dockerfile\",\n    \"Jenkinsfil\",\n}\n\nDEFAULT_EXCLUDE_SUFFIXES = {}\n\nDEFAULT_EXCLUDE_DIRS = {\n    \".git\",\n    \".github\",\n    \".terraform\",\n    \"assets\",\n    \"build\",\n    \"dist\",\n    \"env\",\n    \"docs\",\n    \"env\",\n    \"images\",\n    \"temp\",\n    \"tests\",\n    \"tmp\",\n    \"venv\",\n}\n\n\ndef parse_args() -> typing.Any:\n    \"\"\"\n    Parse command line arguments.\n    :return: Parsed arguments\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Generate documentation for a project using AI.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    parser.add_argument(\n        \"-n\",\n        \"--name\",\n        help=\"local Repository name\",\n        default=\"repo_name\",\n        )\n\n    parser.add_argument(\n        \"-d\",\n        \"--directory\",\n        help=\"top level directory to scan, default is current working directory\",\n        default=os.getcwd(),\n        )\n\n    parser.add_argument(\n        \"-e\",\n        \"--exclude_dirs\",\n        nargs='*',\n        help=\"comma separated list of directories to exclude\",\n        default=\",\"\n        )\n\n    return parser.parse_args()\n\nmodel = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\nproject=os.getenv(\"GEMINI_PROJECT_ID\", \"sre-ai-dev\")\n```": "The Python code begins by importing necessary modules like `typing`, `argparse`, and `os`. It then defines three sets as constants: `DEFAULT_INCLUDE_SUFFIXES`, `DEFAULT_EXCLUDE_SUFFIXES`, and `DEFAULT_EXCLUDE_DIRS`, containing file extensions to include, file extensions to exclude, and directory names to exclude, respectively.\n\nThe core logic resides in the `parse_args` function. This function uses the `argparse` module to define and parse command-line arguments. It defines three arguments: `-n` or `--name` for the repository name (defaulting to \"repo_name\"), `-d` or `--directory` for the top-level directory to scan (defaulting to the current working directory), and `-e` or `--exclude_dirs` for a comma-separated list of directories to exclude (defaulting to an empty string).  The function returns the parsed arguments object.\n\nFinally, the code retrieves environment variables. It attempts to retrieve the value of `GEMINI_MODEL` from the environment, defaulting to \"gemini-2.5-pro-exp-03-25\" if the environment variable is not set.  Similarly, it attempts to retrieve the value of `GEMINI_PROJECT_ID` from the environment, defaulting to \"sre-ai-dev\" if the environment variable is not set.\n\nIn essence, this code sets up the configuration for a documentation generation tool. It defines default settings for file inclusion and exclusion, allows the user to override these settings via command-line arguments, and fetches necessary configuration values from environment variables. The purpose of the code is to prepare and collect the configuration data required for the rest of the documentation generation process.\n", "Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Do not include the code in the output. Explain the purpose of the code in terms of its logic.\n\nCode:\n```python\nfrom google import genai\nfrom google.genai import types\nimport os\nimport logging\nimport json\nfrom datetime import datetime\n\n\n# Configure logging\nlog_directory = os.getenv(\"LOG_DIR\", \"logs\")\nos.makedirs(log_directory, exist_ok=True)\nlog_file = os.path.join(\n    log_directory, f\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\"\n)\n\n# Set up logger\nlogger = logging.getLogger(\"llm_logger\")\nlogger.setLevel(logging.INFO)\nlogger.propagate = False  # Prevent propagation to root logger\nfile_handler = logging.FileHandler(log_file)\nfile_handler.setFormatter(\n    logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n)\nlogger.addHandler(file_handler)\n\n# Simple cache configuration\ncache_file = \"llm_cache.json\"\n\n\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\n    # Log the prompt\n    logger.info(f\"PROMPT: {prompt}\")\n\n    # Check cache if enabled\n    if use_cache:\n        # Load cache from disk\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, \"r\") as f:\n                    cache = json.load(f)\n            except:\n                logger.warning(f\"Failed to load cache, starting with empty cache\")\n\n        # Return from cache if exists\n        if prompt in cache:\n            logger.info(f\"RESPONSE: {cache[prompt]}\")\n            return cache[prompt]\n\n    # Call the LLM if not in cache or cache disabled\n    client = genai.Client(\n        vertexai=True,\n        project=os.getenv(\"GEMINI_PROJECT_ID\", \"ai-sre-dev-84b7\"),\n        location=os.getenv(\"GEMINI_LOCATION\", \"us-central1\"),\n        http_options=types.HttpOptions(api_version='v1')\n    )\n\n    model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.0-flash-exp\")\n    # for model in client.models.list():\n    #     print(model)\n    response = client.models.generate_content(model=model, contents=[prompt])\n    response_text = response.text\n\n    # Log the response\n    logger.info(f\"RESPONSE: {response_text}\")\n\n    # Update cache if enabled\n    if use_cache:\n        # Load cache again to avoid overwrites\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, \"r\") as f:\n                    cache = json.load(f)\n            except:\n                pass\n\n        # Add to cache and save\n        cache[prompt] = response_text\n        try:\n            with open(cache_file, \"w\") as f:\n                json.dump(cache, f)\n        except Exception as e:\n            logger.error(f\"Failed to save cache: {e}\")\n\n    return response_text\n\n\nif __name__ == \"__main__\":\n    test_prompt = \"Hello, how are you?\"\n\n    # First call - should hit the API\n    print(\"Making call...\")\n    response1 = call_llm(test_prompt, use_cache=False)\n    print(f\"Response: {response1}\")\n\n```": "The code defines a function `call_llm` that interacts with a Large Language Model (LLM) via the `google.genai` library and implements a simple caching mechanism to reduce API calls.\n\nThe script begins by setting up logging to a file named \"llm_calls_{date}.log\" in a \"logs\" directory. It configures a logger to write informational messages, including prompts and responses.\n\nThe core logic resides within the `call_llm` function. This function accepts a prompt string and a boolean flag, `use_cache`, as input.\n\nFirst, it logs the input prompt using the configured logger. Then, if `use_cache` is true, it attempts to load a JSON file named \"llm_cache.json\" into a dictionary. If the file exists and can be loaded, it checks if the prompt already exists as a key in the dictionary. If the prompt is found in the cache, the cached response is logged and immediately returned, avoiding an LLM API call. If loading the cache fails, a warning is logged, and the process continues as if there was no cache.\n\nIf caching is disabled (`use_cache` is false) or the prompt is not found in the cache, the function proceeds to call the LLM API. It initializes a `genai.Client` using environment variables for project ID, location, and model name. It then uses the client to generate content based on the provided prompt. The LLM's response is then stored in the `response_text` variable. This response text is then logged using the logger.\n\nAfter receiving the response from the LLM, if `use_cache` is true, the function attempts to update the cache. It loads the cache again (to avoid overwriting any changes made by other concurrent calls).  Then, it adds the current prompt and its corresponding response to the cache dictionary. Finally, it attempts to save the updated cache dictionary back to the \"llm_cache.json\" file. If saving the cache fails, an error message is logged.\n\nFinally, the function returns the LLM's `response_text`.\n\nThe `if __name__ == \"__main__\":` block demonstrates a simple usage of the `call_llm` function.  It defines a test prompt and calls the `call_llm` function with the prompt and `use_cache` set to `False`, demonstrating a direct API call.  The returned response is printed to the console.\n", "Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Do not include the code in the output. Explain the purpose of the code in terms of its logic.\n\nCode:\n```python\n# A script to crawl files from a given directory and its subdirectories.\n# It uses the `os` module to walk through the directory structure and collects file paths based on specified include and exclude patterns.\n# files that are not in the exclude patterns and are in the include patterns are collected.\nimport os\nimport typing\nimport logging\nfrom pathlib import Path\nfrom settings import DEFAULT_EXCLUDE_SUFFIXES, DEFAULT_INCLUDE_SUFFIXES, DEFAULT_EXCLUDE_DIRS\n\n\n\ndef crawl_files(\n    directory: str,\n    include_patterns: typing.Set[str] = DEFAULT_INCLUDE_SUFFIXES,\n    exclude_suffixes: typing.Set[str] = DEFAULT_EXCLUDE_SUFFIXES,\n    exclude_dirs: typing.Set[str] = DEFAULT_EXCLUDE_DIRS,\n    ) -> typing.List[str]:\n    try:\n        files = {\n            str(item.relative_to(directory))  # Store the relative path string\n            for item in Path(directory).rglob(\"*\") # Iterate through all items recursively\n            if item.is_file() # Consider only files\n            if item.suffix in include_patterns # Check 0: Suffix is included\n            and item.suffix not in exclude_suffixes # Check 1: Suffix is not excluded\n            and not any(part in exclude_dirs for part in item.relative_to(directory).parts) # Check 2: No path part is excluded\n        }\n\n    except ValueError as e:\n        logging.error(f\"Error calculating relative paths: {e}\")\n        logging.error(f\"Ensure the search directory '{directory.resolve()}' is within or is the \"\n            f\"current working directory '{directory.resolve()}'\")\n\n    logging.info(f\"Found {len(files)} files in {directory}\")\n    logging.debug(f\"Files found: {files}\")\n\n    return files\n\n\ndef main() -> None:\n    print(\"Crawling files...\")\n    directory = os.getcwd()  \n    print(f\"Files crawled from {directory}:\")\n    print (crawl_files(directory))\n\nif __name__ == \"__main__\":\n    main()\n```": "The code defines a function `crawl_files` that recursively traverses a directory and its subdirectories to identify files that meet specific criteria. It uses the `os` module and `pathlib` for directory traversal and file system operations.\n\nThe `crawl_files` function first takes a directory path as input, along with optional sets of include and exclude file suffixes, and directories to exclude. It walks the directory tree using `Path(directory).rglob(\"*\")`, retrieving every item within the directory and its subdirectories.\n\nFor each item, it checks if it is a file using `item.is_file()`. If it is a file, it further checks if its suffix is present in the `include_patterns` set and absent from the `exclude_suffixes` set. Additionally, it verifies that no part of the relative path is present in the `exclude_dirs` set. The relative path is calculated using `item.relative_to(directory)`. Only the files satisfying all these conditions are collected and stored as relative paths (strings) in a set.\n\nThe `main` function calls the `crawl_files` function using the current working directory. Finally, it prints the results of the file crawling operation to the standard output.\n\nThe primary data transformations involve converting Path objects to their relative string representations using `str(item.relative_to(directory))`, and building a set of these string representations.\n", "Summarize this text in 50 words: \n\n {\"Summarize this text in 50 words: \\n\\n # A script to crawl files from a given directory and its subdirectories.\\n# It uses the `os` module to walk through the directory structure and collects file paths based on specified include and exclude patterns.\\n# files that are not in the exclude patterns and are in the include patterns are collected.\\nimport os\\nimport typing\\nimport logging\\nfrom pathlib import Path\\nfrom settings import DEFAULT_EXCLUDE_SUFFIXES, DEFAULT_INCLUDE_SUFFIXES, DEFAULT_EXCLUDE_DIRS\\n\\n\\n\\ndef crawl_files(\\n    directory: str,\\n    include_patterns: typing.Set[str] = DEFAULT_INCLUDE_SUFFIXES,\\n    exclude_suffixes: typing.Set[str] = DEFAULT_EXCLUDE_SUFFIXES,\\n    exclude_dirs: typing.Set[str] = DEFAULT_EXCLUDE_DIRS,\\n    ) -> typing.List[str]:\\n    try:\\n        files = {\\n            str(item.relative_to(directory))  # Store the relative path string\\n            for item in Path(directory).rglob(\\\"*\\\") # Iterate through all items recursively\\n            if item.is_file() # Consider only files\\n            if item.suffix in include_patterns # Check 0: Suffix is included\\n            and item.suffix not in exclude_suffixes # Check 1: Suffix is not excluded\\n            and not any(part in exclude_dirs for part in item.relative_to(directory).parts) # Check 2: No path part is excluded\\n        }\\n\\n    except ValueError as e:\\n        logging.error(f\\\"Error calculating relative paths: {e}\\\")\\n        logging.error(f\\\"Ensure the search directory '{directory.resolve()}' is within or is the \\\"\\n            f\\\"current working directory '{directory.resolve()}'\\\")\\n\\n    logging.info(f\\\"Found {len(files)} files in {directory}\\\")\\n    logging.debug(f\\\"Files found: {files}\\\")\\n\\n    return files\\n\\n\\ndef main() -> None:\\n    print(\\\"Crawling files...\\\")\\n    directory = os.getcwd()  \\n    print(f\\\"Files crawled from {directory}:\\\")\\n    print (crawl_files(directory))\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\": \"This script crawls a directory and its subdirectories to find files. It uses `os` to traverse the directory structure and `pathlib` to construct paths. Files are collected based on include and exclude patterns for suffixes and directories, returning a list of relative file paths. A main method demonstrates use of crawl_files from the current directory.\\n\", \"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\\n\\nCode:\\n```python\\n# A script to crawl files from a given directory and its subdirectories.\\n# It uses the `os` module to walk through the directory structure and collects file paths based on specified include and exclude patterns.\\n# files that are not in the exclude patterns and are in the include patterns are collected.\\nimport os\\nimport typing\\nimport logging\\nfrom pathlib import Path\\nfrom settings import DEFAULT_EXCLUDE_SUFFIXES, DEFAULT_INCLUDE_SUFFIXES, DEFAULT_EXCLUDE_DIRS\\n\\n\\n\\ndef crawl_files(\\n    directory: str,\\n    include_patterns: typing.Set[str] = DEFAULT_INCLUDE_SUFFIXES,\\n    exclude_suffixes: typing.Set[str] = DEFAULT_EXCLUDE_SUFFIXES,\\n    exclude_dirs: typing.Set[str] = DEFAULT_EXCLUDE_DIRS,\\n    ) -> typing.List[str]:\\n    try:\\n        files = {\\n            str(item.relative_to(directory))  # Store the relative path string\\n            for item in Path(directory).rglob(\\\"*\\\") # Iterate through all items recursively\\n            if item.is_file() # Consider only files\\n            if item.suffix in include_patterns # Check 0: Suffix is included\\n            and item.suffix not in exclude_suffixes # Check 1: Suffix is not excluded\\n            and not any(part in exclude_dirs for part in item.relative_to(directory).parts) # Check 2: No path part is excluded\\n        }\\n\\n    except ValueError as e:\\n        logging.error(f\\\"Error calculating relative paths: {e}\\\")\\n        logging.error(f\\\"Ensure the search directory '{directory.resolve()}' is within or is the \\\"\\n            f\\\"current working directory '{directory.resolve()}'\\\")\\n\\n    logging.info(f\\\"Found {len(files)} files in {directory}\\\")\\n    logging.debug(f\\\"Files found: {files}\\\")\\n\\n    return files\\n\\n\\ndef main() -> None:\\n    print(\\\"Crawling files...\\\")\\n    directory = os.getcwd()  \\n    print(f\\\"Files crawled from {directory}:\\\")\\n    print (crawl_files(directory))\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n```\": \"```python\\n# A script to crawl files from a given directory and its subdirectories.\\n# It uses the `os` module to walk through the directory structure and collects file paths based on specified include and exclude patterns.\\n# files that are not in the exclude patterns and are in the include patterns are collected.\\nimport os\\nimport typing\\nimport logging\\nfrom pathlib import Path\\nfrom settings import DEFAULT_EXCLUDE_SUFFIXES, DEFAULT_INCLUDE_SUFFIXES, DEFAULT_EXCLUDE_DIRS\\n\\n\\n\\ndef crawl_files(\\n    directory: str,\\n    include_patterns: typing.Set[str] = DEFAULT_INCLUDE_SUFFIXES,\\n    exclude_suffixes: typing.Set[str] = DEFAULT_EXCLUDE_SUFFIXES,\\n    exclude_dirs: typing.Set[str] = DEFAULT_EXCLUDE_DIRS,\\n    ) -> typing.List[str]:\\n    \\\"\\\"\\\"\\n    Crawls files from a given directory and its subdirectories based on specified include and exclude patterns.\\n\\n    Args:\\n        directory (str): The directory to crawl.\\n        include_patterns (typing.Set[str], optional): A set of file suffixes to include. Defaults to DEFAULT_INCLUDE_SUFFIXES.\\n        exclude_suffixes (typing.Set[str], optional): A set of file suffixes to exclude. Defaults to DEFAULT_EXCLUDE_SUFFIXES.\\n        exclude_dirs (typing.Set[str], optional): A set of directory names to exclude. Defaults to DEFAULT_EXCLUDE_DIRS.\\n\\n    Returns:\\n        typing.List[str]: A list of file paths relative to the given directory.\\n    \\\"\\\"\\\"\\n    try:\\n        files = {\\n            str(item.relative_to(directory))  # Store the relative path string\\n            for item in Path(directory).rglob(\\\"*\\\") # Iterate through all items recursively\\n            if item.is_file() # Consider only files\\n            if item.suffix in include_patterns # Check 0: Suffix is included\\n            and item.suffix not in exclude_suffixes # Check 1: Suffix is not excluded\\n            and not any(part in exclude_dirs for part in item.relative_to(directory).parts) # Check 2: No path part is excluded\\n        }\\n\\n    except ValueError as e:\\n        logging.error(f\\\"Error calculating relative paths: {e}\\\")\\n        logging.error(f\\\"Ensure the search directory '{directory.resolve()}' is within or is the \\\"\\n            f\\\"current working directory '{directory.resolve()}'\\\")\\n\\n    logging.info(f\\\"Found {len(files)} files in {directory}\\\")\\n    logging.debug(f\\\"Files found: {files}\\\")\\n\\n    return files\\n\\n\\ndef main() -> None:\\n    \\\"\\\"\\\"\\n    Main function to crawl files from the current working directory and print the results.\\n    \\\"\\\"\\\"\\n    print(\\\"Crawling files...\\\")\\n    directory = os.getcwd()  \\n    print(f\\\"Files crawled from {directory}:\\\")\\n    print (crawl_files(directory))\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n```\\n\\n**Algorithm and Logic Flow Analysis:**\\n\\n1.  **`crawl_files` Function:**\\n    *   **Purpose:** This function is the core of the script. It recursively crawls a directory and its subdirectories to find files that match specific criteria (include and exclude patterns for file suffixes and directories).\\n    *   **Input:**\\n        *   `directory` (str): The path to the directory to crawl.\\n        *   `include_patterns` (typing.Set\\\\[str]): A set of file suffixes to include (e.g., `'.txt'`, `'.py'`).\\n        *   `exclude_suffixes` (typing.Set\\\\[str]): A set of file suffixes to exclude (e.g., `'.log'`, `'.tmp'`).\\n        *   `exclude_dirs` (typing.Set\\\\[str]): A set of directory names to exclude (e.g., `'venv'`, `'__pycache__'` ).\\n    *   **`Path(directory).rglob(\\\"*\\\")`**: This creates a `Path` object representing the input `directory`. The `rglob(\\\"*\\\")` method recursively finds all files and subdirectories within the directory.  It returns a generator that yields `Path` objects representing each item.\\n    *   **Set Comprehension**:  The heart of the function uses a set comprehension.  For each `item` (a `Path` object) found by `rglob`:\\n        *   **`item.is_file()`**: It checks if the item is a file. Only files are considered.\\n        *   **`item.suffix in include_patterns`**:  It checks if the file's suffix (e.g., `'.txt'`) is present in the `include_patterns` set. This implements the \\\"include\\\" filter.\\n        *   **`item.suffix not in exclude_suffixes`**: It checks if the file's suffix is *not* present in the `exclude_suffixes` set. This implements the \\\"exclude\\\" filter based on suffix.\\n        *   **`not any(part in exclude_dirs for part in item.relative_to(directory).parts)`**:  This is the directory exclusion logic. `item.relative_to(directory)` calculates the relative path from the input directory to the current file.  `.parts` splits the relative path into its directory components.  The `any()` function then checks if any of these directory components are present in the `exclude_dirs` set.  If any part of the path is in `exclude_dirs`, the file is excluded.\\n        *   **`str(item.relative_to(directory))`**: If all the conditions above are met (the item is a file, its suffix is included, its suffix is not excluded, and none of its directory components are excluded), the relative path of the file (as a string) is added to the `files` set.  Using a set ensures that the result contains no duplicates.\\n    *   **Error Handling**: A `try...except` block catches `ValueError` exceptions, which can occur if `item` is not within the `directory`. A logging message is raised if this happens.\\n    *   **Logging**:  The code uses the `logging` module to log the number of files found and the list of files (at the debug level).\\n    *   **Output:** Returns a `typing.List[str]` containing the relative paths (strings) of all the files that meet the inclusion/exclusion criteria.\\n\\n2.  **`main` Function:**\\n    *   **Purpose:** This function is the entry point of the script.\\n    *   It gets the current working directory using `os.getcwd()`.\\n    *   It calls the `crawl_files` function with the current working directory as input.\\n    *   It prints the results returned by `crawl_files`.\\n\\n3.  **`if __name__ == \\\"__main__\\\":` Block:**\\n    *   This standard Python idiom ensures that the `main` function is called only when the script is executed directly (not when it is imported as a module).\\n\\n**In Summary:**\\n\\nThe code's main algorithm is to recursively traverse a directory, filter files based on include and exclude patterns applied to file suffixes and directory names, and return a list of the relative paths of the matching files.  The set comprehension provides a concise way to combine the directory traversal, filtering, and path transformation into a single expression. The inclusion and exclusion mechanisms give fine-grained control over the files that are collected.  Error handling is included to address potential issues with relative path calculations. Logging provides information about the process and the results.\\n\", \"Summarize this text in 50 words: \\n\\n from google import genai\\nfrom google.genai import types\\nimport os\\nimport logging\\nimport json\\nfrom datetime import datetime\\n\\n\\n# Configure logging\\nlog_directory = os.getenv(\\\"LOG_DIR\\\", \\\"logs\\\")\\nos.makedirs(log_directory, exist_ok=True)\\nlog_file = os.path.join(\\n    log_directory, f\\\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\\\"\\n)\\n\\n# Set up logger\\nlogger = logging.getLogger(\\\"llm_logger\\\")\\nlogger.setLevel(logging.INFO)\\nlogger.propagate = False  # Prevent propagation to root logger\\nfile_handler = logging.FileHandler(log_file)\\nfile_handler.setFormatter(\\n    logging.Formatter(\\\"%(asctime)s - %(levelname)s - %(message)s\\\")\\n)\\nlogger.addHandler(file_handler)\\n\\n# Simple cache configuration\\ncache_file = \\\"llm_cache.json\\\"\\n\\n\\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\\n    # Log the prompt\\n    logger.info(f\\\"PROMPT: {prompt}\\\")\\n\\n    # Check cache if enabled\\n    if use_cache:\\n        # Load cache from disk\\n        cache = {}\\n        if os.path.exists(cache_file):\\n            try:\\n                with open(cache_file, \\\"r\\\") as f:\\n                    cache = json.load(f)\\n            except:\\n                logger.warning(f\\\"Failed to load cache, starting with empty cache\\\")\\n\\n        # Return from cache if exists\\n        if prompt in cache:\\n            logger.info(f\\\"RESPONSE: {cache[prompt]}\\\")\\n            return cache[prompt]\\n\\n    # Call the LLM if not in cache or cache disabled\\n    client = genai.Client(\\n        vertexai=True,\\n        project=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"ai-sre-dev-84b7\\\"),\\n        location=os.getenv(\\\"GEMINI_LOCATION\\\", \\\"us-central1\\\"),\\n        http_options=types.HttpOptions(api_version='v1')\\n    )\\n\\n    model = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.0-flash-exp\\\")\\n    # for model in client.models.list():\\n    #     print(model)\\n    response = client.models.generate_content(model=model, contents=[prompt])\\n    response_text = response.text\\n\\n    # Log the response\\n    logger.info(f\\\"RESPONSE: {response_text}\\\")\\n\\n    # Update cache if enabled\\n    if use_cache:\\n        # Load cache again to avoid overwrites\\n        cache = {}\\n        if os.path.exists(cache_file):\\n            try:\\n                with open(cache_file, \\\"r\\\") as f:\\n                    cache = json.load(f)\\n            except:\\n                pass\\n\\n        # Add to cache and save\\n        cache[prompt] = response_text\\n        try:\\n            with open(cache_file, \\\"w\\\") as f:\\n                json.dump(cache, f)\\n        except Exception as e:\\n            logger.error(f\\\"Failed to save cache: {e}\\\")\\n\\n    return response_text\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    test_prompt = \\\"Hello, how are you?\\\"\\n\\n    # First call - should hit the API\\n    print(\\\"Making call...\\\")\\n    response1 = call_llm(test_prompt, use_cache=False)\\n    print(f\\\"Response: {response1}\\\")\\n\": \"This code defines a function `call_llm` that interacts with a Google LLM (Gemini) API.  It logs prompts and responses, utilizes a JSON file for caching responses to avoid redundant API calls, and includes error handling for loading/saving the cache. The main block calls the function.\\n\", \"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\\n\\nCode:\\n```python\\nfrom google import genai\\nfrom google.genai import types\\nimport os\\nimport logging\\nimport json\\nfrom datetime import datetime\\n\\n\\n# Configure logging\\nlog_directory = os.getenv(\\\"LOG_DIR\\\", \\\"logs\\\")\\nos.makedirs(log_directory, exist_ok=True)\\nlog_file = os.path.join(\\n    log_directory, f\\\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\\\"\\n)\\n\\n# Set up logger\\nlogger = logging.getLogger(\\\"llm_logger\\\")\\nlogger.setLevel(logging.INFO)\\nlogger.propagate = False  # Prevent propagation to root logger\\nfile_handler = logging.FileHandler(log_file)\\nfile_handler.setFormatter(\\n    logging.Formatter(\\\"%(asctime)s - %(levelname)s - %(message)s\\\")\\n)\\nlogger.addHandler(file_handler)\\n\\n# Simple cache configuration\\ncache_file = \\\"llm_cache.json\\\"\\n\\n\\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\\n    # Log the prompt\\n    logger.info(f\\\"PROMPT: {prompt}\\\")\\n\\n    # Check cache if enabled\\n    if use_cache:\\n        # Load cache from disk\\n        cache = {}\\n        if os.path.exists(cache_file):\\n            try:\\n                with open(cache_file, \\\"r\\\") as f:\\n                    cache = json.load(f)\\n            except:\\n                logger.warning(f\\\"Failed to load cache, starting with empty cache\\\")\\n\\n        # Return from cache if exists\\n        if prompt in cache:\\n            logger.info(f\\\"RESPONSE: {cache[prompt]}\\\")\\n            return cache[prompt]\\n\\n    # Call the LLM if not in cache or cache disabled\\n    client = genai.Client(\\n        vertexai=True,\\n        project=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"ai-sre-dev-84b7\\\"),\\n        location=os.getenv(\\\"GEMINI_LOCATION\\\", \\\"us-central1\\\"),\\n        http_options=types.HttpOptions(api_version='v1')\\n    )\\n\\n    model = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.0-flash-exp\\\")\\n    # for model in client.models.list():\\n    #     print(model)\\n    response = client.models.generate_content(model=model, contents=[prompt])\\n    response_text = response.text\\n\\n    # Log the response\\n    logger.info(f\\\"RESPONSE: {response_text}\\\")\\n\\n    # Update cache if enabled\\n    if use_cache:\\n        # Load cache again to avoid overwrites\\n        cache = {}\\n        if os.path.exists(cache_file):\\n            try:\\n                with open(cache_file, \\\"r\\\") as f:\\n                    cache = json.load(f)\\n            except:\\n                pass\\n\\n        # Add to cache and save\\n        cache[prompt] = response_text\\n        try:\\n            with open(cache_file, \\\"w\\\") as f:\\n                json.dump(cache, f)\\n        except Exception as e:\\n            logger.error(f\\\"Failed to save cache: {e}\\\")\\n\\n    return response_text\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    test_prompt = \\\"Hello, how are you?\\\"\\n\\n    # First call - should hit the API\\n    print(\\\"Making call...\\\")\\n    response1 = call_llm(test_prompt, use_cache=False)\\n    print(f\\\"Response: {response1}\\\")\\n\\n```\": \"```python\\nfrom google import genai\\nfrom google.genai import types\\nimport os\\nimport logging\\nimport json\\nfrom datetime import datetime\\n\\n\\n# Configure logging\\nlog_directory = os.getenv(\\\"LOG_DIR\\\", \\\"logs\\\")\\nos.makedirs(log_directory, exist_ok=True)\\nlog_file = os.path.join(\\n    log_directory, f\\\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\\\"\\n)\\n\\n# Set up logger\\nlogger = logging.getLogger(\\\"llm_logger\\\")\\nlogger.setLevel(logging.INFO)\\nlogger.propagate = False  # Prevent propagation to root logger\\nfile_handler = logging.FileHandler(log_file)\\nfile_handler.setFormatter(\\n    logging.Formatter(\\\"%(asctime)s - %(levelname)s - %(message)s\\\")\\n)\\nlogger.addHandler(file_handler)\\n\\n# Simple cache configuration\\ncache_file = \\\"llm_cache.json\\\"\\n\\n\\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\\n    # Log the prompt\\n    logger.info(f\\\"PROMPT: {prompt}\\\")\\n\\n    # Check cache if enabled\\n    if use_cache:\\n        # Load cache from disk\\n        cache = {}\\n        if os.path.exists(cache_file):\\n            try:\\n                with open(cache_file, \\\"r\\\") as f:\\n                    cache = json.load(f)\\n            except:\\n                logger.warning(f\\\"Failed to load cache, starting with empty cache\\\")\\n\\n        # Return from cache if exists\\n        if prompt in cache:\\n            logger.info(f\\\"RESPONSE: {cache[prompt]}\\\")\\n            return cache[prompt]\\n\\n    # Call the LLM if not in cache or cache disabled\\n    client = genai.Client(\\n        vertexai=True,\\n        project=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"ai-sre-dev-84b7\\\"),\\n        location=os.getenv(\\\"GEMINI_LOCATION\\\", \\\"us-central1\\\"),\\n        http_options=types.HttpOptions(api_version='v1')\\n    )\\n\\n    model = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.0-flash-exp\\\")\\n    # for model in client.models.list():\\n    #     print(model)\\n    response = client.models.generate_content(model=model, contents=[prompt])\\n    response_text = response.text\\n\\n    # Log the response\\n    logger.info(f\\\"RESPONSE: {response_text}\\\")\\n\\n    # Update cache if enabled\\n    if use_cache:\\n        # Load cache again to avoid overwrites\\n        cache = {}\\n        if os.path.exists(cache_file):\\n            try:\\n                with open(cache_file, \\\"r\\\") as f:\\n                    cache = json.load(f)\\n            except:\\n                pass\\n\\n        # Add to cache and save\\n        cache[prompt] = response_text\\n        try:\\n            with open(cache_file, \\\"w\\\") as f:\\n                json.dump(cache, f)\\n        except Exception as e:\\n            logger.error(f\\\"Failed to save cache: {e}\\\")\\n\\n    return response_text\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    test_prompt = \\\"Hello, how are you?\\\"\\n\\n    # First call - should hit the API\\n    print(\\\"Making call...\\\")\\n    response1 = call_llm(test_prompt, use_cache=False)\\n    print(f\\\"Response: {response1}\\\")\\n```\\n\\n**Overall Purpose:**\\n\\nThe Python code defines a function `call_llm` that interacts with a Large Language Model (LLM) from Google's `genai` library. It provides functionality for caching responses to avoid redundant API calls.  The main goal is to simplify querying the LLM while incorporating logging and a basic caching mechanism.\\n\\n**Detailed Analysis of Logic Flow:**\\n\\n1. **Initialization:**\\n   - Imports necessary libraries: `google.genai`, `os`, `logging`, `json`, and `datetime`.\\n   - Configures logging:\\n     - Sets up a directory for logs (defaulting to \\\"logs\\\").\\n     - Creates a log file named with the current date (e.g., `llm_calls_20241027.log`).\\n     - Configures a logger to write messages to the log file with timestamps and severity levels.  The logger is configured to *not* propagate to the root logger, meaning it operates independently.\\n   - Defines the cache file name: `llm_cache.json`.\\n\\n2. **`call_llm(prompt: str, use_cache: bool = True) -> str` Function:**\\n   - **Input:**\\n     - `prompt` (str): The text prompt to send to the LLM.\\n     - `use_cache` (bool):  A flag indicating whether to use the cache (defaults to `True`).\\n   - **Output:**\\n     - (str): The LLM's response to the prompt.\\n   - **Logic:**\\n     - **Logging:** Logs the incoming `prompt` to the configured logger.\\n     - **Cache Check (if `use_cache` is True):**\\n       - Loads the cache from `llm_cache.json`. If the file doesn't exist or if loading fails, an empty dictionary is used.  A warning is logged if the cache fails to load.\\n       - Checks if the `prompt` already exists as a key in the `cache` dictionary.\\n       - If the prompt is found in the cache, the corresponding cached response is logged and returned directly, avoiding an LLM call.\\n     - **LLM Call (if not in cache or `use_cache` is False):**\\n       - Initializes the `genai` client using environment variables for project ID and location. It uses Vertex AI as the backend.  It configures the `api_version` to 'v1' through `HttpOptions`.\\n       - Sets the `model` name using an environment variable.\\n       - Calls `client.models.generate_content(model=model, contents=[prompt])` to send the prompt to the LLM.\\n       - Extracts the response text from the LLM's response.\\n     - **Logging:** Logs the LLM's response to the logger.\\n     - **Cache Update (if `use_cache` is True):**\\n       - Reloads the cache from disk *again*. This is done to prevent overwriting any changes made to the cache by other processes since the initial load.\\n       - Adds the `prompt` and the LLM's `response_text` to the `cache` dictionary.\\n       - Saves the updated `cache` to `llm_cache.json`. If saving fails, an error is logged.\\n     - **Return:** Returns the LLM's `response_text`.\\n\\n3. **`if __name__ == \\\"__main__\\\":` Block:**\\n   - This block is executed only when the script is run directly (not when imported as a module).\\n   - Defines a `test_prompt`.\\n   - Calls the `call_llm` function with `use_cache=False` to force an API call to the LLM.\\n   - Prints the returned response.\\n\\n**Control Structures:**\\n\\n- **`if use_cache:`:** Conditional statement to enable or disable the cache functionality.\\n- **`if os.path.exists(cache_file):`:**  Conditional statement to check if the cache file exists before attempting to load it.\\n- **`try...except:`:** Error handling blocks to catch exceptions during cache loading and saving.\\n\\n**Data Transformations:**\\n\\n- `datetime.now().strftime('%Y%m%d')`: Converts the current date and time into a string format (YYYYMMDD) for use in the log file name.\\n- `json.load(f)`: Parses the JSON content of the cache file into a Python dictionary.\\n- `json.dump(cache, f)`: Serializes the Python dictionary `cache` into JSON format and writes it to the cache file.\\n\\n**In summary, the code defines a `call_llm` function that intelligently queries a Large Language Model. It uses a JSON file as a simple cache to store and retrieve LLM responses based on the input prompt, minimizing redundant API calls.  It also includes comprehensive logging of prompts and responses for debugging and auditing purposes.**\\n\", \"Summarize this text in 50 words: \\n\\n import logging\\nfrom pathlib import Path\\nfrom pocketflow import Node\\nfrom call_llm import call_llm\\nfrom settings import (\\n    DEFAULT_EXCLUDE_SUFFIXES,\\n    DEFAULT_INCLUDE_SUFFIXES,\\n    DEFAULT_EXCLUDE_DIRS,\\n)\\nfrom settings import parse_args\\nimport fetch_files\\n\\n\\nclass Summarise(Node):\\n    \\\"\\\"\\\"\\n    This class identifies abstractions in the code.\\n    \\\"\\\"\\\"\\n\\n    def prep(self,shared):\\n        return file_contents[\\\"data\\\"]\\n    \\n    def exec(self, text):\\n        prompt = f\\\"Summarize this text in 50 words: \\\\n\\\\n {text}\\\"\\n        return call_llm(prompt)\\n    \\n    def post(self, shared, prep_res, exec_res):\\n        # Store the result in the shared settings\\n        file_contents[\\\"summaries\\\"] = exec_res\\n        return \\\"default\\\"\\n\\n\\n# New Node for Logic Flow Analysis\\nclass AnalyzeLogicFlow(Node):\\n    \\\"\\\"\\\"\\n    This class analyzes the code to describe its main algorithm or logic flow.\\n    \\\"\\\"\\\"\\n\\n    # prep and post might not be needed if main loop handles orchestration\\n    # def prep(self, shared):\\n    #     pass\\n\\n    def exec(self, text):\\n        prompt = f\\\"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\\\\n\\\\nCode:\\\\n```python\\\\n{text}\\\\n```\\\"\\n        # Assuming call_llm handles potential errors and returns a string\\n        return call_llm(prompt)\\n\\n    # def post(self, shared, prep_res, exec_res):\\n    #     pass\\n\\n\\ndef main() -> None:\\n    args = parse_args()\\n\\n    # Create a dictionary of settings to be used in the script\\n    shared_settings = {\\n        \\\"name\\\": args.name,\\n        \\\"directory\\\": args.directory, # Target directory to analyze\\n        \\\"exclude_patterns\\\": DEFAULT_EXCLUDE_SUFFIXES,\\n        \\\"exclude_dirs\\\": DEFAULT_EXCLUDE_DIRS,\\n        \\\"include_suffixes\\\": DEFAULT_INCLUDE_SUFFIXES,\\n        \\\"additional_exclude_dirs\\\": args.exclude_dirs,\\n    }\\n    logging.debug(f\\\"Shared settings: {shared_settings}\\\")\\n\\n    # Get a list of files from the target directory\\n    # Ensure paths are relative to the target directory for correct output structure\\n    target_dir = Path(shared_settings[\\\"directory\\\"]).resolve()\\n    logging.info(f\\\"Analyzing directory: {target_dir}\\\")\\n\\n    # fetch_files.crawl_files already returns relative paths if called correctly\\n    # Let's ensure the base directory passed to crawl_files is the target directory\\n    files_to_process = fetch_files.crawl_files(\\n        directory=str(target_dir), # Pass absolute path to crawl\\n        include_patterns=shared_settings[\\\"include_suffixes\\\"],\\n        exclude_suffixes=shared_settings[\\\"exclude_patterns\\\"],\\n        exclude_dirs=shared_settings[\\\"exclude_dirs\\\"].union(\\n            shared_settings[\\\"additional_exclude_dirs\\\"]\\n        ),\\n    )\\n    \\n    # Instantiate nodes\\n    summariser = Summarise()\\n    logic_analyzer = AnalyzeLogicFlow()\\n\\n    results = {}\\n    logging.info(f\\\"Processing {len(files_to_process)} files...\\\")\\n\\n    # Process each file\\n    for relative_file_path in files_to_process:\\n        absolute_file_path = target_dir / relative_file_path\\n        logging.debug(f\\\"Processing file: {absolute_file_path}\\\")\\n        print(f\\\"Processing: {relative_file_path}\\\") # User feedback\\n        try:\\n            with open(absolute_file_path, \\\"r\\\", encoding='utf-8') as f:\\n                content = f.read()\\n                if content.strip(): # Ensure content is not empty\\n                    # Call LLM for summary\\n                    summary = summariser.exec(content)\\n                    # Call LLM for logic flow\\n                    logic_flow = logic_analyzer.exec(content)\\n                    results[relative_file_path] = {\\\"summary\\\": summary, \\\"logic_flow\\\": logic_flow}\\n                else:\\n                    logging.warning(f\\\"Skipped empty file: {absolute_file_path}\\\")\\n                    print(f\\\"Skipped empty file: {relative_file_path}\\\")\\n        except Exception as e:\\n            logging.error(f\\\"Error processing file {absolute_file_path}: {e}\\\")\\n            print(f\\\"Error processing file {relative_file_path}: {e}\\\")\\n\\n    # Generate Markdown output\\n    output_base_dir = target_dir / \\\"docs\\\"\\n    logging.info(f\\\"Generating documentation in: {output_base_dir}\\\")\\n    try:\\n        output_base_dir.mkdir(parents=True, exist_ok=True)\\n    except Exception as e:\\n        logging.error(f\\\"Failed to create output directory {output_base_dir}: {e}\\\")\\n        print(f\\\"Error: Could not create output directory {output_base_dir}. Aborting.\\\")\\n        return # Exit if we can't create the output dir\\n\\n    for relative_file_path, data in results.items():\\n        md_filename = Path(relative_file_path).with_suffix(\\\".md\\\")\\n        md_path = output_base_dir / md_filename\\n        \\n        try:\\n            # Ensure subdirectory exists\\n            md_path.parent.mkdir(parents=True, exist_ok=True)\\n\\n            # Format Markdown content\\n            md_content = f\\\"\\\"\\\"# File: {relative_file_path}\\n\\n## Summary\\n\\n{data['summary']}\\n\\n## Logic Flow\\n\\n{data['logic_flow']}\\n\\\"\\\"\\\"\\n            # Write the Markdown file\\n            with open(md_path, \\\"w\\\", encoding='utf-8') as f:\\n                f.write(md_content)\\n            logging.info(f\\\"Generated: {md_path}\\\")\\n            print(f\\\"Generated: {md_path.relative_to(target_dir)}\\\") # Show relative path to user\\n\\n        except Exception as e:\\n            logging.error(f\\\"Failed to write Markdown file {md_path}: {e}\\\")\\n            print(f\\\"Error writing file {md_path.relative_to(target_dir)}: {e}\\\")\\n\\n    print(f\\\"\\\\nDocumentation generation complete. Files written to: {output_base_dir}\\\")\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n\": \"This Python script analyzes code in a directory, generating summaries and logic flow descriptions for each file using a large language model (LLM). It uses `Summarise` and `AnalyzeLogicFlow` nodes, crawls files, processes them, and outputs results in Markdown format to a \\\"docs\\\" subdirectory within the target directory.\\n\", \"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\\n\\nCode:\\n```python\\nimport logging\\nfrom pathlib import Path\\nfrom pocketflow import Node\\nfrom call_llm import call_llm\\nfrom settings import (\\n    DEFAULT_EXCLUDE_SUFFIXES,\\n    DEFAULT_INCLUDE_SUFFIXES,\\n    DEFAULT_EXCLUDE_DIRS,\\n)\\nfrom settings import parse_args\\nimport fetch_files\\n\\n\\nclass Summarise(Node):\\n    \\\"\\\"\\\"\\n    This class identifies abstractions in the code.\\n    \\\"\\\"\\\"\\n\\n    def prep(self,shared):\\n        return file_contents[\\\"data\\\"]\\n    \\n    def exec(self, text):\\n        prompt = f\\\"Summarize this text in 50 words: \\\\n\\\\n {text}\\\"\\n        return call_llm(prompt)\\n    \\n    def post(self, shared, prep_res, exec_res):\\n        # Store the result in the shared settings\\n        file_contents[\\\"summaries\\\"] = exec_res\\n        return \\\"default\\\"\\n\\n\\n# New Node for Logic Flow Analysis\\nclass AnalyzeLogicFlow(Node):\\n    \\\"\\\"\\\"\\n    This class analyzes the code to describe its main algorithm or logic flow.\\n    \\\"\\\"\\\"\\n\\n    # prep and post might not be needed if main loop handles orchestration\\n    # def prep(self, shared):\\n    #     pass\\n\\n    def exec(self, text):\\n        prompt = f\\\"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\\\\n\\\\nCode:\\\\n```python\\\\n{text}\\\\n```\\\"\\n        # Assuming call_llm handles potential errors and returns a string\\n        return call_llm(prompt)\\n\\n    # def post(self, shared, prep_res, exec_res):\\n    #     pass\\n\\n\\ndef main() -> None:\\n    args = parse_args()\\n\\n    # Create a dictionary of settings to be used in the script\\n    shared_settings = {\\n        \\\"name\\\": args.name,\\n        \\\"directory\\\": args.directory, # Target directory to analyze\\n        \\\"exclude_patterns\\\": DEFAULT_EXCLUDE_SUFFIXES,\\n        \\\"exclude_dirs\\\": DEFAULT_EXCLUDE_DIRS,\\n        \\\"include_suffixes\\\": DEFAULT_INCLUDE_SUFFIXES,\\n        \\\"additional_exclude_dirs\\\": args.exclude_dirs,\\n    }\\n    logging.debug(f\\\"Shared settings: {shared_settings}\\\")\\n\\n    # Get a list of files from the target directory\\n    # Ensure paths are relative to the target directory for correct output structure\\n    target_dir = Path(shared_settings[\\\"directory\\\"]).resolve()\\n    logging.info(f\\\"Analyzing directory: {target_dir}\\\")\\n\\n    # fetch_files.crawl_files already returns relative paths if called correctly\\n    # Let's ensure the base directory passed to crawl_files is the target directory\\n    files_to_process = fetch_files.crawl_files(\\n        directory=str(target_dir), # Pass absolute path to crawl\\n        include_patterns=shared_settings[\\\"include_suffixes\\\"],\\n        exclude_suffixes=shared_settings[\\\"exclude_patterns\\\"],\\n        exclude_dirs=shared_settings[\\\"exclude_dirs\\\"].union(\\n            shared_settings[\\\"additional_exclude_dirs\\\"]\\n        ),\\n    )\\n    \\n    # Instantiate nodes\\n    summariser = Summarise()\\n    logic_analyzer = AnalyzeLogicFlow()\\n\\n    results = {}\\n    logging.info(f\\\"Processing {len(files_to_process)} files...\\\")\\n\\n    # Process each file\\n    for relative_file_path in files_to_process:\\n        absolute_file_path = target_dir / relative_file_path\\n        logging.debug(f\\\"Processing file: {absolute_file_path}\\\")\\n        print(f\\\"Processing: {relative_file_path}\\\") # User feedback\\n        try:\\n            with open(absolute_file_path, \\\"r\\\", encoding='utf-8') as f:\\n                content = f.read()\\n                if content.strip(): # Ensure content is not empty\\n                    # Call LLM for summary\\n                    summary = summariser.exec(content)\\n                    # Call LLM for logic flow\\n                    logic_flow = logic_analyzer.exec(content)\\n                    results[relative_file_path] = {\\\"summary\\\": summary, \\\"logic_flow\\\": logic_flow}\\n                else:\\n                    logging.warning(f\\\"Skipped empty file: {absolute_file_path}\\\")\\n                    print(f\\\"Skipped empty file: {relative_file_path}\\\")\\n        except Exception as e:\\n            logging.error(f\\\"Error processing file {absolute_file_path}: {e}\\\")\\n            print(f\\\"Error processing file {relative_file_path}: {e}\\\")\\n\\n    # Generate Markdown output\\n    output_base_dir = target_dir / \\\"docs\\\"\\n    logging.info(f\\\"Generating documentation in: {output_base_dir}\\\")\\n    try:\\n        output_base_dir.mkdir(parents=True, exist_ok=True)\\n    except Exception as e:\\n        logging.error(f\\\"Failed to create output directory {output_base_dir}: {e}\\\")\\n        print(f\\\"Error: Could not create output directory {output_base_dir}. Aborting.\\\")\\n        return # Exit if we can't create the output dir\\n\\n    for relative_file_path, data in results.items():\\n        md_filename = Path(relative_file_path).with_suffix(\\\".md\\\")\\n        md_path = output_base_dir / md_filename\\n        \\n        try:\\n            # Ensure subdirectory exists\\n            md_path.parent.mkdir(parents=True, exist_ok=True)\\n\\n            # Format Markdown content\\n            md_content = f\\\"\\\"\\\"# File: {relative_file_path}\\n\\n## Summary\\n\\n{data['summary']}\\n\\n## Logic Flow\\n\\n{data['logic_flow']}\\n\\\"\\\"\\\"\\n            # Write the Markdown file\\n            with open(md_path, \\\"w\\\", encoding='utf-8') as f:\\n                f.write(md_content)\\n            logging.info(f\\\"Generated: {md_path}\\\")\\n            print(f\\\"Generated: {md_path.relative_to(target_dir)}\\\") # Show relative path to user\\n\\n        except Exception as e:\\n            logging.error(f\\\"Failed to write Markdown file {md_path}: {e}\\\")\\n            print(f\\\"Error writing file {md_path.relative_to(target_dir)}: {e}\\\")\\n\\n    print(f\\\"\\\\nDocumentation generation complete. Files written to: {output_base_dir}\\\")\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n\\n```\": \"Okay, let's break down the Python code's algorithm and logic flow.\\n\\n**Purpose:**\\n\\nThe code aims to automatically generate documentation for Python code files in a given directory. It uses a Large Language Model (LLM) to create summaries and analyze the logic flow of each file.  The results are then saved as Markdown files within a `docs` subdirectory of the original directory.\\n\\n**Algorithm/Logic Flow:**\\n\\n1.  **Initialization and Argument Parsing:**\\n    *   The `main()` function starts by parsing command-line arguments using `parse_args()`.  The expected arguments probably include the target directory (`--directory`) and potentially exclude directories (`--exclude_dirs`).\\n    *   Default settings are defined from the `settings` module:\\n        *   `DEFAULT_EXCLUDE_SUFFIXES`:  File extensions to exclude (e.g., `.pyc`, `.log`).\\n        *   `DEFAULT_INCLUDE_SUFFIXES`: File extensions to include (e.g., `.py`).\\n        *   `DEFAULT_EXCLUDE_DIRS`: Directories to exclude (e.g., `.git`, `__pycache__`).\\n    *   A `shared_settings` dictionary is created, combining the command-line arguments and the default values. This dictionary configures the file processing.\\n\\n2.  **File Discovery:**\\n    *   The code resolves the target directory path using `Path(shared_settings[\\\"directory\\\"]).resolve()`.\\n    *   The `fetch_files.crawl_files()` function is called to generate a list of Python files to process.  This function recursively crawls the target directory, filtering files based on the include/exclude patterns defined in `shared_settings`. Crucially, `crawl_files` is expected to return *relative* paths of files, *relative to the target directory*.\\n    * The code ensures that it passes the absolute path of the target directory to `crawl_files`.\\n3.  **Node Instantiation:**\\n    *   Two node objects `Summarise` and `AnalyzeLogicFlow` are created. These nodes leverage the `pocketflow` framework and encapsulate the LLM call logic.\\n\\n4.  **File Processing Loop:**\\n    *   The code iterates through each file found by `crawl_files()`. The loop performs the core analysis for each file.\\n    *   Inside the loop:\\n        *   It constructs the absolute file path by combining the `target_dir` and the relative file path from `crawl_files()`.\\n        *   It opens the file in read mode (`\\\"r\\\"`) with UTF-8 encoding to handle different character sets.\\n        *   It reads the file content into the `content` variable.\\n        *   A check `if content.strip()` ensures the file is not empty.\\n        *   If the file contains content:\\n            *   It calls the `exec` method of `summariser` object, passing in the `content` to generate a summary of the file.\\n            *   It calls the `exec` method of `logic_analyzer` object, passing in the `content` to generate an analysis of the logic flow of the file.\\n            *   The returned summary and logic flow are stored in the `results` dictionary using the relative path as the key.\\n        *   If the file is empty, a warning message is logged, and the file is skipped.\\n        *   A `try...except` block handles potential errors during file processing (e.g., file not found, permission errors).\\n\\n5.  **Markdown Output Generation:**\\n    *   After processing all files, the code creates the output directory ( `target_dir / \\\"docs\\\"` ) if it doesn't exist.\\n    *   It iterates through the `results` dictionary, which contains the summaries and logic flows for each file.\\n    *   For each file:\\n        *   It constructs the output Markdown file path in the \\\"docs\\\" subdirectory. The filename is derived from the original Python file's name, but with a `.md` extension.\\n        *   It creates a Markdown string (`md_content`) with a header containing the filename, followed by sections for the summary and logic flow, which are populated from the `results` dictionary.\\n        *   It writes the Markdown content to the output file using UTF-8 encoding.\\n        *   A `try...except` block handles potential errors during file writing.\\n        * The code ensures that the parent directories of the markdown file exist by calling `md_path.parent.mkdir(parents=True, exist_ok=True)`\\n\\n6.  **Completion Message:**\\n    *   Finally, the code prints a message indicating that the documentation generation is complete and shows the location of the generated files.\\n\\n**Control Structures:**\\n\\n*   **`for` loop:** Iterates through the list of files to process.\\n*   **`try...except` blocks:** Handle potential exceptions during file processing and markdown generation.\\n*   **`if` statement:** Checks if the file content is empty before processing.\\n\\n**Function Calls:**\\n\\n*   `parse_args()`: Parses command-line arguments.\\n*   `fetch_files.crawl_files()`:  Recursively finds files in a directory based on include/exclude rules.\\n*   `open()`: Opens files for reading and writing.\\n*   `file.read()`: Reads the content of a file.\\n*   `Summarise.exec()`: Calls the LLM to generate a summary of the code.\\n*   `AnalyzeLogicFlow.exec()`: Calls the LLM to analyze the logic flow of the code.\\n*   `Path.mkdir()`: Creates a directory (and parent directories if necessary).\\n*   `file.write()`: Writes content to a file.\\n*   `logging.info()`, `logging.debug()`, `logging.error()`:  Logs informational, debugging, and error messages.\\n\\n**Data Transformations:**\\n\\n*   Command-line arguments are parsed and stored in the `shared_settings` dictionary.\\n*   File paths are manipulated using the `Path` object.\\n*   The `crawl_files()` function transforms a directory and include/exclude patterns into a list of file paths.\\n*   The LLM functions (`Summarise.exec()`, `AnalyzeLogicFlow.exec()`) transform code content into summaries and logic flow analyses (strings).\\n*   The output is formatted as Markdown.\\n\\n**Key Classes and Nodes:**\\n\\n*   `Summarise`: A `pocketflow` Node responsible for summarizing code using an LLM.  The `exec` method defines the LLM prompt.\\n*   `AnalyzeLogicFlow`: A `pocketflow` Node responsible for analyzing the logic flow of code using an LLM.  The `exec` method defines the LLM prompt.\\n*   `Node` (from `pocketflow`):  Likely a base class for defining processing nodes in a pipeline. The `prep`, `exec`, and `post` methods suggest a process-execute-postprocess workflow.\\n\\n**Assumptions:**\\n\\n*   The `call_llm()` function is responsible for interacting with the LLM, handling API requests, and potentially error handling.  It's expected to return a string containing the LLM's response.\\n*   The `pocketflow` library provides the `Node` base class and handles the execution and coordination of the nodes. The prep, exec, and post methods provide the entry points to the nodes' functionality.\\n*   The `fetch_files.crawl_files()` function is assumed to return relative file paths with respect to the target directory.\\n* The logging level is assumed to be configured elsewhere.\\n\\n**Improvements/Observations:**\\n\\n*   **Error Handling:** The `try...except` blocks are good, but could be more specific (e.g., catching `FileNotFoundError` separately).\\n*   **LLM Interaction:** The `call_llm()` function's error handling and retry logic are hidden.  Ideally, this would be exposed or handled more explicitly.\\n*   **Output Formatting:** The Markdown output format is basic.  It could be made more configurable or provide options for different documentation styles.\\n*   **Modularity:**  The code is fairly modular, with the `Node` classes separating the LLM interaction logic.  However, the Markdown generation could be extracted into a separate function or class.\\n*   **Shared State:**  The use of `shared_settings` is a good practice for passing configuration data.\\n* The nodes don't implement the prep and post functions, which suggests that they are not used to share state.\\n\", \"Summarize this text in 50 words: \\n\\n import typing\\nimport argparse\\nimport os\\n\\n\\nDEFAULT_INCLUDE_SUFFIXES = {\\n    \\\".js\\\",\\n    \\\".json\\\",\\n    \\\".jsx\\\",\\n    \\\".md\\\",\\n    \\\".py\\\",\\n    \\\".pyi\\\",\\n    \\\".pyx\\\",\\n    \\\".rst\\\",\\n    \\\".tf\\\",\\n    \\\".tfvars\\\",\\n    \\\".ts\\\",\\n    \\\".tsx\\\",\\n    \\\".yaml\\\",\\n    \\\".yml\\\",\\n    \\\"Dockerfile\\\",\\n    \\\"Jenkinsfil\\\",\\n}\\n\\nDEFAULT_EXCLUDE_SUFFIXES = {}\\n\\nDEFAULT_EXCLUDE_DIRS = {\\n    \\\".git\\\",\\n    \\\".github\\\",\\n    \\\".terraform\\\",\\n    \\\"assets\\\",\\n    \\\"build\\\",\\n    \\\"dist\\\",\\n    \\\"env\\\",\\n    \\\"docs\\\",\\n    \\\"env\\\",\\n    \\\"images\\\",\\n    \\\"temp\\\",\\n    \\\"tests\\\",\\n    \\\"tmp\\\",\\n    \\\"venv\\\",\\n}\\n\\n\\ndef parse_args() -> typing.Any:\\n    \\\"\\\"\\\"\\n    Parse command line arguments.\\n    :return: Parsed arguments\\n    \\\"\\\"\\\"\\n    parser = argparse.ArgumentParser(\\n        description=\\\"Generate documentation for a project using AI.\\\",\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\n    )\\n\\n    parser.add_argument(\\n        \\\"-n\\\",\\n        \\\"--name\\\",\\n        help=\\\"local Repository name\\\",\\n        default=\\\"repo_name\\\",\\n        )\\n\\n    parser.add_argument(\\n        \\\"-d\\\",\\n        \\\"--directory\\\",\\n        help=\\\"top level directory to scan, default is current working directory\\\",\\n        default=os.getcwd(),\\n        )\\n\\n    parser.add_argument(\\n        \\\"-e\\\",\\n        \\\"--exclude_dirs\\\",\\n        nargs='*',\\n        help=\\\"comma separated list of directories to exclude\\\",\\n        default=\\\",\\\"\\n        )\\n\\n    return parser.parse_args()\\n\\nmodel = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.5-pro-exp-03-25\\\")\\nproject=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"sre-ai-dev\\\")\": \"This code defines default file suffixes and directories to include/exclude when generating documentation. It uses `argparse` to handle command-line arguments for specifying the repository name, target directory, and directories to exclude. It also retrieves the Gemini model and project ID from environment variables.\\n\", \"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\\n\\nCode:\\n```python\\nimport typing\\nimport argparse\\nimport os\\n\\n\\nDEFAULT_INCLUDE_SUFFIXES = {\\n    \\\".js\\\",\\n    \\\".json\\\",\\n    \\\".jsx\\\",\\n    \\\".md\\\",\\n    \\\".py\\\",\\n    \\\".pyi\\\",\\n    \\\".pyx\\\",\\n    \\\".rst\\\",\\n    \\\".tf\\\",\\n    \\\".tfvars\\\",\\n    \\\".ts\\\",\\n    \\\".tsx\\\",\\n    \\\".yaml\\\",\\n    \\\".yml\\\",\\n    \\\"Dockerfile\\\",\\n    \\\"Jenkinsfil\\\",\\n}\\n\\nDEFAULT_EXCLUDE_SUFFIXES = {}\\n\\nDEFAULT_EXCLUDE_DIRS = {\\n    \\\".git\\\",\\n    \\\".github\\\",\\n    \\\".terraform\\\",\\n    \\\"assets\\\",\\n    \\\"build\\\",\\n    \\\"dist\\\",\\n    \\\"env\\\",\\n    \\\"docs\\\",\\n    \\\"env\\\",\\n    \\\"images\\\",\\n    \\\"temp\\\",\\n    \\\"tests\\\",\\n    \\\"tmp\\\",\\n    \\\"venv\\\",\\n}\\n\\n\\ndef parse_args() -> typing.Any:\\n    \\\"\\\"\\\"\\n    Parse command line arguments.\\n    :return: Parsed arguments\\n    \\\"\\\"\\\"\\n    parser = argparse.ArgumentParser(\\n        description=\\\"Generate documentation for a project using AI.\\\",\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\n    )\\n\\n    parser.add_argument(\\n        \\\"-n\\\",\\n        \\\"--name\\\",\\n        help=\\\"local Repository name\\\",\\n        default=\\\"repo_name\\\",\\n        )\\n\\n    parser.add_argument(\\n        \\\"-d\\\",\\n        \\\"--directory\\\",\\n        help=\\\"top level directory to scan, default is current working directory\\\",\\n        default=os.getcwd(),\\n        )\\n\\n    parser.add_argument(\\n        \\\"-e\\\",\\n        \\\"--exclude_dirs\\\",\\n        nargs='*',\\n        help=\\\"comma separated list of directories to exclude\\\",\\n        default=\\\",\\\"\\n        )\\n\\n    return parser.parse_args()\\n\\nmodel = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.5-pro-exp-03-25\\\")\\nproject=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"sre-ai-dev\\\")\\n```\": \"```python\\nimport typing\\nimport argparse\\nimport os\\n\\n\\nDEFAULT_INCLUDE_SUFFIXES = {\\n    \\\".js\\\",\\n    \\\".json\\\",\\n    \\\".jsx\\\",\\n    \\\".md\\\",\\n    \\\".py\\\",\\n    \\\".pyi\\\",\\n    \\\".pyx\\\",\\n    \\\".rst\\\",\\n    \\\".tf\\\",\\n    \\\".tfvars\\\",\\n    \\\".ts\\\",\\n    \\\".tsx\\\",\\n    \\\".yaml\\\",\\n    \\\".yml\\\",\\n    \\\"Dockerfile\\\",\\n    \\\"Jenkinsfil\\\",\\n}\\n\\nDEFAULT_EXCLUDE_SUFFIXES = {}\\n\\nDEFAULT_EXCLUDE_DIRS = {\\n    \\\".git\\\",\\n    \\\".github\\\",\\n    \\\".terraform\\\",\\n    \\\"assets\\\",\\n    \\\"build\\\",\\n    \\\"dist\\\",\\n    \\\"env\\\",\\n    \\\"docs\\\",\\n    \\\"env\\\",\\n    \\\"images\\\",\\n    \\\"temp\\\",\\n    \\\"tests\\\",\\n    \\\"tmp\\\",\\n    \\\"venv\\\",\\n}\\n\\n\\ndef parse_args() -> typing.Any:\\n    \\\"\\\"\\\"\\n    Parse command line arguments.\\n    :return: Parsed arguments\\n    \\\"\\\"\\\"\\n    parser = argparse.ArgumentParser(\\n        description=\\\"Generate documentation for a project using AI.\\\",\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\n    )\\n\\n    parser.add_argument(\\n        \\\"-n\\\",\\n        \\\"--name\\\",\\n        help=\\\"local Repository name\\\",\\n        default=\\\"repo_name\\\",\\n        )\\n\\n    parser.add_argument(\\n        \\\"-d\\\",\\n        \\\"--directory\\\",\\n        help=\\\"top level directory to scan, default is current working directory\\\",\\n        default=os.getcwd(),\\n        )\\n\\n    parser.add_argument(\\n        \\\"-e\\\",\\n        \\\"--exclude_dirs\\\",\\n        nargs='*',\\n        help=\\\"comma separated list of directories to exclude\\\",\\n        default=\\\",\\\"\\n        )\\n\\n    return parser.parse_args()\\n\\nmodel = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.5-pro-exp-03-25\\\")\\nproject=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"sre-ai-dev\\\")\\n```\\n\\n**Analysis of the Python Code:**\\n\\n**1. Imports:**\\n\\n*   `typing`: Used for type hinting, improving code readability and maintainability.\\n*   `argparse`:  A standard library module used for parsing command-line arguments. It allows the program to receive input from the user when executed from the command line.\\n*   `os`: Provides a way of using operating system-dependent functionality, such as getting the current working directory.\\n\\n**2. Default Configuration Sets:**\\n\\n*   `DEFAULT_INCLUDE_SUFFIXES`: A set containing file extensions that the program will consider for processing (e.g., \\\".js\\\", \\\".py\\\", \\\".md\\\").  This suggests the code will process files of these types.\\n*   `DEFAULT_EXCLUDE_SUFFIXES`: An empty set, presumably designed to hold file extensions that should be excluded from processing.  Currently, it excludes none.\\n*   `DEFAULT_EXCLUDE_DIRS`: A set of directory names that should be excluded from the scanning process (e.g., \\\".git\\\", \\\"venv\\\", \\\"tests\\\"). This is useful for ignoring irrelevant directories.\\n\\n**3. `parse_args()` Function:**\\n\\n*   **Purpose:** This function defines how the program accepts and processes command-line arguments.\\n*   **`argparse.ArgumentParser`:** Creates an argument parser object. The `description` provides a brief explanation of the program's purpose, and `formatter_class` improves the help message format.\\n*   **`parser.add_argument()`:**  This method is called multiple times to define the different command-line arguments that the program accepts.\\n    *   `-n`, `--name`:  Defines an argument for the repository name. It has a default value of \\\"repo\\\\_name\\\".\\n    *   `-d`, `--directory`: Defines an argument for the top-level directory to scan. Its default value is the current working directory obtained using `os.getcwd()`.\\n    *   `-e`, `--exclude_dirs`:  Defines an argument for a list of directories to exclude. It accepts multiple values (`nargs='*'`) and has a default value of a single comma \\\",\\\". This means that if the user doesn't specify any excluded directories, the default value is a single comma, which likely requires further processing to handle correctly (e.g., splitting it into an empty list).\\n*   **`parser.parse_args()`:**  Parses the command-line arguments and returns an object containing the values provided by the user (or the default values if no arguments are provided).\\n*   **Return Value:**  The function returns an `argparse.Namespace` object (aliased as `typing.Any` for generic type hinting). This object contains the parsed arguments as attributes.\\n\\n**4. Environment Variables:**\\n\\n*   `model = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.5-pro-exp-03-25\\\")`: Reads the value of the environment variable `GEMINI_MODEL`. If the environment variable is not set, it defaults to \\\"gemini-2.5-pro-exp-03-25\\\".  This likely specifies an AI model to be used.\\n*   `project=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"sre-ai-dev\\\")`: Reads the value of the environment variable `GEMINI_PROJECT_ID`. If the environment variable is not set, it defaults to \\\"sre-ai-dev\\\". This probably specifies the Google Cloud project ID for the AI model.\\n\\n**Overall Purpose and Logic Flow:**\\n\\nThis code snippet sets up the basic infrastructure for a program that generates documentation for a project using AI.  The core logic is as follows:\\n\\n1.  **Configuration:** Defines default settings for file suffixes to include, file suffixes to exclude, and directories to exclude.\\n2.  **Argument Parsing:** Uses `argparse` to handle command-line arguments, allowing the user to specify:\\n    *   The repository name.\\n    *   The directory to scan.\\n    *   A list of directories to exclude.\\n3.  **Environment Variable Handling:** Reads the `GEMINI_MODEL` and `GEMINI_PROJECT_ID` environment variables (or uses default values if they are not set). These variables appear to be related to the AI model used for generating the documentation.\\n\\nThe next steps (not shown in the code snippet) would likely involve:\\n\\n1.  Retrieving the command-line arguments from the `parse_args()` function.\\n2.  Walking the specified directory, excluding directories specified via command-line or the default exclude list.\\n3.  Filtering files based on the included and excluded suffixes.\\n4.  Processing the remaining files using the specified AI model (from the environment variable), presumably to generate documentation.\\n5.  Outputting the generated documentation.\\n\", \"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Do not include the code in the output. Explain the purpose of the code in terms of its logic.\\n\\nCode:\\n```python\\nimport typing\\nimport argparse\\nimport os\\n\\n\\nDEFAULT_INCLUDE_SUFFIXES = {\\n    \\\".js\\\",\\n    \\\".json\\\",\\n    \\\".jsx\\\",\\n    \\\".md\\\",\\n    \\\".py\\\",\\n    \\\".pyi\\\",\\n    \\\".pyx\\\",\\n    \\\".rst\\\",\\n    \\\".tf\\\",\\n    \\\".tfvars\\\",\\n    \\\".ts\\\",\\n    \\\".tsx\\\",\\n    \\\".yaml\\\",\\n    \\\".yml\\\",\\n    \\\"Dockerfile\\\",\\n    \\\"Jenkinsfil\\\",\\n}\\n\\nDEFAULT_EXCLUDE_SUFFIXES = {}\\n\\nDEFAULT_EXCLUDE_DIRS = {\\n    \\\".git\\\",\\n    \\\".github\\\",\\n    \\\".terraform\\\",\\n    \\\"assets\\\",\\n    \\\"build\\\",\\n    \\\"dist\\\",\\n    \\\"env\\\",\\n    \\\"docs\\\",\\n    \\\"env\\\",\\n    \\\"images\\\",\\n    \\\"temp\\\",\\n    \\\"tests\\\",\\n    \\\"tmp\\\",\\n    \\\"venv\\\",\\n}\\n\\n\\ndef parse_args() -> typing.Any:\\n    \\\"\\\"\\\"\\n    Parse command line arguments.\\n    :return: Parsed arguments\\n    \\\"\\\"\\\"\\n    parser = argparse.ArgumentParser(\\n        description=\\\"Generate documentation for a project using AI.\\\",\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\n    )\\n\\n    parser.add_argument(\\n        \\\"-n\\\",\\n        \\\"--name\\\",\\n        help=\\\"local Repository name\\\",\\n        default=\\\"repo_name\\\",\\n        )\\n\\n    parser.add_argument(\\n        \\\"-d\\\",\\n        \\\"--directory\\\",\\n        help=\\\"top level directory to scan, default is current working directory\\\",\\n        default=os.getcwd(),\\n        )\\n\\n    parser.add_argument(\\n        \\\"-e\\\",\\n        \\\"--exclude_dirs\\\",\\n        nargs='*',\\n        help=\\\"comma separated list of directories to exclude\\\",\\n        default=\\\",\\\"\\n        )\\n\\n    return parser.parse_args()\\n\\nmodel = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.5-pro-exp-03-25\\\")\\nproject=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"sre-ai-dev\\\")\\n```\": \"The Python code begins by importing necessary modules like `typing`, `argparse`, and `os`. It then defines three sets as constants: `DEFAULT_INCLUDE_SUFFIXES`, `DEFAULT_EXCLUDE_SUFFIXES`, and `DEFAULT_EXCLUDE_DIRS`, containing file extensions to include, file extensions to exclude, and directory names to exclude, respectively.\\n\\nThe core logic resides in the `parse_args` function. This function uses the `argparse` module to define and parse command-line arguments. It defines three arguments: `-n` or `--name` for the repository name (defaulting to \\\"repo_name\\\"), `-d` or `--directory` for the top-level directory to scan (defaulting to the current working directory), and `-e` or `--exclude_dirs` for a comma-separated list of directories to exclude (defaulting to an empty string).  The function returns the parsed arguments object.\\n\\nFinally, the code retrieves environment variables. It attempts to retrieve the value of `GEMINI_MODEL` from the environment, defaulting to \\\"gemini-2.5-pro-exp-03-25\\\" if the environment variable is not set.  Similarly, it attempts to retrieve the value of `GEMINI_PROJECT_ID` from the environment, defaulting to \\\"sre-ai-dev\\\" if the environment variable is not set.\\n\\nIn essence, this code sets up the configuration for a documentation generation tool. It defines default settings for file inclusion and exclusion, allows the user to override these settings via command-line arguments, and fetches necessary configuration values from environment variables. The purpose of the code is to prepare and collect the configuration data required for the rest of the documentation generation process.\\n\", \"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Do not include the code in the output. Explain the purpose of the code in terms of its logic.\\n\\nCode:\\n```python\\nfrom google import genai\\nfrom google.genai import types\\nimport os\\nimport logging\\nimport json\\nfrom datetime import datetime\\n\\n\\n# Configure logging\\nlog_directory = os.getenv(\\\"LOG_DIR\\\", \\\"logs\\\")\\nos.makedirs(log_directory, exist_ok=True)\\nlog_file = os.path.join(\\n    log_directory, f\\\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\\\"\\n)\\n\\n# Set up logger\\nlogger = logging.getLogger(\\\"llm_logger\\\")\\nlogger.setLevel(logging.INFO)\\nlogger.propagate = False  # Prevent propagation to root logger\\nfile_handler = logging.FileHandler(log_file)\\nfile_handler.setFormatter(\\n    logging.Formatter(\\\"%(asctime)s - %(levelname)s - %(message)s\\\")\\n)\\nlogger.addHandler(file_handler)\\n\\n# Simple cache configuration\\ncache_file = \\\"llm_cache.json\\\"\\n\\n\\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\\n    # Log the prompt\\n    logger.info(f\\\"PROMPT: {prompt}\\\")\\n\\n    # Check cache if enabled\\n    if use_cache:\\n        # Load cache from disk\\n        cache = {}\\n        if os.path.exists(cache_file):\\n            try:\\n                with open(cache_file, \\\"r\\\") as f:\\n                    cache = json.load(f)\\n            except:\\n                logger.warning(f\\\"Failed to load cache, starting with empty cache\\\")\\n\\n        # Return from cache if exists\\n        if prompt in cache:\\n            logger.info(f\\\"RESPONSE: {cache[prompt]}\\\")\\n            return cache[prompt]\\n\\n    # Call the LLM if not in cache or cache disabled\\n    client = genai.Client(\\n        vertexai=True,\\n        project=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"ai-sre-dev-84b7\\\"),\\n        location=os.getenv(\\\"GEMINI_LOCATION\\\", \\\"us-central1\\\"),\\n        http_options=types.HttpOptions(api_version='v1')\\n    )\\n\\n    model = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.0-flash-exp\\\")\\n    # for model in client.models.list():\\n    #     print(model)\\n    response = client.models.generate_content(model=model, contents=[prompt])\\n    response_text = response.text\\n\\n    # Log the response\\n    logger.info(f\\\"RESPONSE: {response_text}\\\")\\n\\n    # Update cache if enabled\\n    if use_cache:\\n        # Load cache again to avoid overwrites\\n        cache = {}\\n        if os.path.exists(cache_file):\\n            try:\\n                with open(cache_file, \\\"r\\\") as f:\\n                    cache = json.load(f)\\n            except:\\n                pass\\n\\n        # Add to cache and save\\n        cache[prompt] = response_text\\n        try:\\n            with open(cache_file, \\\"w\\\") as f:\\n                json.dump(cache, f)\\n        except Exception as e:\\n            logger.error(f\\\"Failed to save cache: {e}\\\")\\n\\n    return response_text\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    test_prompt = \\\"Hello, how are you?\\\"\\n\\n    # First call - should hit the API\\n    print(\\\"Making call...\\\")\\n    response1 = call_llm(test_prompt, use_cache=False)\\n    print(f\\\"Response: {response1}\\\")\\n\\n```\": \"The code defines a function `call_llm` that interacts with a Large Language Model (LLM) via the `google.genai` library and implements a simple caching mechanism to reduce API calls.\\n\\nThe script begins by setting up logging to a file named \\\"llm_calls_{date}.log\\\" in a \\\"logs\\\" directory. It configures a logger to write informational messages, including prompts and responses.\\n\\nThe core logic resides within the `call_llm` function. This function accepts a prompt string and a boolean flag, `use_cache`, as input.\\n\\nFirst, it logs the input prompt using the configured logger. Then, if `use_cache` is true, it attempts to load a JSON file named \\\"llm_cache.json\\\" into a dictionary. If the file exists and can be loaded, it checks if the prompt already exists as a key in the dictionary. If the prompt is found in the cache, the cached response is logged and immediately returned, avoiding an LLM API call. If loading the cache fails, a warning is logged, and the process continues as if there was no cache.\\n\\nIf caching is disabled (`use_cache` is false) or the prompt is not found in the cache, the function proceeds to call the LLM API. It initializes a `genai.Client` using environment variables for project ID, location, and model name. It then uses the client to generate content based on the provided prompt. The LLM's response is then stored in the `response_text` variable. This response text is then logged using the logger.\\n\\nAfter receiving the response from the LLM, if `use_cache` is true, the function attempts to update the cache. It loads the cache again (to avoid overwriting any changes made by other concurrent calls).  Then, it adds the current prompt and its corresponding response to the cache dictionary. Finally, it attempts to save the updated cache dictionary back to the \\\"llm_cache.json\\\" file. If saving the cache fails, an error message is logged.\\n\\nFinally, the function returns the LLM's `response_text`.\\n\\nThe `if __name__ == \\\"__main__\\\":` block demonstrates a simple usage of the `call_llm` function.  It defines a test prompt and calls the `call_llm` function with the prompt and `use_cache` set to `False`, demonstrating a direct API call.  The returned response is printed to the console.\\n\", \"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Do not include the code in the output. Explain the purpose of the code in terms of its logic.\\n\\nCode:\\n```python\\n# A script to crawl files from a given directory and its subdirectories.\\n# It uses the `os` module to walk through the directory structure and collects file paths based on specified include and exclude patterns.\\n# files that are not in the exclude patterns and are in the include patterns are collected.\\nimport os\\nimport typing\\nimport logging\\nfrom pathlib import Path\\nfrom settings import DEFAULT_EXCLUDE_SUFFIXES, DEFAULT_INCLUDE_SUFFIXES, DEFAULT_EXCLUDE_DIRS\\n\\n\\n\\ndef crawl_files(\\n    directory: str,\\n    include_patterns: typing.Set[str] = DEFAULT_INCLUDE_SUFFIXES,\\n    exclude_suffixes: typing.Set[str] = DEFAULT_EXCLUDE_SUFFIXES,\\n    exclude_dirs: typing.Set[str] = DEFAULT_EXCLUDE_DIRS,\\n    ) -> typing.List[str]:\\n    try:\\n        files = {\\n            str(item.relative_to(directory))  # Store the relative path string\\n            for item in Path(directory).rglob(\\\"*\\\") # Iterate through all items recursively\\n            if item.is_file() # Consider only files\\n            if item.suffix in include_patterns # Check 0: Suffix is included\\n            and item.suffix not in exclude_suffixes # Check 1: Suffix is not excluded\\n            and not any(part in exclude_dirs for part in item.relative_to(directory).parts) # Check 2: No path part is excluded\\n        }\\n\\n    except ValueError as e:\\n        logging.error(f\\\"Error calculating relative paths: {e}\\\")\\n        logging.error(f\\\"Ensure the search directory '{directory.resolve()}' is within or is the \\\"\\n            f\\\"current working directory '{directory.resolve()}'\\\")\\n\\n    logging.info(f\\\"Found {len(files)} files in {directory}\\\")\\n    logging.debug(f\\\"Files found: {files}\\\")\\n\\n    return files\\n\\n\\ndef main() -> None:\\n    print(\\\"Crawling files...\\\")\\n    directory = os.getcwd()  \\n    print(f\\\"Files crawled from {directory}:\\\")\\n    print (crawl_files(directory))\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n```\": \"The code defines a function `crawl_files` that recursively traverses a directory and its subdirectories to identify files that meet specific criteria. It uses the `os` module and `pathlib` for directory traversal and file system operations.\\n\\nThe `crawl_files` function first takes a directory path as input, along with optional sets of include and exclude file suffixes, and directories to exclude. It walks the directory tree using `Path(directory).rglob(\\\"*\\\")`, retrieving every item within the directory and its subdirectories.\\n\\nFor each item, it checks if it is a file using `item.is_file()`. If it is a file, it further checks if its suffix is present in the `include_patterns` set and absent from the `exclude_suffixes` set. Additionally, it verifies that no part of the relative path is present in the `exclude_dirs` set. The relative path is calculated using `item.relative_to(directory)`. Only the files satisfying all these conditions are collected and stored as relative paths (strings) in a set.\\n\\nThe `main` function calls the `crawl_files` function using the current working directory. Finally, it prints the results of the file crawling operation to the standard output.\\n\\nThe primary data transformations involve converting Path objects to their relative string representations using `str(item.relative_to(directory))`, and building a set of these string representations.\\n\"}": "This script crawls a directory and its subdirectories, identifying files based on specified include/exclude patterns for file suffixes and directories. It uses `os` and `pathlib` for traversal and path manipulation, returning a list of relative file paths. The main function demonstrates usage on the current directory.", "Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Do not include the code in the output. Explain the purpose of the code in terms of its logic.\n\nCode:\n```python\n{\"Summarize this text in 50 words: \\n\\n # A script to crawl files from a given directory and its subdirectories.\\n# It uses the `os` module to walk through the directory structure and collects file paths based on specified include and exclude patterns.\\n# files that are not in the exclude patterns and are in the include patterns are collected.\\nimport os\\nimport typing\\nimport logging\\nfrom pathlib import Path\\nfrom settings import DEFAULT_EXCLUDE_SUFFIXES, DEFAULT_INCLUDE_SUFFIXES, DEFAULT_EXCLUDE_DIRS\\n\\n\\n\\ndef crawl_files(\\n    directory: str,\\n    include_patterns: typing.Set[str] = DEFAULT_INCLUDE_SUFFIXES,\\n    exclude_suffixes: typing.Set[str] = DEFAULT_EXCLUDE_SUFFIXES,\\n    exclude_dirs: typing.Set[str] = DEFAULT_EXCLUDE_DIRS,\\n    ) -> typing.List[str]:\\n    try:\\n        files = {\\n            str(item.relative_to(directory))  # Store the relative path string\\n            for item in Path(directory).rglob(\\\"*\\\") # Iterate through all items recursively\\n            if item.is_file() # Consider only files\\n            if item.suffix in include_patterns # Check 0: Suffix is included\\n            and item.suffix not in exclude_suffixes # Check 1: Suffix is not excluded\\n            and not any(part in exclude_dirs for part in item.relative_to(directory).parts) # Check 2: No path part is excluded\\n        }\\n\\n    except ValueError as e:\\n        logging.error(f\\\"Error calculating relative paths: {e}\\\")\\n        logging.error(f\\\"Ensure the search directory '{directory.resolve()}' is within or is the \\\"\\n            f\\\"current working directory '{directory.resolve()}'\\\")\\n\\n    logging.info(f\\\"Found {len(files)} files in {directory}\\\")\\n    logging.debug(f\\\"Files found: {files}\\\")\\n\\n    return files\\n\\n\\ndef main() -> None:\\n    print(\\\"Crawling files...\\\")\\n    directory = os.getcwd()  \\n    print(f\\\"Files crawled from {directory}:\\\")\\n    print (crawl_files(directory))\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\": \"This script crawls a directory and its subdirectories to find files. It uses `os` to traverse the directory structure and `pathlib` to construct paths. Files are collected based on include and exclude patterns for suffixes and directories, returning a list of relative file paths. A main method demonstrates use of crawl_files from the current directory.\\n\", \"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\\n\\nCode:\\n```python\\n# A script to crawl files from a given directory and its subdirectories.\\n# It uses the `os` module to walk through the directory structure and collects file paths based on specified include and exclude patterns.\\n# files that are not in the exclude patterns and are in the include patterns are collected.\\nimport os\\nimport typing\\nimport logging\\nfrom pathlib import Path\\nfrom settings import DEFAULT_EXCLUDE_SUFFIXES, DEFAULT_INCLUDE_SUFFIXES, DEFAULT_EXCLUDE_DIRS\\n\\n\\n\\ndef crawl_files(\\n    directory: str,\\n    include_patterns: typing.Set[str] = DEFAULT_INCLUDE_SUFFIXES,\\n    exclude_suffixes: typing.Set[str] = DEFAULT_EXCLUDE_SUFFIXES,\\n    exclude_dirs: typing.Set[str] = DEFAULT_EXCLUDE_DIRS,\\n    ) -> typing.List[str]:\\n    try:\\n        files = {\\n            str(item.relative_to(directory))  # Store the relative path string\\n            for item in Path(directory).rglob(\\\"*\\\") # Iterate through all items recursively\\n            if item.is_file() # Consider only files\\n            if item.suffix in include_patterns # Check 0: Suffix is included\\n            and item.suffix not in exclude_suffixes # Check 1: Suffix is not excluded\\n            and not any(part in exclude_dirs for part in item.relative_to(directory).parts) # Check 2: No path part is excluded\\n        }\\n\\n    except ValueError as e:\\n        logging.error(f\\\"Error calculating relative paths: {e}\\\")\\n        logging.error(f\\\"Ensure the search directory '{directory.resolve()}' is within or is the \\\"\\n            f\\\"current working directory '{directory.resolve()}'\\\")\\n\\n    logging.info(f\\\"Found {len(files)} files in {directory}\\\")\\n    logging.debug(f\\\"Files found: {files}\\\")\\n\\n    return files\\n\\n\\ndef main() -> None:\\n    print(\\\"Crawling files...\\\")\\n    directory = os.getcwd()  \\n    print(f\\\"Files crawled from {directory}:\\\")\\n    print (crawl_files(directory))\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n```\": \"```python\\n# A script to crawl files from a given directory and its subdirectories.\\n# It uses the `os` module to walk through the directory structure and collects file paths based on specified include and exclude patterns.\\n# files that are not in the exclude patterns and are in the include patterns are collected.\\nimport os\\nimport typing\\nimport logging\\nfrom pathlib import Path\\nfrom settings import DEFAULT_EXCLUDE_SUFFIXES, DEFAULT_INCLUDE_SUFFIXES, DEFAULT_EXCLUDE_DIRS\\n\\n\\n\\ndef crawl_files(\\n    directory: str,\\n    include_patterns: typing.Set[str] = DEFAULT_INCLUDE_SUFFIXES,\\n    exclude_suffixes: typing.Set[str] = DEFAULT_EXCLUDE_SUFFIXES,\\n    exclude_dirs: typing.Set[str] = DEFAULT_EXCLUDE_DIRS,\\n    ) -> typing.List[str]:\\n    \\\"\\\"\\\"\\n    Crawls files from a given directory and its subdirectories based on specified include and exclude patterns.\\n\\n    Args:\\n        directory (str): The directory to crawl.\\n        include_patterns (typing.Set[str], optional): A set of file suffixes to include. Defaults to DEFAULT_INCLUDE_SUFFIXES.\\n        exclude_suffixes (typing.Set[str], optional): A set of file suffixes to exclude. Defaults to DEFAULT_EXCLUDE_SUFFIXES.\\n        exclude_dirs (typing.Set[str], optional): A set of directory names to exclude. Defaults to DEFAULT_EXCLUDE_DIRS.\\n\\n    Returns:\\n        typing.List[str]: A list of file paths relative to the given directory.\\n    \\\"\\\"\\\"\\n    try:\\n        files = {\\n            str(item.relative_to(directory))  # Store the relative path string\\n            for item in Path(directory).rglob(\\\"*\\\") # Iterate through all items recursively\\n            if item.is_file() # Consider only files\\n            if item.suffix in include_patterns # Check 0: Suffix is included\\n            and item.suffix not in exclude_suffixes # Check 1: Suffix is not excluded\\n            and not any(part in exclude_dirs for part in item.relative_to(directory).parts) # Check 2: No path part is excluded\\n        }\\n\\n    except ValueError as e:\\n        logging.error(f\\\"Error calculating relative paths: {e}\\\")\\n        logging.error(f\\\"Ensure the search directory '{directory.resolve()}' is within or is the \\\"\\n            f\\\"current working directory '{directory.resolve()}'\\\")\\n\\n    logging.info(f\\\"Found {len(files)} files in {directory}\\\")\\n    logging.debug(f\\\"Files found: {files}\\\")\\n\\n    return files\\n\\n\\ndef main() -> None:\\n    \\\"\\\"\\\"\\n    Main function to crawl files from the current working directory and print the results.\\n    \\\"\\\"\\\"\\n    print(\\\"Crawling files...\\\")\\n    directory = os.getcwd()  \\n    print(f\\\"Files crawled from {directory}:\\\")\\n    print (crawl_files(directory))\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n```\\n\\n**Algorithm and Logic Flow Analysis:**\\n\\n1.  **`crawl_files` Function:**\\n    *   **Purpose:** This function is the core of the script. It recursively crawls a directory and its subdirectories to find files that match specific criteria (include and exclude patterns for file suffixes and directories).\\n    *   **Input:**\\n        *   `directory` (str): The path to the directory to crawl.\\n        *   `include_patterns` (typing.Set\\\\[str]): A set of file suffixes to include (e.g., `'.txt'`, `'.py'`).\\n        *   `exclude_suffixes` (typing.Set\\\\[str]): A set of file suffixes to exclude (e.g., `'.log'`, `'.tmp'`).\\n        *   `exclude_dirs` (typing.Set\\\\[str]): A set of directory names to exclude (e.g., `'venv'`, `'__pycache__'` ).\\n    *   **`Path(directory).rglob(\\\"*\\\")`**: This creates a `Path` object representing the input `directory`. The `rglob(\\\"*\\\")` method recursively finds all files and subdirectories within the directory.  It returns a generator that yields `Path` objects representing each item.\\n    *   **Set Comprehension**:  The heart of the function uses a set comprehension.  For each `item` (a `Path` object) found by `rglob`:\\n        *   **`item.is_file()`**: It checks if the item is a file. Only files are considered.\\n        *   **`item.suffix in include_patterns`**:  It checks if the file's suffix (e.g., `'.txt'`) is present in the `include_patterns` set. This implements the \\\"include\\\" filter.\\n        *   **`item.suffix not in exclude_suffixes`**: It checks if the file's suffix is *not* present in the `exclude_suffixes` set. This implements the \\\"exclude\\\" filter based on suffix.\\n        *   **`not any(part in exclude_dirs for part in item.relative_to(directory).parts)`**:  This is the directory exclusion logic. `item.relative_to(directory)` calculates the relative path from the input directory to the current file.  `.parts` splits the relative path into its directory components.  The `any()` function then checks if any of these directory components are present in the `exclude_dirs` set.  If any part of the path is in `exclude_dirs`, the file is excluded.\\n        *   **`str(item.relative_to(directory))`**: If all the conditions above are met (the item is a file, its suffix is included, its suffix is not excluded, and none of its directory components are excluded), the relative path of the file (as a string) is added to the `files` set.  Using a set ensures that the result contains no duplicates.\\n    *   **Error Handling**: A `try...except` block catches `ValueError` exceptions, which can occur if `item` is not within the `directory`. A logging message is raised if this happens.\\n    *   **Logging**:  The code uses the `logging` module to log the number of files found and the list of files (at the debug level).\\n    *   **Output:** Returns a `typing.List[str]` containing the relative paths (strings) of all the files that meet the inclusion/exclusion criteria.\\n\\n2.  **`main` Function:**\\n    *   **Purpose:** This function is the entry point of the script.\\n    *   It gets the current working directory using `os.getcwd()`.\\n    *   It calls the `crawl_files` function with the current working directory as input.\\n    *   It prints the results returned by `crawl_files`.\\n\\n3.  **`if __name__ == \\\"__main__\\\":` Block:**\\n    *   This standard Python idiom ensures that the `main` function is called only when the script is executed directly (not when it is imported as a module).\\n\\n**In Summary:**\\n\\nThe code's main algorithm is to recursively traverse a directory, filter files based on include and exclude patterns applied to file suffixes and directory names, and return a list of the relative paths of the matching files.  The set comprehension provides a concise way to combine the directory traversal, filtering, and path transformation into a single expression. The inclusion and exclusion mechanisms give fine-grained control over the files that are collected.  Error handling is included to address potential issues with relative path calculations. Logging provides information about the process and the results.\\n\", \"Summarize this text in 50 words: \\n\\n from google import genai\\nfrom google.genai import types\\nimport os\\nimport logging\\nimport json\\nfrom datetime import datetime\\n\\n\\n# Configure logging\\nlog_directory = os.getenv(\\\"LOG_DIR\\\", \\\"logs\\\")\\nos.makedirs(log_directory, exist_ok=True)\\nlog_file = os.path.join(\\n    log_directory, f\\\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\\\"\\n)\\n\\n# Set up logger\\nlogger = logging.getLogger(\\\"llm_logger\\\")\\nlogger.setLevel(logging.INFO)\\nlogger.propagate = False  # Prevent propagation to root logger\\nfile_handler = logging.FileHandler(log_file)\\nfile_handler.setFormatter(\\n    logging.Formatter(\\\"%(asctime)s - %(levelname)s - %(message)s\\\")\\n)\\nlogger.addHandler(file_handler)\\n\\n# Simple cache configuration\\ncache_file = \\\"llm_cache.json\\\"\\n\\n\\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\\n    # Log the prompt\\n    logger.info(f\\\"PROMPT: {prompt}\\\")\\n\\n    # Check cache if enabled\\n    if use_cache:\\n        # Load cache from disk\\n        cache = {}\\n        if os.path.exists(cache_file):\\n            try:\\n                with open(cache_file, \\\"r\\\") as f:\\n                    cache = json.load(f)\\n            except:\\n                logger.warning(f\\\"Failed to load cache, starting with empty cache\\\")\\n\\n        # Return from cache if exists\\n        if prompt in cache:\\n            logger.info(f\\\"RESPONSE: {cache[prompt]}\\\")\\n            return cache[prompt]\\n\\n    # Call the LLM if not in cache or cache disabled\\n    client = genai.Client(\\n        vertexai=True,\\n        project=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"ai-sre-dev-84b7\\\"),\\n        location=os.getenv(\\\"GEMINI_LOCATION\\\", \\\"us-central1\\\"),\\n        http_options=types.HttpOptions(api_version='v1')\\n    )\\n\\n    model = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.0-flash-exp\\\")\\n    # for model in client.models.list():\\n    #     print(model)\\n    response = client.models.generate_content(model=model, contents=[prompt])\\n    response_text = response.text\\n\\n    # Log the response\\n    logger.info(f\\\"RESPONSE: {response_text}\\\")\\n\\n    # Update cache if enabled\\n    if use_cache:\\n        # Load cache again to avoid overwrites\\n        cache = {}\\n        if os.path.exists(cache_file):\\n            try:\\n                with open(cache_file, \\\"r\\\") as f:\\n                    cache = json.load(f)\\n            except:\\n                pass\\n\\n        # Add to cache and save\\n        cache[prompt] = response_text\\n        try:\\n            with open(cache_file, \\\"w\\\") as f:\\n                json.dump(cache, f)\\n        except Exception as e:\\n            logger.error(f\\\"Failed to save cache: {e}\\\")\\n\\n    return response_text\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    test_prompt = \\\"Hello, how are you?\\\"\\n\\n    # First call - should hit the API\\n    print(\\\"Making call...\\\")\\n    response1 = call_llm(test_prompt, use_cache=False)\\n    print(f\\\"Response: {response1}\\\")\\n\": \"This code defines a function `call_llm` that interacts with a Google LLM (Gemini) API.  It logs prompts and responses, utilizes a JSON file for caching responses to avoid redundant API calls, and includes error handling for loading/saving the cache. The main block calls the function.\\n\", \"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\\n\\nCode:\\n```python\\nfrom google import genai\\nfrom google.genai import types\\nimport os\\nimport logging\\nimport json\\nfrom datetime import datetime\\n\\n\\n# Configure logging\\nlog_directory = os.getenv(\\\"LOG_DIR\\\", \\\"logs\\\")\\nos.makedirs(log_directory, exist_ok=True)\\nlog_file = os.path.join(\\n    log_directory, f\\\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\\\"\\n)\\n\\n# Set up logger\\nlogger = logging.getLogger(\\\"llm_logger\\\")\\nlogger.setLevel(logging.INFO)\\nlogger.propagate = False  # Prevent propagation to root logger\\nfile_handler = logging.FileHandler(log_file)\\nfile_handler.setFormatter(\\n    logging.Formatter(\\\"%(asctime)s - %(levelname)s - %(message)s\\\")\\n)\\nlogger.addHandler(file_handler)\\n\\n# Simple cache configuration\\ncache_file = \\\"llm_cache.json\\\"\\n\\n\\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\\n    # Log the prompt\\n    logger.info(f\\\"PROMPT: {prompt}\\\")\\n\\n    # Check cache if enabled\\n    if use_cache:\\n        # Load cache from disk\\n        cache = {}\\n        if os.path.exists(cache_file):\\n            try:\\n                with open(cache_file, \\\"r\\\") as f:\\n                    cache = json.load(f)\\n            except:\\n                logger.warning(f\\\"Failed to load cache, starting with empty cache\\\")\\n\\n        # Return from cache if exists\\n        if prompt in cache:\\n            logger.info(f\\\"RESPONSE: {cache[prompt]}\\\")\\n            return cache[prompt]\\n\\n    # Call the LLM if not in cache or cache disabled\\n    client = genai.Client(\\n        vertexai=True,\\n        project=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"ai-sre-dev-84b7\\\"),\\n        location=os.getenv(\\\"GEMINI_LOCATION\\\", \\\"us-central1\\\"),\\n        http_options=types.HttpOptions(api_version='v1')\\n    )\\n\\n    model = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.0-flash-exp\\\")\\n    # for model in client.models.list():\\n    #     print(model)\\n    response = client.models.generate_content(model=model, contents=[prompt])\\n    response_text = response.text\\n\\n    # Log the response\\n    logger.info(f\\\"RESPONSE: {response_text}\\\")\\n\\n    # Update cache if enabled\\n    if use_cache:\\n        # Load cache again to avoid overwrites\\n        cache = {}\\n        if os.path.exists(cache_file):\\n            try:\\n                with open(cache_file, \\\"r\\\") as f:\\n                    cache = json.load(f)\\n            except:\\n                pass\\n\\n        # Add to cache and save\\n        cache[prompt] = response_text\\n        try:\\n            with open(cache_file, \\\"w\\\") as f:\\n                json.dump(cache, f)\\n        except Exception as e:\\n            logger.error(f\\\"Failed to save cache: {e}\\\")\\n\\n    return response_text\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    test_prompt = \\\"Hello, how are you?\\\"\\n\\n    # First call - should hit the API\\n    print(\\\"Making call...\\\")\\n    response1 = call_llm(test_prompt, use_cache=False)\\n    print(f\\\"Response: {response1}\\\")\\n\\n```\": \"```python\\nfrom google import genai\\nfrom google.genai import types\\nimport os\\nimport logging\\nimport json\\nfrom datetime import datetime\\n\\n\\n# Configure logging\\nlog_directory = os.getenv(\\\"LOG_DIR\\\", \\\"logs\\\")\\nos.makedirs(log_directory, exist_ok=True)\\nlog_file = os.path.join(\\n    log_directory, f\\\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\\\"\\n)\\n\\n# Set up logger\\nlogger = logging.getLogger(\\\"llm_logger\\\")\\nlogger.setLevel(logging.INFO)\\nlogger.propagate = False  # Prevent propagation to root logger\\nfile_handler = logging.FileHandler(log_file)\\nfile_handler.setFormatter(\\n    logging.Formatter(\\\"%(asctime)s - %(levelname)s - %(message)s\\\")\\n)\\nlogger.addHandler(file_handler)\\n\\n# Simple cache configuration\\ncache_file = \\\"llm_cache.json\\\"\\n\\n\\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\\n    # Log the prompt\\n    logger.info(f\\\"PROMPT: {prompt}\\\")\\n\\n    # Check cache if enabled\\n    if use_cache:\\n        # Load cache from disk\\n        cache = {}\\n        if os.path.exists(cache_file):\\n            try:\\n                with open(cache_file, \\\"r\\\") as f:\\n                    cache = json.load(f)\\n            except:\\n                logger.warning(f\\\"Failed to load cache, starting with empty cache\\\")\\n\\n        # Return from cache if exists\\n        if prompt in cache:\\n            logger.info(f\\\"RESPONSE: {cache[prompt]}\\\")\\n            return cache[prompt]\\n\\n    # Call the LLM if not in cache or cache disabled\\n    client = genai.Client(\\n        vertexai=True,\\n        project=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"ai-sre-dev-84b7\\\"),\\n        location=os.getenv(\\\"GEMINI_LOCATION\\\", \\\"us-central1\\\"),\\n        http_options=types.HttpOptions(api_version='v1')\\n    )\\n\\n    model = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.0-flash-exp\\\")\\n    # for model in client.models.list():\\n    #     print(model)\\n    response = client.models.generate_content(model=model, contents=[prompt])\\n    response_text = response.text\\n\\n    # Log the response\\n    logger.info(f\\\"RESPONSE: {response_text}\\\")\\n\\n    # Update cache if enabled\\n    if use_cache:\\n        # Load cache again to avoid overwrites\\n        cache = {}\\n        if os.path.exists(cache_file):\\n            try:\\n                with open(cache_file, \\\"r\\\") as f:\\n                    cache = json.load(f)\\n            except:\\n                pass\\n\\n        # Add to cache and save\\n        cache[prompt] = response_text\\n        try:\\n            with open(cache_file, \\\"w\\\") as f:\\n                json.dump(cache, f)\\n        except Exception as e:\\n            logger.error(f\\\"Failed to save cache: {e}\\\")\\n\\n    return response_text\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    test_prompt = \\\"Hello, how are you?\\\"\\n\\n    # First call - should hit the API\\n    print(\\\"Making call...\\\")\\n    response1 = call_llm(test_prompt, use_cache=False)\\n    print(f\\\"Response: {response1}\\\")\\n```\\n\\n**Overall Purpose:**\\n\\nThe Python code defines a function `call_llm` that interacts with a Large Language Model (LLM) from Google's `genai` library. It provides functionality for caching responses to avoid redundant API calls.  The main goal is to simplify querying the LLM while incorporating logging and a basic caching mechanism.\\n\\n**Detailed Analysis of Logic Flow:**\\n\\n1. **Initialization:**\\n   - Imports necessary libraries: `google.genai`, `os`, `logging`, `json`, and `datetime`.\\n   - Configures logging:\\n     - Sets up a directory for logs (defaulting to \\\"logs\\\").\\n     - Creates a log file named with the current date (e.g., `llm_calls_20241027.log`).\\n     - Configures a logger to write messages to the log file with timestamps and severity levels.  The logger is configured to *not* propagate to the root logger, meaning it operates independently.\\n   - Defines the cache file name: `llm_cache.json`.\\n\\n2. **`call_llm(prompt: str, use_cache: bool = True) -> str` Function:**\\n   - **Input:**\\n     - `prompt` (str): The text prompt to send to the LLM.\\n     - `use_cache` (bool):  A flag indicating whether to use the cache (defaults to `True`).\\n   - **Output:**\\n     - (str): The LLM's response to the prompt.\\n   - **Logic:**\\n     - **Logging:** Logs the incoming `prompt` to the configured logger.\\n     - **Cache Check (if `use_cache` is True):**\\n       - Loads the cache from `llm_cache.json`. If the file doesn't exist or if loading fails, an empty dictionary is used.  A warning is logged if the cache fails to load.\\n       - Checks if the `prompt` already exists as a key in the `cache` dictionary.\\n       - If the prompt is found in the cache, the corresponding cached response is logged and returned directly, avoiding an LLM call.\\n     - **LLM Call (if not in cache or `use_cache` is False):**\\n       - Initializes the `genai` client using environment variables for project ID and location. It uses Vertex AI as the backend.  It configures the `api_version` to 'v1' through `HttpOptions`.\\n       - Sets the `model` name using an environment variable.\\n       - Calls `client.models.generate_content(model=model, contents=[prompt])` to send the prompt to the LLM.\\n       - Extracts the response text from the LLM's response.\\n     - **Logging:** Logs the LLM's response to the logger.\\n     - **Cache Update (if `use_cache` is True):**\\n       - Reloads the cache from disk *again*. This is done to prevent overwriting any changes made to the cache by other processes since the initial load.\\n       - Adds the `prompt` and the LLM's `response_text` to the `cache` dictionary.\\n       - Saves the updated `cache` to `llm_cache.json`. If saving fails, an error is logged.\\n     - **Return:** Returns the LLM's `response_text`.\\n\\n3. **`if __name__ == \\\"__main__\\\":` Block:**\\n   - This block is executed only when the script is run directly (not when imported as a module).\\n   - Defines a `test_prompt`.\\n   - Calls the `call_llm` function with `use_cache=False` to force an API call to the LLM.\\n   - Prints the returned response.\\n\\n**Control Structures:**\\n\\n- **`if use_cache:`:** Conditional statement to enable or disable the cache functionality.\\n- **`if os.path.exists(cache_file):`:**  Conditional statement to check if the cache file exists before attempting to load it.\\n- **`try...except:`:** Error handling blocks to catch exceptions during cache loading and saving.\\n\\n**Data Transformations:**\\n\\n- `datetime.now().strftime('%Y%m%d')`: Converts the current date and time into a string format (YYYYMMDD) for use in the log file name.\\n- `json.load(f)`: Parses the JSON content of the cache file into a Python dictionary.\\n- `json.dump(cache, f)`: Serializes the Python dictionary `cache` into JSON format and writes it to the cache file.\\n\\n**In summary, the code defines a `call_llm` function that intelligently queries a Large Language Model. It uses a JSON file as a simple cache to store and retrieve LLM responses based on the input prompt, minimizing redundant API calls.  It also includes comprehensive logging of prompts and responses for debugging and auditing purposes.**\\n\", \"Summarize this text in 50 words: \\n\\n import logging\\nfrom pathlib import Path\\nfrom pocketflow import Node\\nfrom call_llm import call_llm\\nfrom settings import (\\n    DEFAULT_EXCLUDE_SUFFIXES,\\n    DEFAULT_INCLUDE_SUFFIXES,\\n    DEFAULT_EXCLUDE_DIRS,\\n)\\nfrom settings import parse_args\\nimport fetch_files\\n\\n\\nclass Summarise(Node):\\n    \\\"\\\"\\\"\\n    This class identifies abstractions in the code.\\n    \\\"\\\"\\\"\\n\\n    def prep(self,shared):\\n        return file_contents[\\\"data\\\"]\\n    \\n    def exec(self, text):\\n        prompt = f\\\"Summarize this text in 50 words: \\\\n\\\\n {text}\\\"\\n        return call_llm(prompt)\\n    \\n    def post(self, shared, prep_res, exec_res):\\n        # Store the result in the shared settings\\n        file_contents[\\\"summaries\\\"] = exec_res\\n        return \\\"default\\\"\\n\\n\\n# New Node for Logic Flow Analysis\\nclass AnalyzeLogicFlow(Node):\\n    \\\"\\\"\\\"\\n    This class analyzes the code to describe its main algorithm or logic flow.\\n    \\\"\\\"\\\"\\n\\n    # prep and post might not be needed if main loop handles orchestration\\n    # def prep(self, shared):\\n    #     pass\\n\\n    def exec(self, text):\\n        prompt = f\\\"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\\\\n\\\\nCode:\\\\n```python\\\\n{text}\\\\n```\\\"\\n        # Assuming call_llm handles potential errors and returns a string\\n        return call_llm(prompt)\\n\\n    # def post(self, shared, prep_res, exec_res):\\n    #     pass\\n\\n\\ndef main() -> None:\\n    args = parse_args()\\n\\n    # Create a dictionary of settings to be used in the script\\n    shared_settings = {\\n        \\\"name\\\": args.name,\\n        \\\"directory\\\": args.directory, # Target directory to analyze\\n        \\\"exclude_patterns\\\": DEFAULT_EXCLUDE_SUFFIXES,\\n        \\\"exclude_dirs\\\": DEFAULT_EXCLUDE_DIRS,\\n        \\\"include_suffixes\\\": DEFAULT_INCLUDE_SUFFIXES,\\n        \\\"additional_exclude_dirs\\\": args.exclude_dirs,\\n    }\\n    logging.debug(f\\\"Shared settings: {shared_settings}\\\")\\n\\n    # Get a list of files from the target directory\\n    # Ensure paths are relative to the target directory for correct output structure\\n    target_dir = Path(shared_settings[\\\"directory\\\"]).resolve()\\n    logging.info(f\\\"Analyzing directory: {target_dir}\\\")\\n\\n    # fetch_files.crawl_files already returns relative paths if called correctly\\n    # Let's ensure the base directory passed to crawl_files is the target directory\\n    files_to_process = fetch_files.crawl_files(\\n        directory=str(target_dir), # Pass absolute path to crawl\\n        include_patterns=shared_settings[\\\"include_suffixes\\\"],\\n        exclude_suffixes=shared_settings[\\\"exclude_patterns\\\"],\\n        exclude_dirs=shared_settings[\\\"exclude_dirs\\\"].union(\\n            shared_settings[\\\"additional_exclude_dirs\\\"]\\n        ),\\n    )\\n    \\n    # Instantiate nodes\\n    summariser = Summarise()\\n    logic_analyzer = AnalyzeLogicFlow()\\n\\n    results = {}\\n    logging.info(f\\\"Processing {len(files_to_process)} files...\\\")\\n\\n    # Process each file\\n    for relative_file_path in files_to_process:\\n        absolute_file_path = target_dir / relative_file_path\\n        logging.debug(f\\\"Processing file: {absolute_file_path}\\\")\\n        print(f\\\"Processing: {relative_file_path}\\\") # User feedback\\n        try:\\n            with open(absolute_file_path, \\\"r\\\", encoding='utf-8') as f:\\n                content = f.read()\\n                if content.strip(): # Ensure content is not empty\\n                    # Call LLM for summary\\n                    summary = summariser.exec(content)\\n                    # Call LLM for logic flow\\n                    logic_flow = logic_analyzer.exec(content)\\n                    results[relative_file_path] = {\\\"summary\\\": summary, \\\"logic_flow\\\": logic_flow}\\n                else:\\n                    logging.warning(f\\\"Skipped empty file: {absolute_file_path}\\\")\\n                    print(f\\\"Skipped empty file: {relative_file_path}\\\")\\n        except Exception as e:\\n            logging.error(f\\\"Error processing file {absolute_file_path}: {e}\\\")\\n            print(f\\\"Error processing file {relative_file_path}: {e}\\\")\\n\\n    # Generate Markdown output\\n    output_base_dir = target_dir / \\\"docs\\\"\\n    logging.info(f\\\"Generating documentation in: {output_base_dir}\\\")\\n    try:\\n        output_base_dir.mkdir(parents=True, exist_ok=True)\\n    except Exception as e:\\n        logging.error(f\\\"Failed to create output directory {output_base_dir}: {e}\\\")\\n        print(f\\\"Error: Could not create output directory {output_base_dir}. Aborting.\\\")\\n        return # Exit if we can't create the output dir\\n\\n    for relative_file_path, data in results.items():\\n        md_filename = Path(relative_file_path).with_suffix(\\\".md\\\")\\n        md_path = output_base_dir / md_filename\\n        \\n        try:\\n            # Ensure subdirectory exists\\n            md_path.parent.mkdir(parents=True, exist_ok=True)\\n\\n            # Format Markdown content\\n            md_content = f\\\"\\\"\\\"# File: {relative_file_path}\\n\\n## Summary\\n\\n{data['summary']}\\n\\n## Logic Flow\\n\\n{data['logic_flow']}\\n\\\"\\\"\\\"\\n            # Write the Markdown file\\n            with open(md_path, \\\"w\\\", encoding='utf-8') as f:\\n                f.write(md_content)\\n            logging.info(f\\\"Generated: {md_path}\\\")\\n            print(f\\\"Generated: {md_path.relative_to(target_dir)}\\\") # Show relative path to user\\n\\n        except Exception as e:\\n            logging.error(f\\\"Failed to write Markdown file {md_path}: {e}\\\")\\n            print(f\\\"Error writing file {md_path.relative_to(target_dir)}: {e}\\\")\\n\\n    print(f\\\"\\\\nDocumentation generation complete. Files written to: {output_base_dir}\\\")\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n\": \"This Python script analyzes code in a directory, generating summaries and logic flow descriptions for each file using a large language model (LLM). It uses `Summarise` and `AnalyzeLogicFlow` nodes, crawls files, processes them, and outputs results in Markdown format to a \\\"docs\\\" subdirectory within the target directory.\\n\", \"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\\n\\nCode:\\n```python\\nimport logging\\nfrom pathlib import Path\\nfrom pocketflow import Node\\nfrom call_llm import call_llm\\nfrom settings import (\\n    DEFAULT_EXCLUDE_SUFFIXES,\\n    DEFAULT_INCLUDE_SUFFIXES,\\n    DEFAULT_EXCLUDE_DIRS,\\n)\\nfrom settings import parse_args\\nimport fetch_files\\n\\n\\nclass Summarise(Node):\\n    \\\"\\\"\\\"\\n    This class identifies abstractions in the code.\\n    \\\"\\\"\\\"\\n\\n    def prep(self,shared):\\n        return file_contents[\\\"data\\\"]\\n    \\n    def exec(self, text):\\n        prompt = f\\\"Summarize this text in 50 words: \\\\n\\\\n {text}\\\"\\n        return call_llm(prompt)\\n    \\n    def post(self, shared, prep_res, exec_res):\\n        # Store the result in the shared settings\\n        file_contents[\\\"summaries\\\"] = exec_res\\n        return \\\"default\\\"\\n\\n\\n# New Node for Logic Flow Analysis\\nclass AnalyzeLogicFlow(Node):\\n    \\\"\\\"\\\"\\n    This class analyzes the code to describe its main algorithm or logic flow.\\n    \\\"\\\"\\\"\\n\\n    # prep and post might not be needed if main loop handles orchestration\\n    # def prep(self, shared):\\n    #     pass\\n\\n    def exec(self, text):\\n        prompt = f\\\"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\\\\n\\\\nCode:\\\\n```python\\\\n{text}\\\\n```\\\"\\n        # Assuming call_llm handles potential errors and returns a string\\n        return call_llm(prompt)\\n\\n    # def post(self, shared, prep_res, exec_res):\\n    #     pass\\n\\n\\ndef main() -> None:\\n    args = parse_args()\\n\\n    # Create a dictionary of settings to be used in the script\\n    shared_settings = {\\n        \\\"name\\\": args.name,\\n        \\\"directory\\\": args.directory, # Target directory to analyze\\n        \\\"exclude_patterns\\\": DEFAULT_EXCLUDE_SUFFIXES,\\n        \\\"exclude_dirs\\\": DEFAULT_EXCLUDE_DIRS,\\n        \\\"include_suffixes\\\": DEFAULT_INCLUDE_SUFFIXES,\\n        \\\"additional_exclude_dirs\\\": args.exclude_dirs,\\n    }\\n    logging.debug(f\\\"Shared settings: {shared_settings}\\\")\\n\\n    # Get a list of files from the target directory\\n    # Ensure paths are relative to the target directory for correct output structure\\n    target_dir = Path(shared_settings[\\\"directory\\\"]).resolve()\\n    logging.info(f\\\"Analyzing directory: {target_dir}\\\")\\n\\n    # fetch_files.crawl_files already returns relative paths if called correctly\\n    # Let's ensure the base directory passed to crawl_files is the target directory\\n    files_to_process = fetch_files.crawl_files(\\n        directory=str(target_dir), # Pass absolute path to crawl\\n        include_patterns=shared_settings[\\\"include_suffixes\\\"],\\n        exclude_suffixes=shared_settings[\\\"exclude_patterns\\\"],\\n        exclude_dirs=shared_settings[\\\"exclude_dirs\\\"].union(\\n            shared_settings[\\\"additional_exclude_dirs\\\"]\\n        ),\\n    )\\n    \\n    # Instantiate nodes\\n    summariser = Summarise()\\n    logic_analyzer = AnalyzeLogicFlow()\\n\\n    results = {}\\n    logging.info(f\\\"Processing {len(files_to_process)} files...\\\")\\n\\n    # Process each file\\n    for relative_file_path in files_to_process:\\n        absolute_file_path = target_dir / relative_file_path\\n        logging.debug(f\\\"Processing file: {absolute_file_path}\\\")\\n        print(f\\\"Processing: {relative_file_path}\\\") # User feedback\\n        try:\\n            with open(absolute_file_path, \\\"r\\\", encoding='utf-8') as f:\\n                content = f.read()\\n                if content.strip(): # Ensure content is not empty\\n                    # Call LLM for summary\\n                    summary = summariser.exec(content)\\n                    # Call LLM for logic flow\\n                    logic_flow = logic_analyzer.exec(content)\\n                    results[relative_file_path] = {\\\"summary\\\": summary, \\\"logic_flow\\\": logic_flow}\\n                else:\\n                    logging.warning(f\\\"Skipped empty file: {absolute_file_path}\\\")\\n                    print(f\\\"Skipped empty file: {relative_file_path}\\\")\\n        except Exception as e:\\n            logging.error(f\\\"Error processing file {absolute_file_path}: {e}\\\")\\n            print(f\\\"Error processing file {relative_file_path}: {e}\\\")\\n\\n    # Generate Markdown output\\n    output_base_dir = target_dir / \\\"docs\\\"\\n    logging.info(f\\\"Generating documentation in: {output_base_dir}\\\")\\n    try:\\n        output_base_dir.mkdir(parents=True, exist_ok=True)\\n    except Exception as e:\\n        logging.error(f\\\"Failed to create output directory {output_base_dir}: {e}\\\")\\n        print(f\\\"Error: Could not create output directory {output_base_dir}. Aborting.\\\")\\n        return # Exit if we can't create the output dir\\n\\n    for relative_file_path, data in results.items():\\n        md_filename = Path(relative_file_path).with_suffix(\\\".md\\\")\\n        md_path = output_base_dir / md_filename\\n        \\n        try:\\n            # Ensure subdirectory exists\\n            md_path.parent.mkdir(parents=True, exist_ok=True)\\n\\n            # Format Markdown content\\n            md_content = f\\\"\\\"\\\"# File: {relative_file_path}\\n\\n## Summary\\n\\n{data['summary']}\\n\\n## Logic Flow\\n\\n{data['logic_flow']}\\n\\\"\\\"\\\"\\n            # Write the Markdown file\\n            with open(md_path, \\\"w\\\", encoding='utf-8') as f:\\n                f.write(md_content)\\n            logging.info(f\\\"Generated: {md_path}\\\")\\n            print(f\\\"Generated: {md_path.relative_to(target_dir)}\\\") # Show relative path to user\\n\\n        except Exception as e:\\n            logging.error(f\\\"Failed to write Markdown file {md_path}: {e}\\\")\\n            print(f\\\"Error writing file {md_path.relative_to(target_dir)}: {e}\\\")\\n\\n    print(f\\\"\\\\nDocumentation generation complete. Files written to: {output_base_dir}\\\")\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n\\n```\": \"Okay, let's break down the Python code's algorithm and logic flow.\\n\\n**Purpose:**\\n\\nThe code aims to automatically generate documentation for Python code files in a given directory. It uses a Large Language Model (LLM) to create summaries and analyze the logic flow of each file.  The results are then saved as Markdown files within a `docs` subdirectory of the original directory.\\n\\n**Algorithm/Logic Flow:**\\n\\n1.  **Initialization and Argument Parsing:**\\n    *   The `main()` function starts by parsing command-line arguments using `parse_args()`.  The expected arguments probably include the target directory (`--directory`) and potentially exclude directories (`--exclude_dirs`).\\n    *   Default settings are defined from the `settings` module:\\n        *   `DEFAULT_EXCLUDE_SUFFIXES`:  File extensions to exclude (e.g., `.pyc`, `.log`).\\n        *   `DEFAULT_INCLUDE_SUFFIXES`: File extensions to include (e.g., `.py`).\\n        *   `DEFAULT_EXCLUDE_DIRS`: Directories to exclude (e.g., `.git`, `__pycache__`).\\n    *   A `shared_settings` dictionary is created, combining the command-line arguments and the default values. This dictionary configures the file processing.\\n\\n2.  **File Discovery:**\\n    *   The code resolves the target directory path using `Path(shared_settings[\\\"directory\\\"]).resolve()`.\\n    *   The `fetch_files.crawl_files()` function is called to generate a list of Python files to process.  This function recursively crawls the target directory, filtering files based on the include/exclude patterns defined in `shared_settings`. Crucially, `crawl_files` is expected to return *relative* paths of files, *relative to the target directory*.\\n    * The code ensures that it passes the absolute path of the target directory to `crawl_files`.\\n3.  **Node Instantiation:**\\n    *   Two node objects `Summarise` and `AnalyzeLogicFlow` are created. These nodes leverage the `pocketflow` framework and encapsulate the LLM call logic.\\n\\n4.  **File Processing Loop:**\\n    *   The code iterates through each file found by `crawl_files()`. The loop performs the core analysis for each file.\\n    *   Inside the loop:\\n        *   It constructs the absolute file path by combining the `target_dir` and the relative file path from `crawl_files()`.\\n        *   It opens the file in read mode (`\\\"r\\\"`) with UTF-8 encoding to handle different character sets.\\n        *   It reads the file content into the `content` variable.\\n        *   A check `if content.strip()` ensures the file is not empty.\\n        *   If the file contains content:\\n            *   It calls the `exec` method of `summariser` object, passing in the `content` to generate a summary of the file.\\n            *   It calls the `exec` method of `logic_analyzer` object, passing in the `content` to generate an analysis of the logic flow of the file.\\n            *   The returned summary and logic flow are stored in the `results` dictionary using the relative path as the key.\\n        *   If the file is empty, a warning message is logged, and the file is skipped.\\n        *   A `try...except` block handles potential errors during file processing (e.g., file not found, permission errors).\\n\\n5.  **Markdown Output Generation:**\\n    *   After processing all files, the code creates the output directory ( `target_dir / \\\"docs\\\"` ) if it doesn't exist.\\n    *   It iterates through the `results` dictionary, which contains the summaries and logic flows for each file.\\n    *   For each file:\\n        *   It constructs the output Markdown file path in the \\\"docs\\\" subdirectory. The filename is derived from the original Python file's name, but with a `.md` extension.\\n        *   It creates a Markdown string (`md_content`) with a header containing the filename, followed by sections for the summary and logic flow, which are populated from the `results` dictionary.\\n        *   It writes the Markdown content to the output file using UTF-8 encoding.\\n        *   A `try...except` block handles potential errors during file writing.\\n        * The code ensures that the parent directories of the markdown file exist by calling `md_path.parent.mkdir(parents=True, exist_ok=True)`\\n\\n6.  **Completion Message:**\\n    *   Finally, the code prints a message indicating that the documentation generation is complete and shows the location of the generated files.\\n\\n**Control Structures:**\\n\\n*   **`for` loop:** Iterates through the list of files to process.\\n*   **`try...except` blocks:** Handle potential exceptions during file processing and markdown generation.\\n*   **`if` statement:** Checks if the file content is empty before processing.\\n\\n**Function Calls:**\\n\\n*   `parse_args()`: Parses command-line arguments.\\n*   `fetch_files.crawl_files()`:  Recursively finds files in a directory based on include/exclude rules.\\n*   `open()`: Opens files for reading and writing.\\n*   `file.read()`: Reads the content of a file.\\n*   `Summarise.exec()`: Calls the LLM to generate a summary of the code.\\n*   `AnalyzeLogicFlow.exec()`: Calls the LLM to analyze the logic flow of the code.\\n*   `Path.mkdir()`: Creates a directory (and parent directories if necessary).\\n*   `file.write()`: Writes content to a file.\\n*   `logging.info()`, `logging.debug()`, `logging.error()`:  Logs informational, debugging, and error messages.\\n\\n**Data Transformations:**\\n\\n*   Command-line arguments are parsed and stored in the `shared_settings` dictionary.\\n*   File paths are manipulated using the `Path` object.\\n*   The `crawl_files()` function transforms a directory and include/exclude patterns into a list of file paths.\\n*   The LLM functions (`Summarise.exec()`, `AnalyzeLogicFlow.exec()`) transform code content into summaries and logic flow analyses (strings).\\n*   The output is formatted as Markdown.\\n\\n**Key Classes and Nodes:**\\n\\n*   `Summarise`: A `pocketflow` Node responsible for summarizing code using an LLM.  The `exec` method defines the LLM prompt.\\n*   `AnalyzeLogicFlow`: A `pocketflow` Node responsible for analyzing the logic flow of code using an LLM.  The `exec` method defines the LLM prompt.\\n*   `Node` (from `pocketflow`):  Likely a base class for defining processing nodes in a pipeline. The `prep`, `exec`, and `post` methods suggest a process-execute-postprocess workflow.\\n\\n**Assumptions:**\\n\\n*   The `call_llm()` function is responsible for interacting with the LLM, handling API requests, and potentially error handling.  It's expected to return a string containing the LLM's response.\\n*   The `pocketflow` library provides the `Node` base class and handles the execution and coordination of the nodes. The prep, exec, and post methods provide the entry points to the nodes' functionality.\\n*   The `fetch_files.crawl_files()` function is assumed to return relative file paths with respect to the target directory.\\n* The logging level is assumed to be configured elsewhere.\\n\\n**Improvements/Observations:**\\n\\n*   **Error Handling:** The `try...except` blocks are good, but could be more specific (e.g., catching `FileNotFoundError` separately).\\n*   **LLM Interaction:** The `call_llm()` function's error handling and retry logic are hidden.  Ideally, this would be exposed or handled more explicitly.\\n*   **Output Formatting:** The Markdown output format is basic.  It could be made more configurable or provide options for different documentation styles.\\n*   **Modularity:**  The code is fairly modular, with the `Node` classes separating the LLM interaction logic.  However, the Markdown generation could be extracted into a separate function or class.\\n*   **Shared State:**  The use of `shared_settings` is a good practice for passing configuration data.\\n* The nodes don't implement the prep and post functions, which suggests that they are not used to share state.\\n\", \"Summarize this text in 50 words: \\n\\n import typing\\nimport argparse\\nimport os\\n\\n\\nDEFAULT_INCLUDE_SUFFIXES = {\\n    \\\".js\\\",\\n    \\\".json\\\",\\n    \\\".jsx\\\",\\n    \\\".md\\\",\\n    \\\".py\\\",\\n    \\\".pyi\\\",\\n    \\\".pyx\\\",\\n    \\\".rst\\\",\\n    \\\".tf\\\",\\n    \\\".tfvars\\\",\\n    \\\".ts\\\",\\n    \\\".tsx\\\",\\n    \\\".yaml\\\",\\n    \\\".yml\\\",\\n    \\\"Dockerfile\\\",\\n    \\\"Jenkinsfil\\\",\\n}\\n\\nDEFAULT_EXCLUDE_SUFFIXES = {}\\n\\nDEFAULT_EXCLUDE_DIRS = {\\n    \\\".git\\\",\\n    \\\".github\\\",\\n    \\\".terraform\\\",\\n    \\\"assets\\\",\\n    \\\"build\\\",\\n    \\\"dist\\\",\\n    \\\"env\\\",\\n    \\\"docs\\\",\\n    \\\"env\\\",\\n    \\\"images\\\",\\n    \\\"temp\\\",\\n    \\\"tests\\\",\\n    \\\"tmp\\\",\\n    \\\"venv\\\",\\n}\\n\\n\\ndef parse_args() -> typing.Any:\\n    \\\"\\\"\\\"\\n    Parse command line arguments.\\n    :return: Parsed arguments\\n    \\\"\\\"\\\"\\n    parser = argparse.ArgumentParser(\\n        description=\\\"Generate documentation for a project using AI.\\\",\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\n    )\\n\\n    parser.add_argument(\\n        \\\"-n\\\",\\n        \\\"--name\\\",\\n        help=\\\"local Repository name\\\",\\n        default=\\\"repo_name\\\",\\n        )\\n\\n    parser.add_argument(\\n        \\\"-d\\\",\\n        \\\"--directory\\\",\\n        help=\\\"top level directory to scan, default is current working directory\\\",\\n        default=os.getcwd(),\\n        )\\n\\n    parser.add_argument(\\n        \\\"-e\\\",\\n        \\\"--exclude_dirs\\\",\\n        nargs='*',\\n        help=\\\"comma separated list of directories to exclude\\\",\\n        default=\\\",\\\"\\n        )\\n\\n    return parser.parse_args()\\n\\nmodel = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.5-pro-exp-03-25\\\")\\nproject=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"sre-ai-dev\\\")\": \"This code defines default file suffixes and directories to include/exclude when generating documentation. It uses `argparse` to handle command-line arguments for specifying the repository name, target directory, and directories to exclude. It also retrieves the Gemini model and project ID from environment variables.\\n\", \"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\\n\\nCode:\\n```python\\nimport typing\\nimport argparse\\nimport os\\n\\n\\nDEFAULT_INCLUDE_SUFFIXES = {\\n    \\\".js\\\",\\n    \\\".json\\\",\\n    \\\".jsx\\\",\\n    \\\".md\\\",\\n    \\\".py\\\",\\n    \\\".pyi\\\",\\n    \\\".pyx\\\",\\n    \\\".rst\\\",\\n    \\\".tf\\\",\\n    \\\".tfvars\\\",\\n    \\\".ts\\\",\\n    \\\".tsx\\\",\\n    \\\".yaml\\\",\\n    \\\".yml\\\",\\n    \\\"Dockerfile\\\",\\n    \\\"Jenkinsfil\\\",\\n}\\n\\nDEFAULT_EXCLUDE_SUFFIXES = {}\\n\\nDEFAULT_EXCLUDE_DIRS = {\\n    \\\".git\\\",\\n    \\\".github\\\",\\n    \\\".terraform\\\",\\n    \\\"assets\\\",\\n    \\\"build\\\",\\n    \\\"dist\\\",\\n    \\\"env\\\",\\n    \\\"docs\\\",\\n    \\\"env\\\",\\n    \\\"images\\\",\\n    \\\"temp\\\",\\n    \\\"tests\\\",\\n    \\\"tmp\\\",\\n    \\\"venv\\\",\\n}\\n\\n\\ndef parse_args() -> typing.Any:\\n    \\\"\\\"\\\"\\n    Parse command line arguments.\\n    :return: Parsed arguments\\n    \\\"\\\"\\\"\\n    parser = argparse.ArgumentParser(\\n        description=\\\"Generate documentation for a project using AI.\\\",\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\n    )\\n\\n    parser.add_argument(\\n        \\\"-n\\\",\\n        \\\"--name\\\",\\n        help=\\\"local Repository name\\\",\\n        default=\\\"repo_name\\\",\\n        )\\n\\n    parser.add_argument(\\n        \\\"-d\\\",\\n        \\\"--directory\\\",\\n        help=\\\"top level directory to scan, default is current working directory\\\",\\n        default=os.getcwd(),\\n        )\\n\\n    parser.add_argument(\\n        \\\"-e\\\",\\n        \\\"--exclude_dirs\\\",\\n        nargs='*',\\n        help=\\\"comma separated list of directories to exclude\\\",\\n        default=\\\",\\\"\\n        )\\n\\n    return parser.parse_args()\\n\\nmodel = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.5-pro-exp-03-25\\\")\\nproject=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"sre-ai-dev\\\")\\n```\": \"```python\\nimport typing\\nimport argparse\\nimport os\\n\\n\\nDEFAULT_INCLUDE_SUFFIXES = {\\n    \\\".js\\\",\\n    \\\".json\\\",\\n    \\\".jsx\\\",\\n    \\\".md\\\",\\n    \\\".py\\\",\\n    \\\".pyi\\\",\\n    \\\".pyx\\\",\\n    \\\".rst\\\",\\n    \\\".tf\\\",\\n    \\\".tfvars\\\",\\n    \\\".ts\\\",\\n    \\\".tsx\\\",\\n    \\\".yaml\\\",\\n    \\\".yml\\\",\\n    \\\"Dockerfile\\\",\\n    \\\"Jenkinsfil\\\",\\n}\\n\\nDEFAULT_EXCLUDE_SUFFIXES = {}\\n\\nDEFAULT_EXCLUDE_DIRS = {\\n    \\\".git\\\",\\n    \\\".github\\\",\\n    \\\".terraform\\\",\\n    \\\"assets\\\",\\n    \\\"build\\\",\\n    \\\"dist\\\",\\n    \\\"env\\\",\\n    \\\"docs\\\",\\n    \\\"env\\\",\\n    \\\"images\\\",\\n    \\\"temp\\\",\\n    \\\"tests\\\",\\n    \\\"tmp\\\",\\n    \\\"venv\\\",\\n}\\n\\n\\ndef parse_args() -> typing.Any:\\n    \\\"\\\"\\\"\\n    Parse command line arguments.\\n    :return: Parsed arguments\\n    \\\"\\\"\\\"\\n    parser = argparse.ArgumentParser(\\n        description=\\\"Generate documentation for a project using AI.\\\",\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\n    )\\n\\n    parser.add_argument(\\n        \\\"-n\\\",\\n        \\\"--name\\\",\\n        help=\\\"local Repository name\\\",\\n        default=\\\"repo_name\\\",\\n        )\\n\\n    parser.add_argument(\\n        \\\"-d\\\",\\n        \\\"--directory\\\",\\n        help=\\\"top level directory to scan, default is current working directory\\\",\\n        default=os.getcwd(),\\n        )\\n\\n    parser.add_argument(\\n        \\\"-e\\\",\\n        \\\"--exclude_dirs\\\",\\n        nargs='*',\\n        help=\\\"comma separated list of directories to exclude\\\",\\n        default=\\\",\\\"\\n        )\\n\\n    return parser.parse_args()\\n\\nmodel = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.5-pro-exp-03-25\\\")\\nproject=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"sre-ai-dev\\\")\\n```\\n\\n**Analysis of the Python Code:**\\n\\n**1. Imports:**\\n\\n*   `typing`: Used for type hinting, improving code readability and maintainability.\\n*   `argparse`:  A standard library module used for parsing command-line arguments. It allows the program to receive input from the user when executed from the command line.\\n*   `os`: Provides a way of using operating system-dependent functionality, such as getting the current working directory.\\n\\n**2. Default Configuration Sets:**\\n\\n*   `DEFAULT_INCLUDE_SUFFIXES`: A set containing file extensions that the program will consider for processing (e.g., \\\".js\\\", \\\".py\\\", \\\".md\\\").  This suggests the code will process files of these types.\\n*   `DEFAULT_EXCLUDE_SUFFIXES`: An empty set, presumably designed to hold file extensions that should be excluded from processing.  Currently, it excludes none.\\n*   `DEFAULT_EXCLUDE_DIRS`: A set of directory names that should be excluded from the scanning process (e.g., \\\".git\\\", \\\"venv\\\", \\\"tests\\\"). This is useful for ignoring irrelevant directories.\\n\\n**3. `parse_args()` Function:**\\n\\n*   **Purpose:** This function defines how the program accepts and processes command-line arguments.\\n*   **`argparse.ArgumentParser`:** Creates an argument parser object. The `description` provides a brief explanation of the program's purpose, and `formatter_class` improves the help message format.\\n*   **`parser.add_argument()`:**  This method is called multiple times to define the different command-line arguments that the program accepts.\\n    *   `-n`, `--name`:  Defines an argument for the repository name. It has a default value of \\\"repo\\\\_name\\\".\\n    *   `-d`, `--directory`: Defines an argument for the top-level directory to scan. Its default value is the current working directory obtained using `os.getcwd()`.\\n    *   `-e`, `--exclude_dirs`:  Defines an argument for a list of directories to exclude. It accepts multiple values (`nargs='*'`) and has a default value of a single comma \\\",\\\". This means that if the user doesn't specify any excluded directories, the default value is a single comma, which likely requires further processing to handle correctly (e.g., splitting it into an empty list).\\n*   **`parser.parse_args()`:**  Parses the command-line arguments and returns an object containing the values provided by the user (or the default values if no arguments are provided).\\n*   **Return Value:**  The function returns an `argparse.Namespace` object (aliased as `typing.Any` for generic type hinting). This object contains the parsed arguments as attributes.\\n\\n**4. Environment Variables:**\\n\\n*   `model = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.5-pro-exp-03-25\\\")`: Reads the value of the environment variable `GEMINI_MODEL`. If the environment variable is not set, it defaults to \\\"gemini-2.5-pro-exp-03-25\\\".  This likely specifies an AI model to be used.\\n*   `project=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"sre-ai-dev\\\")`: Reads the value of the environment variable `GEMINI_PROJECT_ID`. If the environment variable is not set, it defaults to \\\"sre-ai-dev\\\". This probably specifies the Google Cloud project ID for the AI model.\\n\\n**Overall Purpose and Logic Flow:**\\n\\nThis code snippet sets up the basic infrastructure for a program that generates documentation for a project using AI.  The core logic is as follows:\\n\\n1.  **Configuration:** Defines default settings for file suffixes to include, file suffixes to exclude, and directories to exclude.\\n2.  **Argument Parsing:** Uses `argparse` to handle command-line arguments, allowing the user to specify:\\n    *   The repository name.\\n    *   The directory to scan.\\n    *   A list of directories to exclude.\\n3.  **Environment Variable Handling:** Reads the `GEMINI_MODEL` and `GEMINI_PROJECT_ID` environment variables (or uses default values if they are not set). These variables appear to be related to the AI model used for generating the documentation.\\n\\nThe next steps (not shown in the code snippet) would likely involve:\\n\\n1.  Retrieving the command-line arguments from the `parse_args()` function.\\n2.  Walking the specified directory, excluding directories specified via command-line or the default exclude list.\\n3.  Filtering files based on the included and excluded suffixes.\\n4.  Processing the remaining files using the specified AI model (from the environment variable), presumably to generate documentation.\\n5.  Outputting the generated documentation.\\n\", \"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Do not include the code in the output. Explain the purpose of the code in terms of its logic.\\n\\nCode:\\n```python\\nimport typing\\nimport argparse\\nimport os\\n\\n\\nDEFAULT_INCLUDE_SUFFIXES = {\\n    \\\".js\\\",\\n    \\\".json\\\",\\n    \\\".jsx\\\",\\n    \\\".md\\\",\\n    \\\".py\\\",\\n    \\\".pyi\\\",\\n    \\\".pyx\\\",\\n    \\\".rst\\\",\\n    \\\".tf\\\",\\n    \\\".tfvars\\\",\\n    \\\".ts\\\",\\n    \\\".tsx\\\",\\n    \\\".yaml\\\",\\n    \\\".yml\\\",\\n    \\\"Dockerfile\\\",\\n    \\\"Jenkinsfil\\\",\\n}\\n\\nDEFAULT_EXCLUDE_SUFFIXES = {}\\n\\nDEFAULT_EXCLUDE_DIRS = {\\n    \\\".git\\\",\\n    \\\".github\\\",\\n    \\\".terraform\\\",\\n    \\\"assets\\\",\\n    \\\"build\\\",\\n    \\\"dist\\\",\\n    \\\"env\\\",\\n    \\\"docs\\\",\\n    \\\"env\\\",\\n    \\\"images\\\",\\n    \\\"temp\\\",\\n    \\\"tests\\\",\\n    \\\"tmp\\\",\\n    \\\"venv\\\",\\n}\\n\\n\\ndef parse_args() -> typing.Any:\\n    \\\"\\\"\\\"\\n    Parse command line arguments.\\n    :return: Parsed arguments\\n    \\\"\\\"\\\"\\n    parser = argparse.ArgumentParser(\\n        description=\\\"Generate documentation for a project using AI.\\\",\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\n    )\\n\\n    parser.add_argument(\\n        \\\"-n\\\",\\n        \\\"--name\\\",\\n        help=\\\"local Repository name\\\",\\n        default=\\\"repo_name\\\",\\n        )\\n\\n    parser.add_argument(\\n        \\\"-d\\\",\\n        \\\"--directory\\\",\\n        help=\\\"top level directory to scan, default is current working directory\\\",\\n        default=os.getcwd(),\\n        )\\n\\n    parser.add_argument(\\n        \\\"-e\\\",\\n        \\\"--exclude_dirs\\\",\\n        nargs='*',\\n        help=\\\"comma separated list of directories to exclude\\\",\\n        default=\\\",\\\"\\n        )\\n\\n    return parser.parse_args()\\n\\nmodel = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.5-pro-exp-03-25\\\")\\nproject=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"sre-ai-dev\\\")\\n```\": \"The Python code begins by importing necessary modules like `typing`, `argparse`, and `os`. It then defines three sets as constants: `DEFAULT_INCLUDE_SUFFIXES`, `DEFAULT_EXCLUDE_SUFFIXES`, and `DEFAULT_EXCLUDE_DIRS`, containing file extensions to include, file extensions to exclude, and directory names to exclude, respectively.\\n\\nThe core logic resides in the `parse_args` function. This function uses the `argparse` module to define and parse command-line arguments. It defines three arguments: `-n` or `--name` for the repository name (defaulting to \\\"repo_name\\\"), `-d` or `--directory` for the top-level directory to scan (defaulting to the current working directory), and `-e` or `--exclude_dirs` for a comma-separated list of directories to exclude (defaulting to an empty string).  The function returns the parsed arguments object.\\n\\nFinally, the code retrieves environment variables. It attempts to retrieve the value of `GEMINI_MODEL` from the environment, defaulting to \\\"gemini-2.5-pro-exp-03-25\\\" if the environment variable is not set.  Similarly, it attempts to retrieve the value of `GEMINI_PROJECT_ID` from the environment, defaulting to \\\"sre-ai-dev\\\" if the environment variable is not set.\\n\\nIn essence, this code sets up the configuration for a documentation generation tool. It defines default settings for file inclusion and exclusion, allows the user to override these settings via command-line arguments, and fetches necessary configuration values from environment variables. The purpose of the code is to prepare and collect the configuration data required for the rest of the documentation generation process.\\n\", \"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Do not include the code in the output. Explain the purpose of the code in terms of its logic.\\n\\nCode:\\n```python\\nfrom google import genai\\nfrom google.genai import types\\nimport os\\nimport logging\\nimport json\\nfrom datetime import datetime\\n\\n\\n# Configure logging\\nlog_directory = os.getenv(\\\"LOG_DIR\\\", \\\"logs\\\")\\nos.makedirs(log_directory, exist_ok=True)\\nlog_file = os.path.join(\\n    log_directory, f\\\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\\\"\\n)\\n\\n# Set up logger\\nlogger = logging.getLogger(\\\"llm_logger\\\")\\nlogger.setLevel(logging.INFO)\\nlogger.propagate = False  # Prevent propagation to root logger\\nfile_handler = logging.FileHandler(log_file)\\nfile_handler.setFormatter(\\n    logging.Formatter(\\\"%(asctime)s - %(levelname)s - %(message)s\\\")\\n)\\nlogger.addHandler(file_handler)\\n\\n# Simple cache configuration\\ncache_file = \\\"llm_cache.json\\\"\\n\\n\\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\\n    # Log the prompt\\n    logger.info(f\\\"PROMPT: {prompt}\\\")\\n\\n    # Check cache if enabled\\n    if use_cache:\\n        # Load cache from disk\\n        cache = {}\\n        if os.path.exists(cache_file):\\n            try:\\n                with open(cache_file, \\\"r\\\") as f:\\n                    cache = json.load(f)\\n            except:\\n                logger.warning(f\\\"Failed to load cache, starting with empty cache\\\")\\n\\n        # Return from cache if exists\\n        if prompt in cache:\\n            logger.info(f\\\"RESPONSE: {cache[prompt]}\\\")\\n            return cache[prompt]\\n\\n    # Call the LLM if not in cache or cache disabled\\n    client = genai.Client(\\n        vertexai=True,\\n        project=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"ai-sre-dev-84b7\\\"),\\n        location=os.getenv(\\\"GEMINI_LOCATION\\\", \\\"us-central1\\\"),\\n        http_options=types.HttpOptions(api_version='v1')\\n    )\\n\\n    model = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.0-flash-exp\\\")\\n    # for model in client.models.list():\\n    #     print(model)\\n    response = client.models.generate_content(model=model, contents=[prompt])\\n    response_text = response.text\\n\\n    # Log the response\\n    logger.info(f\\\"RESPONSE: {response_text}\\\")\\n\\n    # Update cache if enabled\\n    if use_cache:\\n        # Load cache again to avoid overwrites\\n        cache = {}\\n        if os.path.exists(cache_file):\\n            try:\\n                with open(cache_file, \\\"r\\\") as f:\\n                    cache = json.load(f)\\n            except:\\n                pass\\n\\n        # Add to cache and save\\n        cache[prompt] = response_text\\n        try:\\n            with open(cache_file, \\\"w\\\") as f:\\n                json.dump(cache, f)\\n        except Exception as e:\\n            logger.error(f\\\"Failed to save cache: {e}\\\")\\n\\n    return response_text\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    test_prompt = \\\"Hello, how are you?\\\"\\n\\n    # First call - should hit the API\\n    print(\\\"Making call...\\\")\\n    response1 = call_llm(test_prompt, use_cache=False)\\n    print(f\\\"Response: {response1}\\\")\\n\\n```\": \"The code defines a function `call_llm` that interacts with a Large Language Model (LLM) via the `google.genai` library and implements a simple caching mechanism to reduce API calls.\\n\\nThe script begins by setting up logging to a file named \\\"llm_calls_{date}.log\\\" in a \\\"logs\\\" directory. It configures a logger to write informational messages, including prompts and responses.\\n\\nThe core logic resides within the `call_llm` function. This function accepts a prompt string and a boolean flag, `use_cache`, as input.\\n\\nFirst, it logs the input prompt using the configured logger. Then, if `use_cache` is true, it attempts to load a JSON file named \\\"llm_cache.json\\\" into a dictionary. If the file exists and can be loaded, it checks if the prompt already exists as a key in the dictionary. If the prompt is found in the cache, the cached response is logged and immediately returned, avoiding an LLM API call. If loading the cache fails, a warning is logged, and the process continues as if there was no cache.\\n\\nIf caching is disabled (`use_cache` is false) or the prompt is not found in the cache, the function proceeds to call the LLM API. It initializes a `genai.Client` using environment variables for project ID, location, and model name. It then uses the client to generate content based on the provided prompt. The LLM's response is then stored in the `response_text` variable. This response text is then logged using the logger.\\n\\nAfter receiving the response from the LLM, if `use_cache` is true, the function attempts to update the cache. It loads the cache again (to avoid overwriting any changes made by other concurrent calls).  Then, it adds the current prompt and its corresponding response to the cache dictionary. Finally, it attempts to save the updated cache dictionary back to the \\\"llm_cache.json\\\" file. If saving the cache fails, an error message is logged.\\n\\nFinally, the function returns the LLM's `response_text`.\\n\\nThe `if __name__ == \\\"__main__\\\":` block demonstrates a simple usage of the `call_llm` function.  It defines a test prompt and calls the `call_llm` function with the prompt and `use_cache` set to `False`, demonstrating a direct API call.  The returned response is printed to the console.\\n\", \"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Do not include the code in the output. Explain the purpose of the code in terms of its logic.\\n\\nCode:\\n```python\\n# A script to crawl files from a given directory and its subdirectories.\\n# It uses the `os` module to walk through the directory structure and collects file paths based on specified include and exclude patterns.\\n# files that are not in the exclude patterns and are in the include patterns are collected.\\nimport os\\nimport typing\\nimport logging\\nfrom pathlib import Path\\nfrom settings import DEFAULT_EXCLUDE_SUFFIXES, DEFAULT_INCLUDE_SUFFIXES, DEFAULT_EXCLUDE_DIRS\\n\\n\\n\\ndef crawl_files(\\n    directory: str,\\n    include_patterns: typing.Set[str] = DEFAULT_INCLUDE_SUFFIXES,\\n    exclude_suffixes: typing.Set[str] = DEFAULT_EXCLUDE_SUFFIXES,\\n    exclude_dirs: typing.Set[str] = DEFAULT_EXCLUDE_DIRS,\\n    ) -> typing.List[str]:\\n    try:\\n        files = {\\n            str(item.relative_to(directory))  # Store the relative path string\\n            for item in Path(directory).rglob(\\\"*\\\") # Iterate through all items recursively\\n            if item.is_file() # Consider only files\\n            if item.suffix in include_patterns # Check 0: Suffix is included\\n            and item.suffix not in exclude_suffixes # Check 1: Suffix is not excluded\\n            and not any(part in exclude_dirs for part in item.relative_to(directory).parts) # Check 2: No path part is excluded\\n        }\\n\\n    except ValueError as e:\\n        logging.error(f\\\"Error calculating relative paths: {e}\\\")\\n        logging.error(f\\\"Ensure the search directory '{directory.resolve()}' is within or is the \\\"\\n            f\\\"current working directory '{directory.resolve()}'\\\")\\n\\n    logging.info(f\\\"Found {len(files)} files in {directory}\\\")\\n    logging.debug(f\\\"Files found: {files}\\\")\\n\\n    return files\\n\\n\\ndef main() -> None:\\n    print(\\\"Crawling files...\\\")\\n    directory = os.getcwd()  \\n    print(f\\\"Files crawled from {directory}:\\\")\\n    print (crawl_files(directory))\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n```\": \"The code defines a function `crawl_files` that recursively traverses a directory and its subdirectories to identify files that meet specific criteria. It uses the `os` module and `pathlib` for directory traversal and file system operations.\\n\\nThe `crawl_files` function first takes a directory path as input, along with optional sets of include and exclude file suffixes, and directories to exclude. It walks the directory tree using `Path(directory).rglob(\\\"*\\\")`, retrieving every item within the directory and its subdirectories.\\n\\nFor each item, it checks if it is a file using `item.is_file()`. If it is a file, it further checks if its suffix is present in the `include_patterns` set and absent from the `exclude_suffixes` set. Additionally, it verifies that no part of the relative path is present in the `exclude_dirs` set. The relative path is calculated using `item.relative_to(directory)`. Only the files satisfying all these conditions are collected and stored as relative paths (strings) in a set.\\n\\nThe `main` function calls the `crawl_files` function using the current working directory. Finally, it prints the results of the file crawling operation to the standard output.\\n\\nThe primary data transformations involve converting Path objects to their relative string representations using `str(item.relative_to(directory))`, and building a set of these string representations.\\n\"}\n```": "The code defines a function `crawl_files` that recursively traverses a directory and its subdirectories to identify files that meet specific criteria. It uses the `os` module and `pathlib` for directory traversal and file system operations.\n\nThe `crawl_files` function first takes a directory path as input, along with optional sets of include and exclude file suffixes, and directories to exclude. It walks the directory tree using `Path(directory).rglob(\"*\")`, retrieving every item within the directory and its subdirectories.\n\nFor each item, it checks if it is a file using `item.is_file()`. If it is a file, it further checks if its suffix is present in the `include_patterns` set and absent from the `exclude_suffixes` set. Additionally, it verifies that no part of the relative path is present in the `exclude_dirs` set. The relative path is calculated using `item.relative_to(directory)`. Only the files satisfying all these conditions are collected and stored as relative paths (strings) in a set.\n\nThe `main` function calls the `crawl_files` function using the current working directory. Finally, it prints the results of the file crawling operation to the standard output.\n\nThe primary data transformations involve converting Path objects to their relative string representations using `str(item.relative_to(directory))`, and building a set of these string representations.\n", "Summarize this text in 50 words: \n\n import logging\nfrom pathlib import Path\nfrom pocketflow import Node\nfrom call_llm import call_llm\nfrom settings import (\n    DEFAULT_EXCLUDE_SUFFIXES,\n    DEFAULT_INCLUDE_SUFFIXES,\n    DEFAULT_EXCLUDE_DIRS,\n)\nfrom settings import parse_args\nimport fetch_files\n\n\nclass Summarise(Node):\n    \"\"\"\n    This class identifies abstractions in the code.\n    \"\"\"\n\n    def prep(self,shared):\n        return file_contents[\"data\"]\n    \n    def exec(self, text):\n        prompt = f\"Summarize this text in 50 words: \\n\\n {text}\"\n        return call_llm(prompt)\n    \n    def post(self, shared, prep_res, exec_res):\n        # Store the result in the shared settings\n        file_contents[\"summaries\"] = exec_res\n        return \"default\"\n\n\n# New Node for Logic Flow Analysis\nclass AnalyzeLogicFlow(Node):\n    \"\"\"\n    This class analyzes the code to describe its main algorithm or logic flow.\n    \"\"\"\n\n    # prep and post might not be needed if main loop handles orchestration\n    # def prep(self, shared):\n    #     pass\n\n    def exec(self, text):\n        prompt = f\"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Do not include the code in the output. Explain the purpose of the code in terms of its logic.\\n\\nCode:\\n```python\\n{text}\\n```\"\n        # Assuming call_llm handles potential errors and returns a string\n        return call_llm(prompt)\n\n    # def post(self, shared, prep_res, exec_res):\n    #     pass\n\n\ndef main() -> None:\n    args = parse_args()\n\n    # Create a dictionary of settings to be used in the script\n    shared_settings = {\n        \"name\": args.name,\n        \"directory\": args.directory, # Target directory to analyze\n        \"exclude_patterns\": DEFAULT_EXCLUDE_SUFFIXES,\n        \"exclude_dirs\": DEFAULT_EXCLUDE_DIRS,\n        \"include_suffixes\": DEFAULT_INCLUDE_SUFFIXES,\n        \"additional_exclude_dirs\": args.exclude_dirs,\n    }\n    logging.debug(f\"Shared settings: {shared_settings}\")\n\n    # Get a list of files from the target directory\n    # Ensure paths are relative to the target directory for correct output structure\n    target_dir = Path(shared_settings[\"directory\"]).resolve()\n    logging.info(f\"Analyzing directory: {target_dir}\")\n\n    # fetch_files.crawl_files already returns relative paths if called correctly\n    # Let's ensure the base directory passed to crawl_files is the target directory\n    files_to_process = fetch_files.crawl_files(\n        directory=str(target_dir), # Pass absolute path to crawl\n        include_patterns=shared_settings[\"include_suffixes\"],\n        exclude_suffixes=shared_settings[\"exclude_patterns\"],\n        exclude_dirs=shared_settings[\"exclude_dirs\"].union(\n            shared_settings[\"additional_exclude_dirs\"]\n        ),\n    )\n    \n    # Instantiate nodes\n    summariser = Summarise()\n    logic_analyzer = AnalyzeLogicFlow()\n\n    results = {}\n    logging.info(f\"Processing {len(files_to_process)} files...\")\n\n    # Process each file\n    for relative_file_path in files_to_process:\n        absolute_file_path = target_dir / relative_file_path\n        logging.debug(f\"Processing file: {absolute_file_path}\")\n        print(f\"Processing: {relative_file_path}\") # User feedback\n        try:\n            with open(absolute_file_path, \"r\", encoding='utf-8') as f:\n                content = f.read()\n                if content.strip(): # Ensure content is not empty\n                    # Call LLM for summary\n                    summary = summariser.exec(content)\n                    # Call LLM for logic flow\n                    logic_flow = logic_analyzer.exec(content)\n                    results[relative_file_path] = {\"summary\": summary, \"logic_flow\": logic_flow}\n                else:\n                    logging.warning(f\"Skipped empty file: {absolute_file_path}\")\n                    print(f\"Skipped empty file: {relative_file_path}\")\n        except Exception as e:\n            logging.error(f\"Error processing file {absolute_file_path}: {e}\")\n            print(f\"Error processing file {relative_file_path}: {e}\")\n\n    # Generate Markdown output\n    output_base_dir = target_dir / \"docs\"\n    logging.info(f\"Generating documentation in: {output_base_dir}\")\n    try:\n        output_base_dir.mkdir(parents=True, exist_ok=True)\n    except Exception as e:\n        logging.error(f\"Failed to create output directory {output_base_dir}: {e}\")\n        print(f\"Error: Could not create output directory {output_base_dir}. Aborting.\")\n        return # Exit if we can't create the output dir\n\n    for relative_file_path, data in results.items():\n        md_filename = Path(relative_file_path).with_suffix(\".md\")\n        md_path = output_base_dir / md_filename\n        \n        try:\n            # Ensure subdirectory exists\n            md_path.parent.mkdir(parents=True, exist_ok=True)\n\n            # Format Markdown content\n            md_content = f\"\"\"# File: {relative_file_path}\n\n## Summary\n\n{data['summary']}\n\n## Logic Flow\n\n{data['logic_flow']}\n\"\"\"\n            # Write the Markdown file\n            with open(md_path, \"w\", encoding='utf-8') as f:\n                f.write(md_content)\n            logging.info(f\"Generated: {md_path}\")\n            print(f\"Generated: {md_path.relative_to(target_dir)}\") # Show relative path to user\n\n        except Exception as e:\n            logging.error(f\"Failed to write Markdown file {md_path}: {e}\")\n            print(f\"Error writing file {md_path.relative_to(target_dir)}: {e}\")\n\n    print(f\"\\nDocumentation generation complete. Files written to: {output_base_dir}\")\n\n\nif __name__ == \"__main__\":\n    main()\n": "This Python script analyzes code in a directory using LLMs. It crawls files, summarizes their content, and analyzes their logic flow using `Summarise` and `AnalyzeLogicFlow` nodes.  Results are stored in a dictionary and then outputted as Markdown files within a \"docs\" subdirectory, providing code summaries and logic flow descriptions.\n", "Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Do not include the code in the output. Explain the purpose of the code in terms of its logic.\n\nCode:\n```python\nimport logging\nfrom pathlib import Path\nfrom pocketflow import Node\nfrom call_llm import call_llm\nfrom settings import (\n    DEFAULT_EXCLUDE_SUFFIXES,\n    DEFAULT_INCLUDE_SUFFIXES,\n    DEFAULT_EXCLUDE_DIRS,\n)\nfrom settings import parse_args\nimport fetch_files\n\n\nclass Summarise(Node):\n    \"\"\"\n    This class identifies abstractions in the code.\n    \"\"\"\n\n    def prep(self,shared):\n        return file_contents[\"data\"]\n    \n    def exec(self, text):\n        prompt = f\"Summarize this text in 50 words: \\n\\n {text}\"\n        return call_llm(prompt)\n    \n    def post(self, shared, prep_res, exec_res):\n        # Store the result in the shared settings\n        file_contents[\"summaries\"] = exec_res\n        return \"default\"\n\n\n# New Node for Logic Flow Analysis\nclass AnalyzeLogicFlow(Node):\n    \"\"\"\n    This class analyzes the code to describe its main algorithm or logic flow.\n    \"\"\"\n\n    # prep and post might not be needed if main loop handles orchestration\n    # def prep(self, shared):\n    #     pass\n\n    def exec(self, text):\n        prompt = f\"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Do not include the code in the output. Explain the purpose of the code in terms of its logic.\\n\\nCode:\\n```python\\n{text}\\n```\"\n        # Assuming call_llm handles potential errors and returns a string\n        return call_llm(prompt)\n\n    # def post(self, shared, prep_res, exec_res):\n    #     pass\n\n\ndef main() -> None:\n    args = parse_args()\n\n    # Create a dictionary of settings to be used in the script\n    shared_settings = {\n        \"name\": args.name,\n        \"directory\": args.directory, # Target directory to analyze\n        \"exclude_patterns\": DEFAULT_EXCLUDE_SUFFIXES,\n        \"exclude_dirs\": DEFAULT_EXCLUDE_DIRS,\n        \"include_suffixes\": DEFAULT_INCLUDE_SUFFIXES,\n        \"additional_exclude_dirs\": args.exclude_dirs,\n    }\n    logging.debug(f\"Shared settings: {shared_settings}\")\n\n    # Get a list of files from the target directory\n    # Ensure paths are relative to the target directory for correct output structure\n    target_dir = Path(shared_settings[\"directory\"]).resolve()\n    logging.info(f\"Analyzing directory: {target_dir}\")\n\n    # fetch_files.crawl_files already returns relative paths if called correctly\n    # Let's ensure the base directory passed to crawl_files is the target directory\n    files_to_process = fetch_files.crawl_files(\n        directory=str(target_dir), # Pass absolute path to crawl\n        include_patterns=shared_settings[\"include_suffixes\"],\n        exclude_suffixes=shared_settings[\"exclude_patterns\"],\n        exclude_dirs=shared_settings[\"exclude_dirs\"].union(\n            shared_settings[\"additional_exclude_dirs\"]\n        ),\n    )\n    \n    # Instantiate nodes\n    summariser = Summarise()\n    logic_analyzer = AnalyzeLogicFlow()\n\n    results = {}\n    logging.info(f\"Processing {len(files_to_process)} files...\")\n\n    # Process each file\n    for relative_file_path in files_to_process:\n        absolute_file_path = target_dir / relative_file_path\n        logging.debug(f\"Processing file: {absolute_file_path}\")\n        print(f\"Processing: {relative_file_path}\") # User feedback\n        try:\n            with open(absolute_file_path, \"r\", encoding='utf-8') as f:\n                content = f.read()\n                if content.strip(): # Ensure content is not empty\n                    # Call LLM for summary\n                    summary = summariser.exec(content)\n                    # Call LLM for logic flow\n                    logic_flow = logic_analyzer.exec(content)\n                    results[relative_file_path] = {\"summary\": summary, \"logic_flow\": logic_flow}\n                else:\n                    logging.warning(f\"Skipped empty file: {absolute_file_path}\")\n                    print(f\"Skipped empty file: {relative_file_path}\")\n        except Exception as e:\n            logging.error(f\"Error processing file {absolute_file_path}: {e}\")\n            print(f\"Error processing file {relative_file_path}: {e}\")\n\n    # Generate Markdown output\n    output_base_dir = target_dir / \"docs\"\n    logging.info(f\"Generating documentation in: {output_base_dir}\")\n    try:\n        output_base_dir.mkdir(parents=True, exist_ok=True)\n    except Exception as e:\n        logging.error(f\"Failed to create output directory {output_base_dir}: {e}\")\n        print(f\"Error: Could not create output directory {output_base_dir}. Aborting.\")\n        return # Exit if we can't create the output dir\n\n    for relative_file_path, data in results.items():\n        md_filename = Path(relative_file_path).with_suffix(\".md\")\n        md_path = output_base_dir / md_filename\n        \n        try:\n            # Ensure subdirectory exists\n            md_path.parent.mkdir(parents=True, exist_ok=True)\n\n            # Format Markdown content\n            md_content = f\"\"\"# File: {relative_file_path}\n\n## Summary\n\n{data['summary']}\n\n## Logic Flow\n\n{data['logic_flow']}\n\"\"\"\n            # Write the Markdown file\n            with open(md_path, \"w\", encoding='utf-8') as f:\n                f.write(md_content)\n            logging.info(f\"Generated: {md_path}\")\n            print(f\"Generated: {md_path.relative_to(target_dir)}\") # Show relative path to user\n\n        except Exception as e:\n            logging.error(f\"Failed to write Markdown file {md_path}: {e}\")\n            print(f\"Error writing file {md_path.relative_to(target_dir)}: {e}\")\n\n    print(f\"\\nDocumentation generation complete. Files written to: {output_base_dir}\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n```": "The code's primary purpose is to analyze Python source code files within a specified directory and generate corresponding Markdown documentation that includes a summary and an analysis of the code's logic flow using a large language model (LLM).\n\nThe script begins by parsing command-line arguments using `parse_args` to configure settings like the target directory, file suffixes to include or exclude, and directories to exclude from analysis. It then uses these settings to crawl the specified directory using `fetch_files.crawl_files`, generating a list of Python files to process.  `crawl_files` returns file paths relative to the target directory.\n\nFor each file found, the script reads the file's content. It then calls an LLM (via the `call_llm` function) twice: once to generate a summary of the code's purpose and once to analyze and describe the code's algorithm and logic flow. The prompts include instructions to analyze the provided code string and return specific details based on those instructions. Error handling is included in the read, summarize, and analyze file processing steps.\n\nAfter processing each file, the script stores the generated summary and logic flow analysis in a dictionary called `results`.\n\nFinally, the script iterates through the `results` dictionary and generates a Markdown file for each processed Python file. The Markdown file contains the original filename, the LLM-generated summary, and the LLM-generated logic flow analysis, all properly formatted.  The Markdown files are written to a \"docs\" subdirectory within the original target directory. Error handling is also included here to avoid problems during file creation. The script also logs progress and errors and provides some output to the user via `print` statements.\n\nTwo Node classes, `Summarise` and `AnalyzeLogicFlow` are defined that encapsulate the LLM calls, though the `prep` and `post` methods are not actually being used. The `Summarise` node generates a summary, while the `AnalyzeLogicFlow` node focuses on describing the code's algorithm and logic flow.\n\nIn summary, the algorithm consists of parsing arguments, finding files, reading files, generating LLM-powered summaries and logic flow analyses, and then generating Markdown documentation files.\n"}