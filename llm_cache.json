{"Summarize this text in 50 words: \n\n # Project Enhancement Plan: AI Docs Generation\n\n**Goal:** Enhance the `ai_docs` project to generate both a summary and a description of the main algorithm/logic flow for each processed file, using distinct PocketFlow nodes, and output the combined results into structured Markdown files within a `docs` directory in the target project.\n\n**Detailed Plan:**\n\n1.  **Define Separate PocketFlow Nodes:**\n    *   **File:** `ai_docs/main.py`\n    *   **Node 1 (Summary):** Keep the existing `Summarise` node. Its `exec` method will retain the prompt: `f\"Summarize this text in 50 words: \\n\\n {text}\"`.\n    *   **Node 2 (Logic Flow):** Create a *new* class `AnalyzeLogicFlow(Node)`. Its `exec` method will use the prompt: `f\"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\\n\\nCode:\\n\\`\\`\\`python\\n{text}\\n\\`\\`\\`\"`.\n    *   **Node `prep`/`post`:** For simplicity in this plan, we'll assume the `prep` and `post` methods of these nodes are minimal or unused, and the main loop will handle data flow and result aggregation.\n\n2.  **Orchestrate Node Execution in `main()`:**\n    *   **File:** `ai_docs/main.py`\n    *   **Instantiation:** Create instances of both nodes: `summariser = Summarise()`, `logic_analyzer = AnalyzeLogicFlow()`.\n    *   **Result Storage:** Initialize an empty dictionary `results = {}`.\n    *   **File Iteration:** Loop through each `file_path` in `shared_settings[\"files\"]`.\n    *   **Processing:**\n        *   Read the content of `file_path`. Handle potential errors and skip empty files.\n        *   Call the summarizer: `summary = summariser.exec(content)` (this implicitly uses `call_llm`).\n        *   Call the logic analyzer: `logic_flow = logic_analyzer.exec(content)` (this also uses `call_llm`).\n        *   Store both results: `results[file_path] = {\"summary\": summary, \"logic_flow\": logic_flow}`.\n        *   Log progress (e.g., `print(f\"Processed: {file_path}\")`).\n\n3.  **Generate Markdown Output:**\n    *   **File:** `ai_docs/main.py` (after the processing loop)\n    *   **Determine Output Directory:** Define the base path for documentation: `output_base_dir = Path(shared_settings[\"directory\"]) / \"docs\"`. Use `pathlib.Path` for robust path handling.\n    *   **Create Base Directory:** Ensure the `docs` directory exists: `output_base_dir.mkdir(parents=True, exist_ok=True)`.\n    *   **Iterate and Write Files:** Loop through `results.items()`. For each `file_path`, `data` pair:\n        *   **Construct Markdown Path:** Create the target path, preserving the original file's relative structure within the `docs` directory: `md_path = output_base_dir / Path(file_path).with_suffix(\".md\")`.\n        *   **Ensure Subdirectories:** Make sure the parent directory for the Markdown file exists: `md_path.parent.mkdir(parents=True, exist_ok=True)`.\n        *   **Format Content:** Create the Markdown string:\n            ```markdown\n            # File: {file_path}\n\n            ## Summary\n\n            {data['summary']}\n\n            ## Logic Flow\n\n            {data['logic_flow']}\n            ```\n        *   **Write File:** Write the formatted content to `md_path` (using `with open(...) as f:` and `f.write(...)`, specifying UTF-8 encoding).\n        *   Log output creation (e.g., `print(f\"Generated: {md_path}\")`).\n\n4.  **Dependencies/Settings:**\n    *   No changes anticipated for `call_llm.py`, `fetch_files.py`, or `settings.py` based on this plan. The `pathlib` module is already used in `fetch_files.py`, so it's available.\n\n**Workflow Diagram (Mermaid):**\n\n```mermaid\ngraph TD\n    A[Start: main.py] --> B{Parse Args};\n    B --> C[crawl_files];\n    C --> D{Get Filtered File List};\n    D --> E{Loop Through Files};\n    E -- File Path --> F[Read File Content];\n    F -- Content --> G[Instantiate Summarise Node];\n    F -- Content --> H[Instantiate AnalyzeLogicFlow Node];\n    G -- Content --> I[Call Summarise.exec];\n    H -- Content --> J[Call AnalyzeLogicFlow.exec];\n    I -- Summary Prompt --> K{call_llm};\n    J -- Logic Flow Prompt --> K;\n    K -- Result --> L{Collect Summary};\n    K -- Result --> M{Collect Logic Flow};\n    L & M -- Store Results --> N{results Dict};\n    N --> O{End File Loop};\n    O --> P{Determine Output Dir (target/docs)};\n    P --> Q{Create Output Dir};\n    Q --> R{Loop Through Results};\n    R -- Result Data --> S{Construct MD Path (target/docs/...)};\n    S --> T{Create MD Subdirs};\n    T --> U{Format MD Content};\n    U --> V[Write MD File];\n    V --> W{End Results Loop};\n    W --> X[End];\n\n    subgraph LLM Calls\n        K;\n    end\n\n    subgraph File Processing Loop (main.py)\n        E; F; G; H; I; J; L; M; N; O;\n    end\n\n     subgraph Markdown Output Loop (main.py)\n        P; Q; R; S; T; U; V; W;\n    end": "The `ai_docs` project will be enhanced to generate file summaries and logic flow descriptions. New `AnalyzeLogicFlow` node will analyze code logic.  The `main()` function will orchestrate the execution of both nodes. Markdown files containing the summary and logic flow are output to a `docs` directory. Existing dependencies remain unchanged.\n", "Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\n\nCode:\n```python\n# Project Enhancement Plan: AI Docs Generation\n\n**Goal:** Enhance the `ai_docs` project to generate both a summary and a description of the main algorithm/logic flow for each processed file, using distinct PocketFlow nodes, and output the combined results into structured Markdown files within a `docs` directory in the target project.\n\n**Detailed Plan:**\n\n1.  **Define Separate PocketFlow Nodes:**\n    *   **File:** `ai_docs/main.py`\n    *   **Node 1 (Summary):** Keep the existing `Summarise` node. Its `exec` method will retain the prompt: `f\"Summarize this text in 50 words: \\n\\n {text}\"`.\n    *   **Node 2 (Logic Flow):** Create a *new* class `AnalyzeLogicFlow(Node)`. Its `exec` method will use the prompt: `f\"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\\n\\nCode:\\n\\`\\`\\`python\\n{text}\\n\\`\\`\\`\"`.\n    *   **Node `prep`/`post`:** For simplicity in this plan, we'll assume the `prep` and `post` methods of these nodes are minimal or unused, and the main loop will handle data flow and result aggregation.\n\n2.  **Orchestrate Node Execution in `main()`:**\n    *   **File:** `ai_docs/main.py`\n    *   **Instantiation:** Create instances of both nodes: `summariser = Summarise()`, `logic_analyzer = AnalyzeLogicFlow()`.\n    *   **Result Storage:** Initialize an empty dictionary `results = {}`.\n    *   **File Iteration:** Loop through each `file_path` in `shared_settings[\"files\"]`.\n    *   **Processing:**\n        *   Read the content of `file_path`. Handle potential errors and skip empty files.\n        *   Call the summarizer: `summary = summariser.exec(content)` (this implicitly uses `call_llm`).\n        *   Call the logic analyzer: `logic_flow = logic_analyzer.exec(content)` (this also uses `call_llm`).\n        *   Store both results: `results[file_path] = {\"summary\": summary, \"logic_flow\": logic_flow}`.\n        *   Log progress (e.g., `print(f\"Processed: {file_path}\")`).\n\n3.  **Generate Markdown Output:**\n    *   **File:** `ai_docs/main.py` (after the processing loop)\n    *   **Determine Output Directory:** Define the base path for documentation: `output_base_dir = Path(shared_settings[\"directory\"]) / \"docs\"`. Use `pathlib.Path` for robust path handling.\n    *   **Create Base Directory:** Ensure the `docs` directory exists: `output_base_dir.mkdir(parents=True, exist_ok=True)`.\n    *   **Iterate and Write Files:** Loop through `results.items()`. For each `file_path`, `data` pair:\n        *   **Construct Markdown Path:** Create the target path, preserving the original file's relative structure within the `docs` directory: `md_path = output_base_dir / Path(file_path).with_suffix(\".md\")`.\n        *   **Ensure Subdirectories:** Make sure the parent directory for the Markdown file exists: `md_path.parent.mkdir(parents=True, exist_ok=True)`.\n        *   **Format Content:** Create the Markdown string:\n            ```markdown\n            # File: {file_path}\n\n            ## Summary\n\n            {data['summary']}\n\n            ## Logic Flow\n\n            {data['logic_flow']}\n            ```\n        *   **Write File:** Write the formatted content to `md_path` (using `with open(...) as f:` and `f.write(...)`, specifying UTF-8 encoding).\n        *   Log output creation (e.g., `print(f\"Generated: {md_path}\")`).\n\n4.  **Dependencies/Settings:**\n    *   No changes anticipated for `call_llm.py`, `fetch_files.py`, or `settings.py` based on this plan. The `pathlib` module is already used in `fetch_files.py`, so it's available.\n\n**Workflow Diagram (Mermaid):**\n\n```mermaid\ngraph TD\n    A[Start: main.py] --> B{Parse Args};\n    B --> C[crawl_files];\n    C --> D{Get Filtered File List};\n    D --> E{Loop Through Files};\n    E -- File Path --> F[Read File Content];\n    F -- Content --> G[Instantiate Summarise Node];\n    F -- Content --> H[Instantiate AnalyzeLogicFlow Node];\n    G -- Content --> I[Call Summarise.exec];\n    H -- Content --> J[Call AnalyzeLogicFlow.exec];\n    I -- Summary Prompt --> K{call_llm};\n    J -- Logic Flow Prompt --> K;\n    K -- Result --> L{Collect Summary};\n    K -- Result --> M{Collect Logic Flow};\n    L & M -- Store Results --> N{results Dict};\n    N --> O{End File Loop};\n    O --> P{Determine Output Dir (target/docs)};\n    P --> Q{Create Output Dir};\n    Q --> R{Loop Through Results};\n    R -- Result Data --> S{Construct MD Path (target/docs/...)};\n    S --> T{Create MD Subdirs};\n    T --> U{Format MD Content};\n    U --> V[Write MD File];\n    V --> W{End Results Loop};\n    W --> X[End];\n\n    subgraph LLM Calls\n        K;\n    end\n\n    subgraph File Processing Loop (main.py)\n        E; F; G; H; I; J; L; M; N; O;\n    end\n\n     subgraph Markdown Output Loop (main.py)\n        P; Q; R; S; T; U; V; W;\n    end\n```": "Okay, this document outlines a plan to enhance the `ai_docs` project to generate both a summary *and* an analysis of the logic flow of Python code, using separate PocketFlow nodes, and outputting the results in Markdown format. Let's break down the algorithm and logic flow described in the plan.\n\n**Overall Purpose:**\n\nThe purpose is to automatically generate documentation for Python code, including both a concise summary and a more detailed description of the code's algorithm/logic.  This automation aims to reduce manual effort in documenting code, improve code understanding, and potentially aid in identifying potential issues or inefficiencies in the logic.\n\n**Algorithm/Logic Flow:**\n\nThe process can be divided into three main stages: setup/instantiation, processing files, and generating Markdown output.\n\n**1. Setup/Instantiation:**\n\n*   **Parse Arguments (Implicit):**  The plan assumes that the list of files to process is obtained via `shared_settings[\"files\"]` and the output directory via  `shared_settings[\"directory\"]`. This suggests that the project takes command-line arguments or has a configuration file to specify these settings. The Mermaid diagram starts with \"Parse Args\", representing this implicit step.\n*   **Instantiate Nodes:**  Two instances of PocketFlow nodes are created:\n    *   `summariser = Summarise()`: This node is responsible for generating a short summary of the code.  Its `exec` method will use a prompt that asks for a 50-word summary.\n    *   `logic_analyzer = AnalyzeLogicFlow()`: This *new* node is responsible for analyzing the code's logic flow. Its `exec` method will use a prompt that instructs the LLM to describe the algorithm, control structures, function calls, and data transformations.\n*   **Initialize Result Storage:** An empty dictionary, `results = {}`, is created.  This dictionary will store the summary and logic flow analysis for each processed file.  The keys will be the file paths, and the values will be dictionaries containing the \"summary\" and \"logic_flow\" strings.\n\n**2. Processing Files (Main Loop):**\n\n*   **Iterate Through Files:** The code iterates through each file path specified in `shared_settings[\"files\"]`.\n*   **Read File Content:** For each `file_path`, the code reads the content of the file into a string variable (named `content` in the plan).  Error handling is mentioned (handling empty files, potential file I/O errors).\n*   **Call Nodes' `exec` Methods:**\n    *   `summary = summariser.exec(content)`: The `exec` method of the `Summarise` node is called with the file content. This is where the LLM is invoked (implicitly through `call_llm`), using the \"Summarize this text in 50 words\" prompt to generate the summary.\n    *   `logic_flow = logic_analyzer.exec(content)`: The `exec` method of the `AnalyzeLogicFlow` node is called with the file content.  This invokes the LLM using the prompt that asks for a detailed logic flow analysis.  This prompt includes the actual code within triple backticks for proper formatting.\n*   **Store Results:**  The generated summary and logic flow analysis are stored in the `results` dictionary: `results[file_path] = {\"summary\": summary, \"logic_flow\": logic_flow}`.\n*   **Log Progress:** A log message (e.g., using `print`) indicates that the file has been processed.\n\n**3. Generate Markdown Output (Output Loop):**\n\n*   **Determine Output Directory:** The target directory for the generated Markdown files is determined by combining the base directory from `shared_settings[\"directory\"]` with the subdirectory \"docs\" (`output_base_dir = Path(shared_settings[\"directory\"]) / \"docs\"`).  The `pathlib` module is used for robust path manipulation.\n*   **Create Output Directory:** The \"docs\" directory (and any necessary parent directories) is created using `output_base_dir.mkdir(parents=True, exist_ok=True)`. `parents=True` creates parent directories if they don't exist, and `exist_ok=True` prevents an error if the directory already exists.\n*   **Iterate Through Results:** The code iterates through the `results` dictionary.\n*   **Construct Markdown Path:** For each file, the target path for the Markdown file is constructed.  This path preserves the original file's relative structure within the \"docs\" directory:  `md_path = output_base_dir / Path(file_path).with_suffix(\".md\")`.  This ensures that if the original file was located in a deeply nested directory structure, the corresponding Markdown file will be created in a matching structure within the \"docs\" directory. The `.with_suffix(\".md\")`  ensures the output file ends with the `.md` extension.\n*   **Ensure Subdirectories:** The parent directory of the Markdown file is created (if it doesn't already exist) to ensure that the file can be written successfully:  `md_path.parent.mkdir(parents=True, exist_ok=True)`.\n*   **Format Content:**  The summary and logic flow analysis are formatted into a Markdown string, including a header for the original file path, \"Summary\" and \"Logic Flow\" sections.\n*   **Write File:** The formatted Markdown content is written to the corresponding `.md` file, using `with open(...) as f:` to ensure the file is properly closed after writing. The `UTF-8` encoding is specified, which is crucial for handling potentially Unicode characters in the code or the generated content.\n*   **Log Output Creation:** A log message indicates that the Markdown file has been generated.\n\n**Control Structures:**\n\n*   **Outer Loop:** Iterates through the list of files to process.\n*   **Inner Loop:** Iterates through the results dictionary to generate Markdown files.\n*   **Conditional Statements:**  The plan mentions error handling during file reading (implicitly using `try...except` or similar) and skipping empty files.  The `mkdir` call uses `exist_ok=True`, effectively acting as a conditional to prevent errors if the directory already exists.\n\n**Data Transformations:**\n\n*   **File Content -> String:** The file content is read into a string variable.\n*   **String -> Summary (LLM):** The LLM (through `Summarise.exec`) transforms the code string into a summary string.\n*   **String -> Logic Flow Analysis (LLM):** The LLM (through `AnalyzeLogicFlow.exec`) transforms the code string into a detailed logic flow analysis string.\n*   **Strings -> Markdown String:** The summary and logic flow strings are formatted into a Markdown string.\n\n**Function Calls:**\n\n*   `Summarise.exec(content)`\n*   `AnalyzeLogicFlow.exec(content)`\n*   `call_llm` (implicitly called by `exec` methods)\n*   `Path.mkdir(parents=True, exist_ok=True)`\n*   `Path.with_suffix(\".md\")`\n*   `open()` with `with` statement for file writing.\n\n**Key Improvements/Considerations:**\n\n*   **Separate PocketFlow Nodes:**  The use of separate `Summarise` and `AnalyzeLogicFlow` nodes allows for customized prompts and potentially different LLM configurations for each task.\n*   **Structured Output:**  The generation of Markdown files with well-defined sections makes the documentation more readable and maintainable.\n*   **Path Handling:** The use of `pathlib` for path manipulation makes the code more robust and platform-independent.\n*   **Error Handling:** The plan mentions error handling for file reading and directory creation.  This is crucial for preventing the program from crashing due to unexpected errors.\n*   **UTF-8 Encoding:**  Specifying `UTF-8` encoding when writing the Markdown files is essential for handling potentially Unicode characters in the code or the generated text.\n*   **LLM Prompt Engineering:** The effectiveness of the documentation heavily relies on the quality of the prompts used in the `Summarise` and `AnalyzeLogicFlow` nodes. The `AnalyzeLogicFlow` prompt, in particular, needs to be carefully designed to elicit a comprehensive and accurate logic flow analysis.\n\nIn summary, the plan describes a well-structured process for automating the generation of code documentation using LLMs, with clear separation of concerns, robust path handling, and structured output.  The success of the plan depends heavily on the prompt engineering for the `AnalyzeLogicFlow` node and the quality of the LLM used by `call_llm`.\n", "Summarize this text in 50 words: \n\n import logging\nfrom pathlib import Path\nfrom pocketflow import Node\nfrom call_llm import call_llm\nfrom settings import (\n    DEFAULT_EXCLUDE_SUFFIXES,\n    DEFAULT_INCLUDE_SUFFIXES,\n    DEFAULT_EXCLUDE_DIRS,\n)\nfrom settings import parse_args\nimport fetch_files\n\n\nclass Summarise(Node):\n    \"\"\"\n    This class identifies abstractions in the code.\n    \"\"\"\n\n    def prep(self,shared):\n        return file_contents[\"data\"]\n    \n    def exec(self, text):\n        prompt = f\"Summarize this text in 50 words: \\n\\n {text}\"\n        return call_llm(prompt)\n    \n    def post(self, shared, prep_res, exec_res):\n        # Store the result in the shared settings\n        file_contents[\"summaries\"] = exec_res\n        return \"default\"\n\n\n# New Node for Logic Flow Analysis\nclass AnalyzeLogicFlow(Node):\n    \"\"\"\n    This class analyzes the code to describe its main algorithm or logic flow.\n    \"\"\"\n\n    # prep and post might not be needed if main loop handles orchestration\n    # def prep(self, shared):\n    #     pass\n\n    def exec(self, text):\n        prompt = f\"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\\n\\nCode:\\n```python\\n{text}\\n```\"\n        # Assuming call_llm handles potential errors and returns a string\n        return call_llm(prompt)\n\n    # def post(self, shared, prep_res, exec_res):\n    #     pass\n\n\ndef main() -> None:\n    args = parse_args()\n\n    # Create a dictionary of settings to be used in the script\n    shared_settings = {\n        \"name\": args.name,\n        \"directory\": args.directory, # Target directory to analyze\n        \"exclude_patterns\": DEFAULT_EXCLUDE_SUFFIXES,\n        \"exclude_dirs\": DEFAULT_EXCLUDE_DIRS,\n        \"include_suffixes\": DEFAULT_INCLUDE_SUFFIXES,\n        \"additional_exclude_dirs\": args.exclude_dirs,\n    }\n    logging.debug(f\"Shared settings: {shared_settings}\")\n\n    # Get a list of files from the target directory\n    # Ensure paths are relative to the target directory for correct output structure\n    target_dir = Path(shared_settings[\"directory\"]).resolve()\n    logging.info(f\"Analyzing directory: {target_dir}\")\n\n    # fetch_files.crawl_files already returns relative paths if called correctly\n    # Let's ensure the base directory passed to crawl_files is the target directory\n    files_to_process = fetch_files.crawl_files(\n        directory=str(target_dir), # Pass absolute path to crawl\n        include_patterns=shared_settings[\"include_suffixes\"],\n        exclude_suffixes=shared_settings[\"exclude_patterns\"],\n        exclude_dirs=shared_settings[\"exclude_dirs\"].union(\n            shared_settings[\"additional_exclude_dirs\"]\n        ),\n    )\n    \n    # Instantiate nodes\n    summariser = Summarise()\n    logic_analyzer = AnalyzeLogicFlow()\n\n    results = {}\n    logging.info(f\"Processing {len(files_to_process)} files...\")\n\n    # Process each file\n    for relative_file_path in files_to_process:\n        absolute_file_path = target_dir / relative_file_path\n        logging.debug(f\"Processing file: {absolute_file_path}\")\n        print(f\"Processing: {relative_file_path}\") # User feedback\n        try:\n            with open(absolute_file_path, \"r\", encoding='utf-8') as f:\n                content = f.read()\n                if content.strip(): # Ensure content is not empty\n                    # Call LLM for summary\n                    summary = summariser.exec(content)\n                    # Call LLM for logic flow\n                    logic_flow = logic_analyzer.exec(content)\n                    results[relative_file_path] = {\"summary\": summary, \"logic_flow\": logic_flow}\n                else:\n                    logging.warning(f\"Skipped empty file: {absolute_file_path}\")\n                    print(f\"Skipped empty file: {relative_file_path}\")\n        except Exception as e:\n            logging.error(f\"Error processing file {absolute_file_path}: {e}\")\n            print(f\"Error processing file {relative_file_path}: {e}\")\n\n    # Generate Markdown output\n    output_base_dir = target_dir / \"docs\"\n    logging.info(f\"Generating documentation in: {output_base_dir}\")\n    try:\n        output_base_dir.mkdir(parents=True, exist_ok=True)\n    except Exception as e:\n        logging.error(f\"Failed to create output directory {output_base_dir}: {e}\")\n        print(f\"Error: Could not create output directory {output_base_dir}. Aborting.\")\n        return # Exit if we can't create the output dir\n\n    for relative_file_path, data in results.items():\n        md_filename = Path(relative_file_path).with_suffix(\".md\")\n        md_path = output_base_dir / md_filename\n        \n        try:\n            # Ensure subdirectory exists\n            md_path.parent.mkdir(parents=True, exist_ok=True)\n\n            # Format Markdown content\n            md_content = f\"\"\"# File: {relative_file_path}\n\n## Summary\n\n{data['summary']}\n\n## Logic Flow\n\n{data['logic_flow']}\n\"\"\"\n            # Write the Markdown file\n            with open(md_path, \"w\", encoding='utf-8') as f:\n                f.write(md_content)\n            logging.info(f\"Generated: {md_path}\")\n            print(f\"Generated: {md_path.relative_to(target_dir)}\") # Show relative path to user\n\n        except Exception as e:\n            logging.error(f\"Failed to write Markdown file {md_path}: {e}\")\n            print(f\"Error writing file {md_path.relative_to(target_dir)}: {e}\")\n\n    print(f\"\\nDocumentation generation complete. Files written to: {output_base_dir}\")\n\n\nif __name__ == \"__main__\":\n    main()\n": "This Python script analyzes code in a directory using LLMs. It crawls files, summarizes their content, and analyzes their logic flow using `Summarise` and `AnalyzeLogicFlow` nodes. Results are stored in Markdown files within a \"docs\" subdirectory.  Command-line arguments control the target directory and exclusion patterns.\n", "Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\n\nCode:\n```python\nimport logging\nfrom pathlib import Path\nfrom pocketflow import Node\nfrom call_llm import call_llm\nfrom settings import (\n    DEFAULT_EXCLUDE_SUFFIXES,\n    DEFAULT_INCLUDE_SUFFIXES,\n    DEFAULT_EXCLUDE_DIRS,\n)\nfrom settings import parse_args\nimport fetch_files\n\n\nclass Summarise(Node):\n    \"\"\"\n    This class identifies abstractions in the code.\n    \"\"\"\n\n    def prep(self,shared):\n        return file_contents[\"data\"]\n    \n    def exec(self, text):\n        prompt = f\"Summarize this text in 50 words: \\n\\n {text}\"\n        return call_llm(prompt)\n    \n    def post(self, shared, prep_res, exec_res):\n        # Store the result in the shared settings\n        file_contents[\"summaries\"] = exec_res\n        return \"default\"\n\n\n# New Node for Logic Flow Analysis\nclass AnalyzeLogicFlow(Node):\n    \"\"\"\n    This class analyzes the code to describe its main algorithm or logic flow.\n    \"\"\"\n\n    # prep and post might not be needed if main loop handles orchestration\n    # def prep(self, shared):\n    #     pass\n\n    def exec(self, text):\n        prompt = f\"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\\n\\nCode:\\n```python\\n{text}\\n```\"\n        # Assuming call_llm handles potential errors and returns a string\n        return call_llm(prompt)\n\n    # def post(self, shared, prep_res, exec_res):\n    #     pass\n\n\ndef main() -> None:\n    args = parse_args()\n\n    # Create a dictionary of settings to be used in the script\n    shared_settings = {\n        \"name\": args.name,\n        \"directory\": args.directory, # Target directory to analyze\n        \"exclude_patterns\": DEFAULT_EXCLUDE_SUFFIXES,\n        \"exclude_dirs\": DEFAULT_EXCLUDE_DIRS,\n        \"include_suffixes\": DEFAULT_INCLUDE_SUFFIXES,\n        \"additional_exclude_dirs\": args.exclude_dirs,\n    }\n    logging.debug(f\"Shared settings: {shared_settings}\")\n\n    # Get a list of files from the target directory\n    # Ensure paths are relative to the target directory for correct output structure\n    target_dir = Path(shared_settings[\"directory\"]).resolve()\n    logging.info(f\"Analyzing directory: {target_dir}\")\n\n    # fetch_files.crawl_files already returns relative paths if called correctly\n    # Let's ensure the base directory passed to crawl_files is the target directory\n    files_to_process = fetch_files.crawl_files(\n        directory=str(target_dir), # Pass absolute path to crawl\n        include_patterns=shared_settings[\"include_suffixes\"],\n        exclude_suffixes=shared_settings[\"exclude_patterns\"],\n        exclude_dirs=shared_settings[\"exclude_dirs\"].union(\n            shared_settings[\"additional_exclude_dirs\"]\n        ),\n    )\n    \n    # Instantiate nodes\n    summariser = Summarise()\n    logic_analyzer = AnalyzeLogicFlow()\n\n    results = {}\n    logging.info(f\"Processing {len(files_to_process)} files...\")\n\n    # Process each file\n    for relative_file_path in files_to_process:\n        absolute_file_path = target_dir / relative_file_path\n        logging.debug(f\"Processing file: {absolute_file_path}\")\n        print(f\"Processing: {relative_file_path}\") # User feedback\n        try:\n            with open(absolute_file_path, \"r\", encoding='utf-8') as f:\n                content = f.read()\n                if content.strip(): # Ensure content is not empty\n                    # Call LLM for summary\n                    summary = summariser.exec(content)\n                    # Call LLM for logic flow\n                    logic_flow = logic_analyzer.exec(content)\n                    results[relative_file_path] = {\"summary\": summary, \"logic_flow\": logic_flow}\n                else:\n                    logging.warning(f\"Skipped empty file: {absolute_file_path}\")\n                    print(f\"Skipped empty file: {relative_file_path}\")\n        except Exception as e:\n            logging.error(f\"Error processing file {absolute_file_path}: {e}\")\n            print(f\"Error processing file {relative_file_path}: {e}\")\n\n    # Generate Markdown output\n    output_base_dir = target_dir / \"docs\"\n    logging.info(f\"Generating documentation in: {output_base_dir}\")\n    try:\n        output_base_dir.mkdir(parents=True, exist_ok=True)\n    except Exception as e:\n        logging.error(f\"Failed to create output directory {output_base_dir}: {e}\")\n        print(f\"Error: Could not create output directory {output_base_dir}. Aborting.\")\n        return # Exit if we can't create the output dir\n\n    for relative_file_path, data in results.items():\n        md_filename = Path(relative_file_path).with_suffix(\".md\")\n        md_path = output_base_dir / md_filename\n        \n        try:\n            # Ensure subdirectory exists\n            md_path.parent.mkdir(parents=True, exist_ok=True)\n\n            # Format Markdown content\n            md_content = f\"\"\"# File: {relative_file_path}\n\n## Summary\n\n{data['summary']}\n\n## Logic Flow\n\n{data['logic_flow']}\n\"\"\"\n            # Write the Markdown file\n            with open(md_path, \"w\", encoding='utf-8') as f:\n                f.write(md_content)\n            logging.info(f\"Generated: {md_path}\")\n            print(f\"Generated: {md_path.relative_to(target_dir)}\") # Show relative path to user\n\n        except Exception as e:\n            logging.error(f\"Failed to write Markdown file {md_path}: {e}\")\n            print(f\"Error writing file {md_path.relative_to(target_dir)}: {e}\")\n\n    print(f\"\\nDocumentation generation complete. Files written to: {output_base_dir}\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n```": "```python\nimport logging\nfrom pathlib import Path\nfrom pocketflow import Node\nfrom call_llm import call_llm\nfrom settings import (\n    DEFAULT_EXCLUDE_SUFFIXES,\n    DEFAULT_INCLUDE_SUFFIXES,\n    DEFAULT_EXCLUDE_DIRS,\n)\nfrom settings import parse_args\nimport fetch_files\n\n\nclass Summarise(Node):\n    \"\"\"\n    This class identifies abstractions in the code.\n    \"\"\"\n\n    def prep(self,shared):\n        return file_contents[\"data\"]\n    \n    def exec(self, text):\n        prompt = f\"Summarize this text in 50 words: \\n\\n {text}\"\n        return call_llm(prompt)\n    \n    def post(self, shared, prep_res, exec_res):\n        # Store the result in the shared settings\n        file_contents[\"summaries\"] = exec_res\n        return \"default\"\n\n\n# New Node for Logic Flow Analysis\nclass AnalyzeLogicFlow(Node):\n    \"\"\"\n    This class analyzes the code to describe its main algorithm or logic flow.\n    \"\"\"\n\n    # prep and post might not be needed if main loop handles orchestration\n    # def prep(self, shared):\n    #     pass\n\n    def exec(self, text):\n        prompt = f\"Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\\n\\nCode:\\n```python\\n{text}\\n```\"\n        # Assuming call_llm handles potential errors and returns a string\n        return call_llm(prompt)\n\n    # def post(self, shared, prep_res, exec_res):\n    #     pass\n\n\ndef main() -> None:\n    args = parse_args()\n\n    # Create a dictionary of settings to be used in the script\n    shared_settings = {\n        \"name\": args.name,\n        \"directory\": args.directory, # Target directory to analyze\n        \"exclude_patterns\": DEFAULT_EXCLUDE_SUFFIXES,\n        \"exclude_dirs\": DEFAULT_EXCLUDE_DIRS,\n        \"include_suffixes\": DEFAULT_INCLUDE_SUFFIXES,\n        \"additional_exclude_dirs\": args.exclude_dirs,\n    }\n    logging.debug(f\"Shared settings: {shared_settings}\")\n\n    # Get a list of files from the target directory\n    # Ensure paths are relative to the target directory for correct output structure\n    target_dir = Path(shared_settings[\"directory\"]).resolve()\n    logging.info(f\"Analyzing directory: {target_dir}\")\n\n    # fetch_files.crawl_files already returns relative paths if called correctly\n    # Let's ensure the base directory passed to crawl_files is the target directory\n    files_to_process = fetch_files.crawl_files(\n        directory=str(target_dir), # Pass absolute path to crawl\n        include_patterns=shared_settings[\"include_suffixes\"],\n        exclude_suffixes=shared_settings[\"exclude_patterns\"],\n        exclude_dirs=shared_settings[\"exclude_dirs\"].union(\n            shared_settings[\"additional_exclude_dirs\"]\n        ),\n    )\n    \n    # Instantiate nodes\n    summariser = Summarise()\n    logic_analyzer = AnalyzeLogicFlow()\n\n    results = {}\n    logging.info(f\"Processing {len(files_to_process)} files...\")\n\n    # Process each file\n    for relative_file_path in files_to_process:\n        absolute_file_path = target_dir / relative_file_path\n        logging.debug(f\"Processing file: {absolute_file_path}\")\n        print(f\"Processing: {relative_file_path}\") # User feedback\n        try:\n            with open(absolute_file_path, \"r\", encoding='utf-8') as f:\n                content = f.read()\n                if content.strip(): # Ensure content is not empty\n                    # Call LLM for summary\n                    summary = summariser.exec(content)\n                    # Call LLM for logic flow\n                    logic_flow = logic_analyzer.exec(content)\n                    results[relative_file_path] = {\"summary\": summary, \"logic_flow\": logic_flow}\n                else:\n                    logging.warning(f\"Skipped empty file: {absolute_file_path}\")\n                    print(f\"Skipped empty file: {relative_file_path}\")\n        except Exception as e:\n            logging.error(f\"Error processing file {absolute_file_path}: {e}\")\n            print(f\"Error processing file {relative_file_path}: {e}\")\n\n    # Generate Markdown output\n    output_base_dir = target_dir / \"docs\"\n    logging.info(f\"Generating documentation in: {output_base_dir}\")\n    try:\n        output_base_dir.mkdir(parents=True, exist_ok=True)\n    except Exception as e:\n        logging.error(f\"Failed to create output directory {output_base_dir}: {e}\")\n        print(f\"Error: Could not create output directory {output_base_dir}. Aborting.\")\n        return # Exit if we can't create the output dir\n\n    for relative_file_path, data in results.items():\n        md_filename = Path(relative_file_path).with_suffix(\".md\")\n        md_path = output_base_dir / md_filename\n        \n        try:\n            # Ensure subdirectory exists\n            md_path.parent.mkdir(parents=True, exist_ok=True)\n\n            # Format Markdown content\n            md_content = f\"\"\"# File: {relative_file_path}\n\n## Summary\n\n{data['summary']}\n\n## Logic Flow\n\n{data['logic_flow']}\n\"\"\"\n            # Write the Markdown file\n            with open(md_path, \"w\", encoding='utf-8') as f:\n                f.write(md_content)\n            logging.info(f\"Generated: {md_path}\")\n            print(f\"Generated: {md_path.relative_to(target_dir)}\") # Show relative path to user\n\n        except Exception as e:\n            logging.error(f\"Failed to write Markdown file {md_path}: {e}\")\n            print(f\"Error writing file {md_path.relative_to(target_dir)}: {e}\")\n\n    print(f\"\\nDocumentation generation complete. Files written to: {output_base_dir}\")\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Code Description\n\nThe Python code defines a program that analyzes source code files within a specified directory and generates Markdown documentation containing summaries and logic flow descriptions of each file using a Large Language Model (LLM). It leverages the `pocketflow` library for defining processing nodes, the `pathlib` library for file system operations, and a custom `call_llm` function to interact with the LLM.\n\n### Execution Analysis\n\n1.  **Initialization:**\n    *   The `main` function begins by parsing command-line arguments using `parse_args()` and stores them in the `args` variable.\n    *   It creates a `shared_settings` dictionary to hold configuration parameters, including the target directory, file inclusion/exclusion patterns, and other settings.\n    *   Logging is initialized for debugging and error tracking.\n    *   The target directory is resolved to an absolute path using `Path(shared_settings[\"directory\"]).resolve()`.\n\n2.  **File Discovery:**\n    *   The `fetch_files.crawl_files()` function is called to retrieve a list of files to be processed within the target directory.  It accepts the directory path, inclusion/exclusion patterns, and exclusion directories as arguments. The result, `files_to_process`, is a list of *relative* file paths.\n\n3.  **Node Instantiation:**\n    *   Instances of the `Summarise` and `AnalyzeLogicFlow` classes (derived from `pocketflow.Node`) are created.  These nodes encapsulate the logic for summarizing and analyzing the logic flow of the code.\n\n4.  **File Processing Loop:**\n    *   The code iterates through the `files_to_process` list. Inside the loop:\n        *   The absolute path of the current file is constructed by joining the target directory with the relative file path.\n        *   The file is opened in read mode (`'r'`) with UTF-8 encoding.\n        *   The file content is read into the `content` variable.\n        *   A check is performed to ensure that the file content is not empty (`content.strip()`). This prevents the program from making calls to the LLM with empty files.\n        *   If the file is not empty:\n            *   The `summariser.exec(content)` function is called to generate a summary of the file content.  This passes the file content as input to the LLM.\n            *   The `logic_analyzer.exec(content)` function is called to analyze the logic flow of the file content. This also passes the file content to the LLM.\n            *   The summary and logic flow are stored in the `results` dictionary, keyed by the relative file path.\n        *   If the file is empty, a warning message is logged and printed to the console.\n        *   A `try...except` block handles potential exceptions that may occur during file processing (e.g., file not found, permission errors).\n\n5.  **Markdown Output Generation:**\n    *   The code creates an output directory (named \"docs\" inside the target directory).\n    *   It iterates through the `results` dictionary. For each file:\n        *   A Markdown file name is generated by replacing the file's suffix with \".md\".\n        *   The code constructs the full path to the Markdown file within the output directory.\n        *   The parent directories of the Markdown file are created if they don't exist.\n        *   The Markdown content is formatted to include the file name, summary, and logic flow analysis.\n        *   The Markdown content is written to the file with UTF-8 encoding.\n        *   Logging and console output indicate the successful generation of the Markdown file.\n        *   A `try...except` block handles potential exceptions during file writing.\n\n6.  **Completion:**\n    *   A completion message is printed to the console, indicating the location of the generated documentation.\n\n### Algorithm and Logic Flow\n\nThe program implements a straightforward, sequential algorithm:\n\n1.  **Configuration:** Load settings and parse command-line arguments.\n2.  **File Discovery:** Crawl the target directory to find files matching the specified criteria.\n3.  **Iterative Processing:** For each file found:\n    *   Read the file content.\n    *   If the file is not empty:\n        *   Call the LLM to summarize the file content.\n        *   Call the LLM to analyze the logic flow of the file content.\n        *   Store the summary and logic flow in a results dictionary.\n4.  **Output Generation:** For each processed file:\n    *   Create a corresponding Markdown file in the output directory.\n    *   Write the file name, summary, and logic flow to the Markdown file.\n\nThe program uses a `for` loop to iterate through the files. It employs `try...except` blocks for error handling during file reading and writing. Conditional statements (`if content.strip()`) are used to avoid processing empty files. The core logic relies on the external `call_llm` function, which is assumed to handle the actual interaction with the LLM.\n\n### Data Transformations\n\n*   **Input:** The program takes a target directory and file inclusion/exclusion patterns as input.\n*   **File Content:** The content of each file is read as a string.\n*   **LLM Interaction:** The file content is transformed into a prompt for the LLM, and the LLM returns a string containing the summary and logic flow analysis.\n*   **Output:** The summary and logic flow are formatted into Markdown content and written to output files.\n\n### Purpose\n\nThe program's purpose is to automatically generate documentation for source code files using an LLM. It aims to provide concise summaries and high-level logic flow descriptions, which can be useful for understanding and maintaining large codebases. The generated Markdown files can be easily integrated into documentation systems.\n", "Summarize this text in 50 words: \n\n from google import genai\nimport os\nimport logging\nimport json\nfrom datetime import datetime\n\n\n# Configure logging\nlog_directory = os.getenv(\"LOG_DIR\", \"logs\")\nos.makedirs(log_directory, exist_ok=True)\nlog_file = os.path.join(\n    log_directory, f\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\"\n)\n\n# Set up logger\nlogger = logging.getLogger(\"llm_logger\")\nlogger.setLevel(logging.INFO)\nlogger.propagate = False  # Prevent propagation to root logger\nfile_handler = logging.FileHandler(log_file)\nfile_handler.setFormatter(\n    logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n)\nlogger.addHandler(file_handler)\n\n# Simple cache configuration\ncache_file = \"llm_cache.json\"\n\n\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\n    # Log the prompt\n    logger.info(f\"PROMPT: {prompt}\")\n\n    # Check cache if enabled\n    if use_cache:\n        # Load cache from disk\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, \"r\") as f:\n                    cache = json.load(f)\n            except:\n                logger.warning(f\"Failed to load cache, starting with empty cache\")\n\n        # Return from cache if exists\n        if prompt in cache:\n            logger.info(f\"RESPONSE: {cache[prompt]}\")\n            return cache[prompt]\n\n    # Call the LLM if not in cache or cache disabled\n    client = genai.Client(\n        vertexai=True,\n        project=os.getenv(\"GEMINI_PROJECT_ID\", \"ai-sre-dev-84b7\"),\n        location=os.getenv(\"GEMINI_LOCATION\", \"us-central1\")\n    )\n\n    model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.0-flash-exp\")\n    \n    response = client.models.generate_content(model=model, contents=[prompt])\n    response_text = response.text\n\n    # Log the response\n    logger.info(f\"RESPONSE: {response_text}\")\n\n    # Update cache if enabled\n    if use_cache:\n        # Load cache again to avoid overwrites\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, \"r\") as f:\n                    cache = json.load(f)\n            except:\n                pass\n\n        # Add to cache and save\n        cache[prompt] = response_text\n        try:\n            with open(cache_file, \"w\") as f:\n                json.dump(cache, f)\n        except Exception as e:\n            logger.error(f\"Failed to save cache: {e}\")\n\n    return response_text\n\n\nif __name__ == \"__main__\":\n    test_prompt = \"Hello, how are you?\"\n\n    # First call - should hit the API\n    print(\"Making call...\")\n    response1 = call_llm(test_prompt, use_cache=False)\n    print(f\"Response: {response1}\")\n": "This code defines a function `call_llm` to interact with a Google LLM, logging prompts and responses to a daily log file. It optionally uses a JSON file (`llm_cache.json`) for caching prompts and their corresponding responses to avoid redundant API calls. A test call is performed to demonstrate functionality.\n", "Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\n\nCode:\n```python\nfrom google import genai\nimport os\nimport logging\nimport json\nfrom datetime import datetime\n\n\n# Configure logging\nlog_directory = os.getenv(\"LOG_DIR\", \"logs\")\nos.makedirs(log_directory, exist_ok=True)\nlog_file = os.path.join(\n    log_directory, f\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\"\n)\n\n# Set up logger\nlogger = logging.getLogger(\"llm_logger\")\nlogger.setLevel(logging.INFO)\nlogger.propagate = False  # Prevent propagation to root logger\nfile_handler = logging.FileHandler(log_file)\nfile_handler.setFormatter(\n    logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n)\nlogger.addHandler(file_handler)\n\n# Simple cache configuration\ncache_file = \"llm_cache.json\"\n\n\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\n    # Log the prompt\n    logger.info(f\"PROMPT: {prompt}\")\n\n    # Check cache if enabled\n    if use_cache:\n        # Load cache from disk\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, \"r\") as f:\n                    cache = json.load(f)\n            except:\n                logger.warning(f\"Failed to load cache, starting with empty cache\")\n\n        # Return from cache if exists\n        if prompt in cache:\n            logger.info(f\"RESPONSE: {cache[prompt]}\")\n            return cache[prompt]\n\n    # Call the LLM if not in cache or cache disabled\n    client = genai.Client(\n        vertexai=True,\n        project=os.getenv(\"GEMINI_PROJECT_ID\", \"ai-sre-dev-84b7\"),\n        location=os.getenv(\"GEMINI_LOCATION\", \"us-central1\")\n    )\n\n    model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.0-flash-exp\")\n    \n    response = client.models.generate_content(model=model, contents=[prompt])\n    response_text = response.text\n\n    # Log the response\n    logger.info(f\"RESPONSE: {response_text}\")\n\n    # Update cache if enabled\n    if use_cache:\n        # Load cache again to avoid overwrites\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, \"r\") as f:\n                    cache = json.load(f)\n            except:\n                pass\n\n        # Add to cache and save\n        cache[prompt] = response_text\n        try:\n            with open(cache_file, \"w\") as f:\n                json.dump(cache, f)\n        except Exception as e:\n            logger.error(f\"Failed to save cache: {e}\")\n\n    return response_text\n\n\nif __name__ == \"__main__\":\n    test_prompt = \"Hello, how are you?\"\n\n    # First call - should hit the API\n    print(\"Making call...\")\n    response1 = call_llm(test_prompt, use_cache=False)\n    print(f\"Response: {response1}\")\n\n```": "```python\nfrom google import genai\nimport os\nimport logging\nimport json\nfrom datetime import datetime\n\n\n# Configure logging\nlog_directory = os.getenv(\"LOG_DIR\", \"logs\")\nos.makedirs(log_directory, exist_ok=True)\nlog_file = os.path.join(\n    log_directory, f\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\"\n)\n\n# Set up logger\nlogger = logging.getLogger(\"llm_logger\")\nlogger.setLevel(logging.INFO)\nlogger.propagate = False  # Prevent propagation to root logger\nfile_handler = logging.FileHandler(log_file)\nfile_handler.setFormatter(\n    logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n)\nlogger.addHandler(file_handler)\n\n# Simple cache configuration\ncache_file = \"llm_cache.json\"\n\n\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\n    # Log the prompt\n    logger.info(f\"PROMPT: {prompt}\")\n\n    # Check cache if enabled\n    if use_cache:\n        # Load cache from disk\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, \"r\") as f:\n                    cache = json.load(f)\n            except:\n                logger.warning(f\"Failed to load cache, starting with empty cache\")\n\n        # Return from cache if exists\n        if prompt in cache:\n            logger.info(f\"RESPONSE: {cache[prompt]}\")\n            return cache[prompt]\n\n    # Call the LLM if not in cache or cache disabled\n    client = genai.Client(\n        vertexai=True,\n        project=os.getenv(\"GEMINI_PROJECT_ID\", \"ai-sre-dev-84b7\"),\n        location=os.getenv(\"GEMINI_LOCATION\", \"us-central1\")\n    )\n\n    model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.0-flash-exp\")\n    \n    response = client.models.generate_content(model=model, contents=[prompt])\n    response_text = response.text\n\n    # Log the response\n    logger.info(f\"RESPONSE: {response_text}\")\n\n    # Update cache if enabled\n    if use_cache:\n        # Load cache again to avoid overwrites\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, \"r\") as f:\n                    cache = json.load(f)\n            except:\n                pass\n\n        # Add to cache and save\n        cache[prompt] = response_text\n        try:\n            with open(cache_file, \"w\") as f:\n                json.dump(cache, f)\n        except Exception as e:\n            logger.error(f\"Failed to save cache: {e}\")\n\n    return response_text\n\n\nif __name__ == \"__main__\":\n    test_prompt = \"Hello, how are you?\"\n\n    # First call - should hit the API\n    print(\"Making call...\")\n    response1 = call_llm(test_prompt, use_cache=False)\n    print(f\"Response: {response1}\")\n```\n\n**Code Description**\n\nThe Python code defines a function `call_llm` that interacts with a Large Language Model (LLM) from Google's `genai` library. It also implements a simple caching mechanism to avoid unnecessary API calls. The script also includes basic logging.\n\n**Algorithm and Logic Flow**\n\n1.  **Initialization:**\n    *   It starts by importing necessary libraries (`google.genai`, `os`, `logging`, `json`, `datetime`).\n    *   It configures a logger to record prompts and responses to a log file. The log file's name includes the current date.  The logging level is set to `INFO`, and log messages are formatted with a timestamp, level, and the message itself.\n    *   It defines the name of a cache file (`llm_cache.json`).\n\n2.  **`call_llm(prompt: str, use_cache: bool = True) -> str` Function:**\n    *   Takes a `prompt` string (the text to send to the LLM) and a `use_cache` boolean (defaulting to `True`) as input.\n    *   Logs the input `prompt` using the logger.\n    *   **Caching (if `use_cache` is True):**\n        *   Loads the cache from the `llm_cache.json` file. If the file doesn't exist or loading fails, it starts with an empty cache.\n        *   Checks if the `prompt` already exists as a key in the `cache`.\n        *   If the `prompt` is found in the cache, it logs the cached `response` and returns it immediately, avoiding the LLM call.\n    *   **LLM Interaction (if `use_cache` is False or prompt not in cache):**\n        *   Creates a `genai.Client` object to connect to the LLM service. It retrieves the project ID and location from environment variables, with fallback defaults.\n        *   It gets the model name from environment variables, with a fallback default.\n        *   Calls the LLM using `client.models.generate_content`, passing the `prompt`.\n        *   Extracts the response text from the LLM's response.\n        *   Logs the `response_text` using the logger.\n    *   **Cache Update (if `use_cache` is True):**\n        *   Loads the cache from file again to avoid overwriting any changes made by other processes.\n        *   Adds the `prompt` and `response_text` to the `cache` dictionary.\n        *   Saves the updated `cache` to the `llm_cache.json` file.  It catches potential exceptions during saving and logs an error message if the save fails.\n    *   Returns the `response_text`.\n\n3.  **`if __name__ == \"__main__\":` Block (Main Execution):**\n    *   Defines a sample `test_prompt`.\n    *   Calls the `call_llm` function *without* caching enabled (`use_cache=False`). This ensures a fresh call to the LLM.\n    *   Prints the `response` to the console.\n\n**Purpose of the Code**\n\nThe code provides a reusable function (`call_llm`) for interacting with a Google LLM.  Its primary purpose is to:\n\n1.  **Abstract LLM interaction:**  It encapsulates the complexity of making API calls to the LLM service.\n2.  **Implement Caching:** It uses a simple file-based cache to store prompts and their corresponding responses, which can significantly reduce the number of API calls and improve response times for repeated prompts.\n3.  **Provide Logging:** It logs prompts and responses, which is valuable for debugging, monitoring, and auditing purposes.\n4.  **Configuration via Environment Variables:** It uses environment variables to configure the LLM project, location, and model, making it more flexible and adaptable to different environments.\n\nIn essence, it's a wrapper around the `genai` library that adds caching and logging capabilities to simplify and optimize LLM usage.\n", "Summarize this text in 50 words: \n\n {\"Summarize this text in 50 words: \\n\\n import typing\\nimport argparse\\nimport os\\n\\n\\nDEFAULT_INCLUDE_SUFFIXES = {\\n    \\\".js\\\",\\n    \\\".json\\\",\\n    \\\".jsx\\\",\\n    \\\".md\\\",\\n    \\\".py\\\",\\n    \\\".pyi\\\",\\n    \\\".pyx\\\",\\n    \\\".rst\\\",\\n    \\\".tf\\\",\\n    \\\".tfvars\\\",\\n    \\\".ts\\\",\\n    \\\".tsx\\\",\\n    \\\".yaml\\\",\\n    \\\".yml\\\",\\n    \\\"Dockerfile\\\",\\n    \\\"Jenkinsfil\\\",\\n}\\n\\nDEFAULT_EXCLUDE_SUFFIXES = {}\\n\\nDEFAULT_EXCLUDE_DIRS = {\\n    \\\".git\\\",\\n    \\\".github\\\",\\n    \\\".terraform\\\",\\n    \\\"assets\\\",\\n    \\\"build\\\",\\n    \\\"dist\\\",\\n    \\\"env\\\",\\n    \\\"docs\\\",\\n    \\\"env\\\",\\n    \\\"images\\\",\\n    \\\"temp\\\",\\n    \\\"tests\\\",\\n    \\\"tmp\\\",\\n    \\\"venv\\\",\\n}\\n\\n\\ndef parse_args() -> typing.Any:\\n    \\\"\\\"\\\"\\n    Parse command line arguments.\\n    :return: Parsed arguments\\n    \\\"\\\"\\\"\\n    parser = argparse.ArgumentParser(\\n        description=\\\"Generate documentation for a project using AI.\\\",\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\n    )\\n\\n    parser.add_argument(\\n        \\\"-n\\\",\\n        \\\"--name\\\",\\n        help=\\\"local Repository name\\\",\\n        default=\\\"repo_name\\\",\\n        )\\n\\n    parser.add_argument(\\n        \\\"-d\\\",\\n        \\\"--directory\\\",\\n        help=\\\"top level directory to scan, default is current working directory\\\",\\n        default=os.getcwd(),\\n        )\\n\\n    parser.add_argument(\\n        \\\"-e\\\",\\n        \\\"--exclude_dirs\\\",\\n        nargs='*',\\n        help=\\\"comma separated list of directories to exclude\\\",\\n        default=\\\",\\\"\\n        )\\n\\n    return parser.parse_args()\\n\\nmodel = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.5-pro-exp-03-25\\\")\\nproject=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"sre-ai-dev\\\")\": \"This code defines default file suffixes to include/exclude for documentation generation. It also specifies directories to exclude from the scan, like \\\".git\\\" and \\\"venv.\\\"  The `parse_args` function handles command-line arguments for repository name, directory, and excluded directories.  Finally, it fetches environment variables for the Gemini model and project ID.\\n\", \"Summarize this text in 50 words: \\n\\n {'settings.py': 'import typing\\\\nimport argparse\\\\nimport os\\\\n\\\\n\\\\nDEFAULT_INCLUDE_SUFFIXES = {\\\\n    \\\".js\\\",\\\\n    \\\".json\\\",\\\\n    \\\".jsx\\\",\\\\n    \\\".md\\\",\\\\n    \\\".py\\\",\\\\n    \\\".pyi\\\",\\\\n    \\\".pyx\\\",\\\\n    \\\".rst\\\",\\\\n    \\\".tf\\\",\\\\n    \\\".tfvars\\\",\\\\n    \\\".ts\\\",\\\\n    \\\".tsx\\\",\\\\n    \\\".yaml\\\",\\\\n    \\\".yml\\\",\\\\n    \\\"Dockerfile\\\",\\\\n    \\\"Jenkinsfil\\\",\\\\n}\\\\n\\\\nDEFAULT_EXCLUDE_SUFFIXES = {}\\\\n\\\\nDEFAULT_EXCLUDE_DIRS = {\\\\n    \\\".git\\\",\\\\n    \\\".github\\\",\\\\n    \\\".terraform\\\",\\\\n    \\\"assets\\\",\\\\n    \\\"build\\\",\\\\n    \\\"dist\\\",\\\\n    \\\"env\\\",\\\\n    \\\"docs\\\",\\\\n    \\\"env\\\",\\\\n    \\\"images\\\",\\\\n    \\\"temp\\\",\\\\n    \\\"tests\\\",\\\\n    \\\"tmp\\\",\\\\n    \\\"venv\\\",\\\\n}\\\\n\\\\n\\\\ndef parse_args() -> typing.Any:\\\\n    \\\"\\\"\\\"\\\\n    Parse command line arguments.\\\\n    :return: Parsed arguments\\\\n    \\\"\\\"\\\"\\\\n    parser = argparse.ArgumentParser(\\\\n        description=\\\"Generate documentation for a project using AI.\\\",\\\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\\\n    )\\\\n\\\\n    parser.add_argument(\\\\n        \\\"-n\\\",\\\\n        \\\"--name\\\",\\\\n        help=\\\"local Repository name\\\",\\\\n        default=\\\"repo_name\\\",\\\\n        )\\\\n\\\\n    parser.add_argument(\\\\n        \\\"-d\\\",\\\\n        \\\"--directory\\\",\\\\n        help=\\\"top level directory to scan, default is current working directory\\\",\\\\n        default=os.getcwd(),\\\\n        )\\\\n\\\\n    parser.add_argument(\\\\n        \\\"-e\\\",\\\\n        \\\"--exclude_dirs\\\",\\\\n        nargs=\\\\'*\\\\',\\\\n        help=\\\"comma separated list of directories to exclude\\\",\\\\n        default=\\\",\\\"\\\\n        )\\\\n\\\\n    return parser.parse_args()\\\\n\\\\nmodel = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.5-pro-exp-03-25\\\")\\\\nproject=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"sre-ai-dev\\\")', 'llm_cache.json': '{\\\"Summarize this text in 50 words: \\\\\\\\n\\\\\\\\n import typing\\\\\\\\nimport argparse\\\\\\\\nimport os\\\\\\\\n\\\\\\\\n\\\\\\\\nDEFAULT_INCLUDE_SUFFIXES = {\\\\\\\\n    \\\\\\\\\\\".js\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".json\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".jsx\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".md\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".py\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".pyi\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".pyx\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".rst\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".tf\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".tfvars\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".ts\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".tsx\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".yaml\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".yml\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"Dockerfile\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"Jenkinsfil\\\\\\\\\\\",\\\\\\\\n}\\\\\\\\n\\\\\\\\nDEFAULT_EXCLUDE_SUFFIXES = {}\\\\\\\\n\\\\\\\\nDEFAULT_EXCLUDE_DIRS = {\\\\\\\\n    \\\\\\\\\\\".git\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".github\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".terraform\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"assets\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"build\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"dist\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"env\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"docs\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"env\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"images\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"temp\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"tests\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"tmp\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"venv\\\\\\\\\\\",\\\\\\\\n}\\\\\\\\n\\\\\\\\n\\\\\\\\ndef parse_args() -> typing.Any:\\\\\\\\n    \\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\n    Parse command line arguments.\\\\\\\\n    :return: Parsed arguments\\\\\\\\n    \\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\n    parser = argparse.ArgumentParser(\\\\\\\\n        description=\\\\\\\\\\\"Generate documentation for a project using AI.\\\\\\\\\\\",\\\\\\\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\\\\\\\n    )\\\\\\\\n\\\\\\\\n    parser.add_argument(\\\\\\\\n        \\\\\\\\\\\"-n\\\\\\\\\\\",\\\\\\\\n        \\\\\\\\\\\"--name\\\\\\\\\\\",\\\\\\\\n        help=\\\\\\\\\\\"local Repository name\\\\\\\\\\\",\\\\\\\\n        default=\\\\\\\\\\\"repo_name\\\\\\\\\\\",\\\\\\\\n        )\\\\\\\\n\\\\\\\\n    parser.add_argument(\\\\\\\\n        \\\\\\\\\\\"-d\\\\\\\\\\\",\\\\\\\\n        \\\\\\\\\\\"--directory\\\\\\\\\\\",\\\\\\\\n        help=\\\\\\\\\\\"top level directory to scan, default is current working directory\\\\\\\\\\\",\\\\\\\\n        default=os.getcwd(),\\\\\\\\n        )\\\\\\\\n\\\\\\\\n    parser.add_argument(\\\\\\\\n        \\\\\\\\\\\"-e\\\\\\\\\\\",\\\\\\\\n        \\\\\\\\\\\"--exclude_dirs\\\\\\\\\\\",\\\\\\\\n        nargs=\\\\'*\\\\',\\\\\\\\n        help=\\\\\\\\\\\"comma separated list of directories to exclude\\\\\\\\\\\",\\\\\\\\n        default=\\\\\\\\\\\",\\\\\\\\\\\"\\\\\\\\n        )\\\\\\\\n\\\\\\\\n    return parser.parse_args()\\\\\\\\n\\\\\\\\nmodel = os.getenv(\\\\\\\\\\\"GEMINI_MODEL\\\\\\\\\\\", \\\\\\\\\\\"gemini-2.5-pro-exp-03-25\\\\\\\\\\\")\\\\\\\\nproject=os.getenv(\\\\\\\\\\\"GEMINI_PROJECT_ID\\\\\\\\\\\", \\\\\\\\\\\"sre-ai-dev\\\\\\\\\\\")\\\": \\\"This code defines default file suffixes to include/exclude for documentation generation. It also specifies directories to exclude from the scan, like \\\\\\\\\\\".git\\\\\\\\\\\" and \\\\\\\\\\\"venv.\\\\\\\\\\\"  The `parse_args` function handles command-line arguments for repository name, directory, and excluded directories.  Finally, it fetches environment variables for the Gemini model and project ID.\\\\\\\\n\\\"}', 'fetch_files.py': '# A script to crawl files from a given directory and its subdirectories.\\\\n# It uses the `os` module to walk through the directory structure and collects file paths based on specified include and exclude patterns.\\\\n# files that are not in the exclude patterns and are in the include patterns are collected.\\\\nimport os\\\\nimport typing\\\\nimport logging\\\\nfrom pathlib import Path\\\\nfrom settings import DEFAULT_EXCLUDE_SUFFIXES, DEFAULT_INCLUDE_SUFFIXES, DEFAULT_EXCLUDE_DIRS\\\\n\\\\n\\\\n\\\\ndef crawl_files(\\\\n    directory: str,\\\\n    include_patterns: typing.Set[str] = DEFAULT_INCLUDE_SUFFIXES,\\\\n    exclude_suffixes: typing.Set[str] = DEFAULT_EXCLUDE_SUFFIXES,\\\\n    exclude_dirs: typing.Set[str] = DEFAULT_EXCLUDE_DIRS,\\\\n    ) -> typing.List[str]:\\\\n    try:\\\\n        files = {\\\\n            str(item.relative_to(directory))  # Store the relative path string\\\\n            for item in Path(directory).rglob(\\\"*\\\") # Iterate through all items recursively\\\\n            if item.is_file() # Consider only files\\\\n            if item.suffix in include_patterns # Check 0: Suffix is included\\\\n            and item.suffix not in exclude_suffixes # Check 1: Suffix is not excluded\\\\n            and not any(part in exclude_dirs for part in item.relative_to(directory).parts) # Check 2: No path part is excluded\\\\n        }\\\\n\\\\n    except ValueError as e:\\\\n        logging.error(f\\\"Error calculating relative paths: {e}\\\")\\\\n        logging.error(f\\\"Ensure the search directory \\\\'{directory.resolve()}\\\\' is within or is the \\\"\\\\n            f\\\"current working directory \\\\'{directory.resolve()}\\\\'\\\")\\\\n\\\\n    logging.info(f\\\"Found {len(files)} files in {directory}\\\")\\\\n    logging.debug(f\\\"Files found: {files}\\\")\\\\n\\\\n    return files\\\\n\\\\n\\\\ndef main() -> None:\\\\n    print(\\\"Crawling files...\\\")\\\\n    directory = os.getcwd()  \\\\n    print(f\\\"Files crawled from {directory}:\\\")\\\\n    print (crawl_files(directory))\\\\n\\\\nif __name__ == \\\"__main__\\\":\\\\n    main()', 'nodes.py': 'from pocketflow import Node\\\\nfrom call_llm import call_llm\\\\n\\\\n\\\\nclass Summarise(Node):\\\\n    \\\"\\\"\\\"\\\\n    This class identifies abstractions in the code.\\\\n    \\\"\\\"\\\"\\\\n\\\\n    def prep(self,shared):\\\\n        return shared_settings[\\\"data\\\"][\\\"settings.py\\\"]\\\\n    \\\\n    def exec(self, text):\\\\n        prompt = f\\\"Summarize this text in 50 words: \\\\\\\\n\\\\\\\\n {text}\\\"\\\\n        return call_llm(prompt)\\\\n    \\\\n    def post(self, shared, prep_res, exec_res):\\\\n        # Store the result in the shared settings\\\\n        shared_settings[\\\"summaries\\\"] = exec_res\\\\n        return \\\"default\\\"\\\\n    \\\\nfile_contents = {\\\\n    \\\"data\\\": {},\\\\n    \\\"summaries\\\": None\\\\n}\\\\n\\\\n# with open(\\\"settings.py\\\", \\\"r\\\") as f:\\\\n#     shared_settings[\\\"data\\\"][\\\"settings.py\\\"] = f.read()\\\\n\\\\n# summarise_node = Summarise()\\\\n# summarise_node.run(shared_settings)\\\\n\\\\n# print(f\\\"Summary: {shared_settings[\\\\'summaries\\\\']}\\\")', '__init__.py': '', 'call_llm.py': 'from google import genai\\\\nimport os\\\\nimport logging\\\\nimport json\\\\nfrom datetime import datetime\\\\nimport requests\\\\n\\\\n# Configure logging\\\\nlog_directory = os.getenv(\\\"LOG_DIR\\\", \\\"logs\\\")\\\\nos.makedirs(log_directory, exist_ok=True)\\\\nlog_file = os.path.join(\\\\n    log_directory, f\\\"llm_calls_{datetime.now().strftime(\\\\'%Y%m%d\\\\')}.log\\\"\\\\n)\\\\n\\\\n# Set up logger\\\\nlogger = logging.getLogger(\\\"llm_logger\\\")\\\\nlogger.setLevel(logging.INFO)\\\\nlogger.propagate = False  # Prevent propagation to root logger\\\\nfile_handler = logging.FileHandler(log_file)\\\\nfile_handler.setFormatter(\\\\n    logging.Formatter(\\\"%(asctime)s - %(levelname)s - %(message)s\\\")\\\\n)\\\\nlogger.addHandler(file_handler)\\\\n\\\\n# Simple cache configuration\\\\ncache_file = \\\"llm_cache.json\\\"\\\\n\\\\n\\\\n# By default, we Google Gemini 2.5 pro, as it shows great performance for code understanding\\\\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\\\\n    # Log the prompt\\\\n    logger.info(f\\\"PROMPT: {prompt}\\\")\\\\n\\\\n    # Check cache if enabled\\\\n    if use_cache:\\\\n        # Load cache from disk\\\\n        cache = {}\\\\n        if os.path.exists(cache_file):\\\\n            try:\\\\n                with open(cache_file, \\\"r\\\") as f:\\\\n                    cache = json.load(f)\\\\n            except:\\\\n                logger.warning(f\\\"Failed to load cache, starting with empty cache\\\")\\\\n\\\\n        # Return from cache if exists\\\\n        if prompt in cache:\\\\n            logger.info(f\\\"RESPONSE: {cache[prompt]}\\\")\\\\n            return cache[prompt]\\\\n\\\\n    # Call the LLM if not in cache or cache disabled\\\\n    client = genai.Client(\\\\n        vertexai=True,\\\\n        # TODO: change to your own project id and location\\\\n        project=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"ai-sre-dev-84b7\\\"),\\\\n        location=os.getenv(\\\"GEMINI_LOCATION\\\", \\\"us-central1\\\")\\\\n    )\\\\n\\\\n    model = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.0-flash-exp\\\")\\\\n    \\\\n    response = client.models.generate_content(model=model, contents=[prompt])\\\\n    response_text = response.text\\\\n\\\\n    # Log the response\\\\n    logger.info(f\\\"RESPONSE: {response_text}\\\")\\\\n\\\\n    # Update cache if enabled\\\\n    if use_cache:\\\\n        # Load cache again to avoid overwrites\\\\n        cache = {}\\\\n        if os.path.exists(cache_file):\\\\n            try:\\\\n                with open(cache_file, \\\"r\\\") as f:\\\\n                    cache = json.load(f)\\\\n            except:\\\\n                pass\\\\n\\\\n        # Add to cache and save\\\\n        cache[prompt] = response_text\\\\n        try:\\\\n            with open(cache_file, \\\"w\\\") as f:\\\\n                json.dump(cache, f)\\\\n        except Exception as e:\\\\n            logger.error(f\\\"Failed to save cache: {e}\\\")\\\\n\\\\n    return response_text\\\\n\\\\n\\\\nif __name__ == \\\"__main__\\\":\\\\n    test_prompt = \\\"Hello, how are you?\\\"\\\\n\\\\n    # First call - should hit the API\\\\n    print(\\\"Making call...\\\")\\\\n    response1 = call_llm(test_prompt, use_cache=False)\\\\n    print(f\\\"Response: {response1}\\\")\\\\n', 'main.py': 'import argparse\\\\nimport os\\\\nimport sys\\\\nimport typing\\\\nimport logging\\\\nfrom pocketflow import Node\\\\nfrom call_llm import call_llm\\\\nfrom settings import (\\\\n    DEFAULT_EXCLUDE_SUFFIXES,\\\\n    DEFAULT_INCLUDE_SUFFIXES,\\\\n    DEFAULT_EXCLUDE_DIRS,\\\\n)\\\\nfrom settings import parse_args\\\\nimport fetch_files\\\\n\\\\n\\\\nclass Summarise(Node):\\\\n    \\\"\\\"\\\"\\\\n    This class identifies abstractions in the code.\\\\n    \\\"\\\"\\\"\\\\n\\\\n    def prep(self,shared):\\\\n        return file_contents[\\\"data\\\"]\\\\n    \\\\n    def exec(self, text):\\\\n        prompt = f\\\"Summarize this text in 50 words: \\\\\\\\n\\\\\\\\n {text}\\\"\\\\n        return call_llm(prompt)\\\\n    \\\\n    def post(self, shared, prep_res, exec_res):\\\\n        # Store the result in the shared settings\\\\n        file_contents[\\\"summaries\\\"] = exec_res\\\\n        return \\\"default\\\"\\\\n    \\\\nfile_contents = {\\\\n\\\"data\\\": {},\\\\n\\\"summaries\\\": None\\\\n}\\\\n\\\\n\\\\ndef main() -> None:\\\\n    args = parse_args()\\\\n\\\\n    # create a dictionarty of settings to be used in the script\\\\n    shared_settings = {\\\\n        \\\"name\\\": args.name,\\\\n        \\\"directory\\\": args.directory,\\\\n        \\\"exclude_patterns\\\": DEFAULT_EXCLUDE_SUFFIXES,\\\\n        \\\"exclude_dirs\\\": DEFAULT_EXCLUDE_DIRS,\\\\n        \\\"include_suffixes\\\": DEFAULT_INCLUDE_SUFFIXES,\\\\n        \\\"additional_exclude_dirs\\\": args.exclude_dirs,\\\\n    }\\\\n    logging.debug(f\\\"Shared settings: {shared_settings}\\\")\\\\n\\\\n    # Get a list of files from the directory and its subdirectories and append to the shared settings\\\\n    shared_settings[\\\"files\\\"] = fetch_files.crawl_files(\\\\n        directory=shared_settings[\\\"directory\\\"],\\\\n        exclude_dirs=shared_settings[\\\"exclude_dirs\\\"].union(\\\\n            shared_settings[\\\"additional_exclude_dirs\\\"]\\\\n        ),\\\\n    )\\\\n\\\\n\\\\n\\\\n    for file in shared_settings[\\\"files\\\"]:\\\\n        with open(file, \\\"r\\\") as f:\\\\n            file_contents[\\\"data\\\"][file] = f.read()\\\\n    print(f\\\"File contents: {file_contents}\\\")\\\\n    \\\\n    summarise_node = Summarise()\\\\n    summarise_node.run(file_contents[\\\"data\\\"])\\\\n\\\\n    print(f\\\"Summary: {file_contents[\\\\'summaries\\\\']}\\\")\\\\n\\\\n    # print (shared_settings)\\\\n\\\\n\\\\nif __name__ == \\\"__main__\\\":\\\\n    main()\\\\n'}\": \"The code generates project documentation using AI. It defines default file types to include/exclude and directories to skip during scanning. Command-line arguments customize the repository name, scan directory, and excluded directories. It crawls files and utilizes a Gemini model via API calls to summarize the content, storing results in a cache.\", \"Summarize this text in 50 words: \\n\\n {'settings.py': 'import typing\\\\nimport argparse\\\\nimport os\\\\n\\\\n\\\\nDEFAULT_INCLUDE_SUFFIXES = {\\\\n    \\\".js\\\",\\\\n    \\\".json\\\",\\\\n    \\\".jsx\\\",\\\\n    \\\".md\\\",\\\\n    \\\".py\\\",\\\\n    \\\".pyi\\\",\\\\n    \\\".pyx\\\",\\\\n    \\\".rst\\\",\\\\n    \\\".tf\\\",\\\\n    \\\".tfvars\\\",\\\\n    \\\".ts\\\",\\\\n    \\\".tsx\\\",\\\\n    \\\".yaml\\\",\\\\n    \\\".yml\\\",\\\\n    \\\"Dockerfile\\\",\\\\n    \\\"Jenkinsfil\\\",\\\\n}\\\\n\\\\nDEFAULT_EXCLUDE_SUFFIXES = {}\\\\n\\\\nDEFAULT_EXCLUDE_DIRS = {\\\\n    \\\".git\\\",\\\\n    \\\".github\\\",\\\\n    \\\".terraform\\\",\\\\n    \\\"assets\\\",\\\\n    \\\"build\\\",\\\\n    \\\"dist\\\",\\\\n    \\\"env\\\",\\\\n    \\\"docs\\\",\\\\n    \\\"env\\\",\\\\n    \\\"images\\\",\\\\n    \\\"temp\\\",\\\\n    \\\"tests\\\",\\\\n    \\\"tmp\\\",\\\\n    \\\"venv\\\",\\\\n}\\\\n\\\\n\\\\ndef parse_args() -> typing.Any:\\\\n    \\\"\\\"\\\"\\\\n    Parse command line arguments.\\\\n    :return: Parsed arguments\\\\n    \\\"\\\"\\\"\\\\n    parser = argparse.ArgumentParser(\\\\n        description=\\\"Generate documentation for a project using AI.\\\",\\\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\\\n    )\\\\n\\\\n    parser.add_argument(\\\\n        \\\"-n\\\",\\\\n        \\\"--name\\\",\\\\n        help=\\\"local Repository name\\\",\\\\n        default=\\\"repo_name\\\",\\\\n        )\\\\n\\\\n    parser.add_argument(\\\\n        \\\"-d\\\",\\\\n        \\\"--directory\\\",\\\\n        help=\\\"top level directory to scan, default is current working directory\\\",\\\\n        default=os.getcwd(),\\\\n        )\\\\n\\\\n    parser.add_argument(\\\\n        \\\"-e\\\",\\\\n        \\\"--exclude_dirs\\\",\\\\n        nargs=\\\\'*\\\\',\\\\n        help=\\\"comma separated list of directories to exclude\\\",\\\\n        default=\\\",\\\"\\\\n        )\\\\n\\\\n    return parser.parse_args()\\\\n\\\\nmodel = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.5-pro-exp-03-25\\\")\\\\nproject=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"sre-ai-dev\\\")', '__init__.py': '', 'main.py': 'import logging\\\\nfrom pocketflow import Node\\\\nfrom call_llm import call_llm\\\\nfrom settings import (\\\\n    DEFAULT_EXCLUDE_SUFFIXES,\\\\n    DEFAULT_INCLUDE_SUFFIXES,\\\\n    DEFAULT_EXCLUDE_DIRS,\\\\n)\\\\nfrom settings import parse_args\\\\nimport fetch_files\\\\n\\\\n\\\\nclass Summarise(Node):\\\\n    \\\"\\\"\\\"\\\\n    This class identifies abstractions in the code.\\\\n    \\\"\\\"\\\"\\\\n\\\\n    def prep(self,shared):\\\\n        return file_contents[\\\"data\\\"]\\\\n    \\\\n    def exec(self, text):\\\\n        prompt = f\\\"Summarize this text in 50 words: \\\\\\\\n\\\\\\\\n {text}\\\"\\\\n        return call_llm(prompt)\\\\n    \\\\n    def post(self, shared, prep_res, exec_res):\\\\n        # Store the result in the shared settings\\\\n        file_contents[\\\"summaries\\\"] = exec_res\\\\n        return \\\"default\\\"\\\\n\\\\n\\\\nfile_contents = {\\\\n\\\"data\\\": {},\\\\n\\\"summaries\\\": None\\\\n}\\\\n\\\\n\\\\ndef main() -> None:\\\\n    args = parse_args()\\\\n\\\\n    # create a dictionarty of settings to be used in the script\\\\n    shared_settings = {\\\\n        \\\"name\\\": args.name,\\\\n        \\\"directory\\\": args.directory,\\\\n        \\\"exclude_patterns\\\": DEFAULT_EXCLUDE_SUFFIXES,\\\\n        \\\"exclude_dirs\\\": DEFAULT_EXCLUDE_DIRS,\\\\n        \\\"include_suffixes\\\": DEFAULT_INCLUDE_SUFFIXES,\\\\n        \\\"additional_exclude_dirs\\\": args.exclude_dirs,\\\\n    }\\\\n    logging.debug(f\\\"Shared settings: {shared_settings}\\\")\\\\n\\\\n    # Get a list of files from the directory and its subdirectories and append to the shared settings\\\\n    shared_settings[\\\"files\\\"] = fetch_files.crawl_files(\\\\n        directory=shared_settings[\\\"directory\\\"],\\\\n        exclude_dirs=shared_settings[\\\"exclude_dirs\\\"].union(\\\\n            shared_settings[\\\"additional_exclude_dirs\\\"]\\\\n        ),\\\\n    )\\\\n\\\\n    for file in shared_settings[\\\"files\\\"]:\\\\n        with open(file, \\\"r\\\") as f:\\\\n            file_contents[\\\"data\\\"][file] = f.read()\\\\n    \\\\n    summarise_node = Summarise()\\\\n    summarise_node.run(file_contents[\\\"data\\\"])\\\\n\\\\n    print(f\\\"Summary: {file_contents[\\\\'summaries\\\\']}\\\")\\\\n\\\\n    # print (shared_settings)\\\\n\\\\n\\\\nif __name__ == \\\"__main__\\\":\\\\n    main()\\\\n', 'fetch_files.py': '# A script to crawl files from a given directory and its subdirectories.\\\\n# It uses the `os` module to walk through the directory structure and collects file paths based on specified include and exclude patterns.\\\\n# files that are not in the exclude patterns and are in the include patterns are collected.\\\\nimport os\\\\nimport typing\\\\nimport logging\\\\nfrom pathlib import Path\\\\nfrom settings import DEFAULT_EXCLUDE_SUFFIXES, DEFAULT_INCLUDE_SUFFIXES, DEFAULT_EXCLUDE_DIRS\\\\n\\\\n\\\\n\\\\ndef crawl_files(\\\\n    directory: str,\\\\n    include_patterns: typing.Set[str] = DEFAULT_INCLUDE_SUFFIXES,\\\\n    exclude_suffixes: typing.Set[str] = DEFAULT_EXCLUDE_SUFFIXES,\\\\n    exclude_dirs: typing.Set[str] = DEFAULT_EXCLUDE_DIRS,\\\\n    ) -> typing.List[str]:\\\\n    try:\\\\n        files = {\\\\n            str(item.relative_to(directory))  # Store the relative path string\\\\n            for item in Path(directory).rglob(\\\"*\\\") # Iterate through all items recursively\\\\n            if item.is_file() # Consider only files\\\\n            if item.suffix in include_patterns # Check 0: Suffix is included\\\\n            and item.suffix not in exclude_suffixes # Check 1: Suffix is not excluded\\\\n            and not any(part in exclude_dirs for part in item.relative_to(directory).parts) # Check 2: No path part is excluded\\\\n        }\\\\n\\\\n    except ValueError as e:\\\\n        logging.error(f\\\"Error calculating relative paths: {e}\\\")\\\\n        logging.error(f\\\"Ensure the search directory \\\\'{directory.resolve()}\\\\' is within or is the \\\"\\\\n            f\\\"current working directory \\\\'{directory.resolve()}\\\\'\\\")\\\\n\\\\n    logging.info(f\\\"Found {len(files)} files in {directory}\\\")\\\\n    logging.debug(f\\\"Files found: {files}\\\")\\\\n\\\\n    return files\\\\n\\\\n\\\\ndef main() -> None:\\\\n    print(\\\"Crawling files...\\\")\\\\n    directory = os.getcwd()  \\\\n    print(f\\\"Files crawled from {directory}:\\\")\\\\n    print (crawl_files(directory))\\\\n\\\\nif __name__ == \\\"__main__\\\":\\\\n    main()', 'call_llm.py': 'from google import genai\\\\nimport os\\\\nimport logging\\\\nimport json\\\\nfrom datetime import datetime\\\\n\\\\n\\\\n# Configure logging\\\\nlog_directory = os.getenv(\\\"LOG_DIR\\\", \\\"logs\\\")\\\\nos.makedirs(log_directory, exist_ok=True)\\\\nlog_file = os.path.join(\\\\n    log_directory, f\\\"llm_calls_{datetime.now().strftime(\\\\'%Y%m%d\\\\')}.log\\\"\\\\n)\\\\n\\\\n# Set up logger\\\\nlogger = logging.getLogger(\\\"llm_logger\\\")\\\\nlogger.setLevel(logging.INFO)\\\\nlogger.propagate = False  # Prevent propagation to root logger\\\\nfile_handler = logging.FileHandler(log_file)\\\\nfile_handler.setFormatter(\\\\n    logging.Formatter(\\\"%(asctime)s - %(levelname)s - %(message)s\\\")\\\\n)\\\\nlogger.addHandler(file_handler)\\\\n\\\\n# Simple cache configuration\\\\ncache_file = \\\"llm_cache.json\\\"\\\\n\\\\n\\\\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\\\\n    # Log the prompt\\\\n    logger.info(f\\\"PROMPT: {prompt}\\\")\\\\n\\\\n    # Check cache if enabled\\\\n    if use_cache:\\\\n        # Load cache from disk\\\\n        cache = {}\\\\n        if os.path.exists(cache_file):\\\\n            try:\\\\n                with open(cache_file, \\\"r\\\") as f:\\\\n                    cache = json.load(f)\\\\n            except:\\\\n                logger.warning(f\\\"Failed to load cache, starting with empty cache\\\")\\\\n\\\\n        # Return from cache if exists\\\\n        if prompt in cache:\\\\n            logger.info(f\\\"RESPONSE: {cache[prompt]}\\\")\\\\n            return cache[prompt]\\\\n\\\\n    # Call the LLM if not in cache or cache disabled\\\\n    client = genai.Client(\\\\n        vertexai=True,\\\\n        project=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"ai-sre-dev-84b7\\\"),\\\\n        location=os.getenv(\\\"GEMINI_LOCATION\\\", \\\"us-central1\\\")\\\\n    )\\\\n\\\\n    model = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.0-flash-exp\\\")\\\\n    \\\\n    response = client.models.generate_content(model=model, contents=[prompt])\\\\n    response_text = response.text\\\\n\\\\n    # Log the response\\\\n    logger.info(f\\\"RESPONSE: {response_text}\\\")\\\\n\\\\n    # Update cache if enabled\\\\n    if use_cache:\\\\n        # Load cache again to avoid overwrites\\\\n        cache = {}\\\\n        if os.path.exists(cache_file):\\\\n            try:\\\\n                with open(cache_file, \\\"r\\\") as f:\\\\n                    cache = json.load(f)\\\\n            except:\\\\n                pass\\\\n\\\\n        # Add to cache and save\\\\n        cache[prompt] = response_text\\\\n        try:\\\\n            with open(cache_file, \\\"w\\\") as f:\\\\n                json.dump(cache, f)\\\\n        except Exception as e:\\\\n            logger.error(f\\\"Failed to save cache: {e}\\\")\\\\n\\\\n    return response_text\\\\n\\\\n\\\\nif __name__ == \\\"__main__\\\":\\\\n    test_prompt = \\\"Hello, how are you?\\\"\\\\n\\\\n    # First call - should hit the API\\\\n    print(\\\"Making call...\\\")\\\\n    response1 = call_llm(test_prompt, use_cache=False)\\\\n    print(f\\\"Response: {response1}\\\")\\\\n', 'llm_cache.json': '{\\\"Summarize this text in 50 words: \\\\\\\\n\\\\\\\\n import typing\\\\\\\\nimport argparse\\\\\\\\nimport os\\\\\\\\n\\\\\\\\n\\\\\\\\nDEFAULT_INCLUDE_SUFFIXES = {\\\\\\\\n    \\\\\\\\\\\".js\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".json\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".jsx\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".md\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".py\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".pyi\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".pyx\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".rst\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".tf\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".tfvars\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".ts\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".tsx\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".yaml\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".yml\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"Dockerfile\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"Jenkinsfil\\\\\\\\\\\",\\\\\\\\n}\\\\\\\\n\\\\\\\\nDEFAULT_EXCLUDE_SUFFIXES = {}\\\\\\\\n\\\\\\\\nDEFAULT_EXCLUDE_DIRS = {\\\\\\\\n    \\\\\\\\\\\".git\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".github\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".terraform\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"assets\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"build\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"dist\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"env\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"docs\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"env\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"images\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"temp\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"tests\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"tmp\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"venv\\\\\\\\\\\",\\\\\\\\n}\\\\\\\\n\\\\\\\\n\\\\\\\\ndef parse_args() -> typing.Any:\\\\\\\\n    \\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\n    Parse command line arguments.\\\\\\\\n    :return: Parsed arguments\\\\\\\\n    \\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\n    parser = argparse.ArgumentParser(\\\\\\\\n        description=\\\\\\\\\\\"Generate documentation for a project using AI.\\\\\\\\\\\",\\\\\\\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\\\\\\\n    )\\\\\\\\n\\\\\\\\n    parser.add_argument(\\\\\\\\n        \\\\\\\\\\\"-n\\\\\\\\\\\",\\\\\\\\n        \\\\\\\\\\\"--name\\\\\\\\\\\",\\\\\\\\n        help=\\\\\\\\\\\"local Repository name\\\\\\\\\\\",\\\\\\\\n        default=\\\\\\\\\\\"repo_name\\\\\\\\\\\",\\\\\\\\n        )\\\\\\\\n\\\\\\\\n    parser.add_argument(\\\\\\\\n        \\\\\\\\\\\"-d\\\\\\\\\\\",\\\\\\\\n        \\\\\\\\\\\"--directory\\\\\\\\\\\",\\\\\\\\n        help=\\\\\\\\\\\"top level directory to scan, default is current working directory\\\\\\\\\\\",\\\\\\\\n        default=os.getcwd(),\\\\\\\\n        )\\\\\\\\n\\\\\\\\n    parser.add_argument(\\\\\\\\n        \\\\\\\\\\\"-e\\\\\\\\\\\",\\\\\\\\n        \\\\\\\\\\\"--exclude_dirs\\\\\\\\\\\",\\\\\\\\n        nargs=\\\\'*\\\\',\\\\\\\\n        help=\\\\\\\\\\\"comma separated list of directories to exclude\\\\\\\\\\\",\\\\\\\\n        default=\\\\\\\\\\\",\\\\\\\\\\\"\\\\\\\\n        )\\\\\\\\n\\\\\\\\n    return parser.parse_args()\\\\\\\\n\\\\\\\\nmodel = os.getenv(\\\\\\\\\\\"GEMINI_MODEL\\\\\\\\\\\", \\\\\\\\\\\"gemini-2.5-pro-exp-03-25\\\\\\\\\\\")\\\\\\\\nproject=os.getenv(\\\\\\\\\\\"GEMINI_PROJECT_ID\\\\\\\\\\\", \\\\\\\\\\\"sre-ai-dev\\\\\\\\\\\")\\\": \\\"This code defines default file suffixes to include/exclude for documentation generation. It also specifies directories to exclude from the scan, like \\\\\\\\\\\".git\\\\\\\\\\\" and \\\\\\\\\\\"venv.\\\\\\\\\\\"  The `parse_args` function handles command-line arguments for repository name, directory, and excluded directories.  Finally, it fetches environment variables for the Gemini model and project ID.\\\\\\\\n\\\", \\\"Summarize this text in 50 words: \\\\\\\\n\\\\\\\\n {\\\\'settings.py\\\\': \\\\'import typing\\\\\\\\\\\\\\\\nimport argparse\\\\\\\\\\\\\\\\nimport os\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nDEFAULT_INCLUDE_SUFFIXES = {\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".js\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".json\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".jsx\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".md\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".py\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".pyi\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".pyx\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".rst\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".tf\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".tfvars\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".ts\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".tsx\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".yaml\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".yml\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"Dockerfile\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"Jenkinsfil\\\\\\\\\\\",\\\\\\\\\\\\\\\\n}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nDEFAULT_EXCLUDE_SUFFIXES = {}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nDEFAULT_EXCLUDE_DIRS = {\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".git\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".github\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".terraform\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"assets\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"build\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"dist\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"env\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"docs\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"env\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"images\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"temp\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"tests\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"tmp\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"venv\\\\\\\\\\\",\\\\\\\\\\\\\\\\n}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\ndef parse_args() -> typing.Any:\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\\\\\\n    Parse command line arguments.\\\\\\\\\\\\\\\\n    :return: Parsed arguments\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\\\\\\n    parser = argparse.ArgumentParser(\\\\\\\\\\\\\\\\n        description=\\\\\\\\\\\"Generate documentation for a project using AI.\\\\\\\\\\\",\\\\\\\\\\\\\\\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\\\\\\\\\\\\\\\n    )\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    parser.add_argument(\\\\\\\\\\\\\\\\n        \\\\\\\\\\\"-n\\\\\\\\\\\",\\\\\\\\\\\\\\\\n        \\\\\\\\\\\"--name\\\\\\\\\\\",\\\\\\\\\\\\\\\\n        help=\\\\\\\\\\\"local Repository name\\\\\\\\\\\",\\\\\\\\\\\\\\\\n        default=\\\\\\\\\\\"repo_name\\\\\\\\\\\",\\\\\\\\\\\\\\\\n        )\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    parser.add_argument(\\\\\\\\\\\\\\\\n        \\\\\\\\\\\"-d\\\\\\\\\\\",\\\\\\\\\\\\\\\\n        \\\\\\\\\\\"--directory\\\\\\\\\\\",\\\\\\\\\\\\\\\\n        help=\\\\\\\\\\\"top level directory to scan, default is current working directory\\\\\\\\\\\",\\\\\\\\\\\\\\\\n        default=os.getcwd(),\\\\\\\\\\\\\\\\n        )\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    parser.add_argument(\\\\\\\\\\\\\\\\n        \\\\\\\\\\\"-e\\\\\\\\\\\",\\\\\\\\\\\\\\\\n        \\\\\\\\\\\"--exclude_dirs\\\\\\\\\\\",\\\\\\\\\\\\\\\\n        nargs=\\\\\\\\\\\\\\\\\\\\'*\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n        help=\\\\\\\\\\\"comma separated list of directories to exclude\\\\\\\\\\\",\\\\\\\\\\\\\\\\n        default=\\\\\\\\\\\",\\\\\\\\\\\"\\\\\\\\\\\\\\\\n        )\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    return parser.parse_args()\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nmodel = os.getenv(\\\\\\\\\\\"GEMINI_MODEL\\\\\\\\\\\", \\\\\\\\\\\"gemini-2.5-pro-exp-03-25\\\\\\\\\\\")\\\\\\\\\\\\\\\\nproject=os.getenv(\\\\\\\\\\\"GEMINI_PROJECT_ID\\\\\\\\\\\", \\\\\\\\\\\"sre-ai-dev\\\\\\\\\\\")\\\\', \\\\'llm_cache.json\\\\': \\\\'{\\\\\\\\\\\"Summarize this text in 50 words: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import typing\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nimport argparse\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nimport os\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nDEFAULT_INCLUDE_SUFFIXES = {\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".js\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".json\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".jsx\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".md\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".pyi\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".pyx\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".rst\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".tf\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".tfvars\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".ts\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".tsx\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".yaml\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".yml\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"Dockerfile\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"Jenkinsfil\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nDEFAULT_EXCLUDE_SUFFIXES = {}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nDEFAULT_EXCLUDE_DIRS = {\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".git\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".github\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".terraform\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"assets\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"build\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"dist\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"env\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"docs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"env\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"images\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"temp\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"tests\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"tmp\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"venv\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndef parse_args() -> typing.Any:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    Parse command line arguments.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    :return: Parsed arguments\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    parser = argparse.ArgumentParser(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        description=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"Generate documentation for a project using AI.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    )\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"-n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"--name\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"local Repository name\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        default=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"repo_name\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        )\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"-d\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"--directory\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"top level directory to scan, default is current working directory\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        default=os.getcwd(),\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        )\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"-e\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"--exclude_dirs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        nargs=\\\\\\\\\\\\\\\\\\\\'*\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"comma separated list of directories to exclude\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        default=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        )\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    return parser.parse_args()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nmodel = os.getenv(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"GEMINI_MODEL\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"gemini-2.5-pro-exp-03-25\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nproject=os.getenv(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"GEMINI_PROJECT_ID\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"sre-ai-dev\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\\\\\\\\\\\": \\\\\\\\\\\"This code defines default file suffixes to include/exclude for documentation generation. It also specifies directories to exclude from the scan, like \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".git\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\" and \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"venv.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"  The `parse_args` function handles command-line arguments for repository name, directory, and excluded directories.  Finally, it fetches environment variables for the Gemini model and project ID.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\"}\\\\', \\\\'fetch_files.py\\\\': \\\\'# A script to crawl files from a given directory and its subdirectories.\\\\\\\\\\\\\\\\n# It uses the `os` module to walk through the directory structure and collects file paths based on specified include and exclude patterns.\\\\\\\\\\\\\\\\n# files that are not in the exclude patterns and are in the include patterns are collected.\\\\\\\\\\\\\\\\nimport os\\\\\\\\\\\\\\\\nimport typing\\\\\\\\\\\\\\\\nimport logging\\\\\\\\\\\\\\\\nfrom pathlib import Path\\\\\\\\\\\\\\\\nfrom settings import DEFAULT_EXCLUDE_SUFFIXES, DEFAULT_INCLUDE_SUFFIXES, DEFAULT_EXCLUDE_DIRS\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\ndef crawl_files(\\\\\\\\\\\\\\\\n    directory: str,\\\\\\\\\\\\\\\\n    include_patterns: typing.Set[str] = DEFAULT_INCLUDE_SUFFIXES,\\\\\\\\\\\\\\\\n    exclude_suffixes: typing.Set[str] = DEFAULT_EXCLUDE_SUFFIXES,\\\\\\\\\\\\\\\\n    exclude_dirs: typing.Set[str] = DEFAULT_EXCLUDE_DIRS,\\\\\\\\\\\\\\\\n    ) -> typing.List[str]:\\\\\\\\\\\\\\\\n    try:\\\\\\\\\\\\\\\\n        files = {\\\\\\\\\\\\\\\\n            str(item.relative_to(directory))  # Store the relative path string\\\\\\\\\\\\\\\\n            for item in Path(directory).rglob(\\\\\\\\\\\"*\\\\\\\\\\\") # Iterate through all items recursively\\\\\\\\\\\\\\\\n            if item.is_file() # Consider only files\\\\\\\\\\\\\\\\n            if item.suffix in include_patterns # Check 0: Suffix is included\\\\\\\\\\\\\\\\n            and item.suffix not in exclude_suffixes # Check 1: Suffix is not excluded\\\\\\\\\\\\\\\\n            and not any(part in exclude_dirs for part in item.relative_to(directory).parts) # Check 2: No path part is excluded\\\\\\\\\\\\\\\\n        }\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    except ValueError as e:\\\\\\\\\\\\\\\\n        logging.error(f\\\\\\\\\\\"Error calculating relative paths: {e}\\\\\\\\\\\")\\\\\\\\\\\\\\\\n        logging.error(f\\\\\\\\\\\"Ensure the search directory \\\\\\\\\\\\\\\\\\\\'{directory.resolve()}\\\\\\\\\\\\\\\\\\\\' is within or is the \\\\\\\\\\\"\\\\\\\\\\\\\\\\n            f\\\\\\\\\\\"current working directory \\\\\\\\\\\\\\\\\\\\'{directory.resolve()}\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\")\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    logging.info(f\\\\\\\\\\\"Found {len(files)} files in {directory}\\\\\\\\\\\")\\\\\\\\\\\\\\\\n    logging.debug(f\\\\\\\\\\\"Files found: {files}\\\\\\\\\\\")\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    return files\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\ndef main() -> None:\\\\\\\\\\\\\\\\n    print(\\\\\\\\\\\"Crawling files...\\\\\\\\\\\")\\\\\\\\\\\\\\\\n    directory = os.getcwd()  \\\\\\\\\\\\\\\\n    print(f\\\\\\\\\\\"Files crawled from {directory}:\\\\\\\\\\\")\\\\\\\\\\\\\\\\n    print (crawl_files(directory))\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nif __name__ == \\\\\\\\\\\"__main__\\\\\\\\\\\":\\\\\\\\\\\\\\\\n    main()\\\\', \\\\'nodes.py\\\\': \\\\'from pocketflow import Node\\\\\\\\\\\\\\\\nfrom call_llm import call_llm\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nclass Summarise(Node):\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\\\\\\n    This class identifies abstractions in the code.\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    def prep(self,shared):\\\\\\\\\\\\\\\\n        return shared_settings[\\\\\\\\\\\"data\\\\\\\\\\\"][\\\\\\\\\\\"settings.py\\\\\\\\\\\"]\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\n    def exec(self, text):\\\\\\\\\\\\\\\\n        prompt = f\\\\\\\\\\\"Summarize this text in 50 words: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n {text}\\\\\\\\\\\"\\\\\\\\\\\\\\\\n        return call_llm(prompt)\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\n    def post(self, shared, prep_res, exec_res):\\\\\\\\\\\\\\\\n        # Store the result in the shared settings\\\\\\\\\\\\\\\\n        shared_settings[\\\\\\\\\\\"summaries\\\\\\\\\\\"] = exec_res\\\\\\\\\\\\\\\\n        return \\\\\\\\\\\"default\\\\\\\\\\\"\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\nfile_contents = {\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"data\\\\\\\\\\\": {},\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"summaries\\\\\\\\\\\": None\\\\\\\\\\\\\\\\n}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n# with open(\\\\\\\\\\\"settings.py\\\\\\\\\\\", \\\\\\\\\\\"r\\\\\\\\\\\") as f:\\\\\\\\\\\\\\\\n#     shared_settings[\\\\\\\\\\\"data\\\\\\\\\\\"][\\\\\\\\\\\"settings.py\\\\\\\\\\\"] = f.read()\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n# summarise_node = Summarise()\\\\\\\\\\\\\\\\n# summarise_node.run(shared_settings)\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n# print(f\\\\\\\\\\\"Summary: {shared_settings[\\\\\\\\\\\\\\\\\\\\'summaries\\\\\\\\\\\\\\\\\\\\']}\\\\\\\\\\\")\\\\', \\\\'__init__.py\\\\': \\\\'\\\\', \\\\'call_llm.py\\\\': \\\\'from google import genai\\\\\\\\\\\\\\\\nimport os\\\\\\\\\\\\\\\\nimport logging\\\\\\\\\\\\\\\\nimport json\\\\\\\\\\\\\\\\nfrom datetime import datetime\\\\\\\\\\\\\\\\nimport requests\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n# Configure logging\\\\\\\\\\\\\\\\nlog_directory = os.getenv(\\\\\\\\\\\"LOG_DIR\\\\\\\\\\\", \\\\\\\\\\\"logs\\\\\\\\\\\")\\\\\\\\\\\\\\\\nos.makedirs(log_directory, exist_ok=True)\\\\\\\\\\\\\\\\nlog_file = os.path.join(\\\\\\\\\\\\\\\\n    log_directory, f\\\\\\\\\\\"llm_calls_{datetime.now().strftime(\\\\\\\\\\\\\\\\\\\\'%Y%m%d\\\\\\\\\\\\\\\\\\\\')}.log\\\\\\\\\\\"\\\\\\\\\\\\\\\\n)\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n# Set up logger\\\\\\\\\\\\\\\\nlogger = logging.getLogger(\\\\\\\\\\\"llm_logger\\\\\\\\\\\")\\\\\\\\\\\\\\\\nlogger.setLevel(logging.INFO)\\\\\\\\\\\\\\\\nlogger.propagate = False  # Prevent propagation to root logger\\\\\\\\\\\\\\\\nfile_handler = logging.FileHandler(log_file)\\\\\\\\\\\\\\\\nfile_handler.setFormatter(\\\\\\\\\\\\\\\\n    logging.Formatter(\\\\\\\\\\\"%(asctime)s - %(levelname)s - %(message)s\\\\\\\\\\\")\\\\\\\\\\\\\\\\n)\\\\\\\\\\\\\\\\nlogger.addHandler(file_handler)\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n# Simple cache configuration\\\\\\\\\\\\\\\\ncache_file = \\\\\\\\\\\"llm_cache.json\\\\\\\\\\\"\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n# By default, we Google Gemini 2.5 pro, as it shows great performance for code understanding\\\\\\\\\\\\\\\\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\\\\\\\\\\\\\\\\n    # Log the prompt\\\\\\\\\\\\\\\\n    logger.info(f\\\\\\\\\\\"PROMPT: {prompt}\\\\\\\\\\\")\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    # Check cache if enabled\\\\\\\\\\\\\\\\n    if use_cache:\\\\\\\\\\\\\\\\n        # Load cache from disk\\\\\\\\\\\\\\\\n        cache = {}\\\\\\\\\\\\\\\\n        if os.path.exists(cache_file):\\\\\\\\\\\\\\\\n            try:\\\\\\\\\\\\\\\\n                with open(cache_file, \\\\\\\\\\\"r\\\\\\\\\\\") as f:\\\\\\\\\\\\\\\\n                    cache = json.load(f)\\\\\\\\\\\\\\\\n            except:\\\\\\\\\\\\\\\\n                logger.warning(f\\\\\\\\\\\"Failed to load cache, starting with empty cache\\\\\\\\\\\")\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n        # Return from cache if exists\\\\\\\\\\\\\\\\n        if prompt in cache:\\\\\\\\\\\\\\\\n            logger.info(f\\\\\\\\\\\"RESPONSE: {cache[prompt]}\\\\\\\\\\\")\\\\\\\\\\\\\\\\n            return cache[prompt]\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    # Call the LLM if not in cache or cache disabled\\\\\\\\\\\\\\\\n    client = genai.Client(\\\\\\\\\\\\\\\\n        vertexai=True,\\\\\\\\\\\\\\\\n        # TODO: change to your own project id and location\\\\\\\\\\\\\\\\n        project=os.getenv(\\\\\\\\\\\"GEMINI_PROJECT_ID\\\\\\\\\\\", \\\\\\\\\\\"ai-sre-dev-84b7\\\\\\\\\\\"),\\\\\\\\\\\\\\\\n        location=os.getenv(\\\\\\\\\\\"GEMINI_LOCATION\\\\\\\\\\\", \\\\\\\\\\\"us-central1\\\\\\\\\\\")\\\\\\\\\\\\\\\\n    )\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    model = os.getenv(\\\\\\\\\\\"GEMINI_MODEL\\\\\\\\\\\", \\\\\\\\\\\"gemini-2.0-flash-exp\\\\\\\\\\\")\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\n    response = client.models.generate_content(model=model, contents=[prompt])\\\\\\\\\\\\\\\\n    response_text = response.text\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    # Log the response\\\\\\\\\\\\\\\\n    logger.info(f\\\\\\\\\\\"RESPONSE: {response_text}\\\\\\\\\\\")\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    # Update cache if enabled\\\\\\\\\\\\\\\\n    if use_cache:\\\\\\\\\\\\\\\\n        # Load cache again to avoid overwrites\\\\\\\\\\\\\\\\n        cache = {}\\\\\\\\\\\\\\\\n        if os.path.exists(cache_file):\\\\\\\\\\\\\\\\n            try:\\\\\\\\\\\\\\\\n                with open(cache_file, \\\\\\\\\\\"r\\\\\\\\\\\") as f:\\\\\\\\\\\\\\\\n                    cache = json.load(f)\\\\\\\\\\\\\\\\n            except:\\\\\\\\\\\\\\\\n                pass\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n        # Add to cache and save\\\\\\\\\\\\\\\\n        cache[prompt] = response_text\\\\\\\\\\\\\\\\n        try:\\\\\\\\\\\\\\\\n            with open(cache_file, \\\\\\\\\\\"w\\\\\\\\\\\") as f:\\\\\\\\\\\\\\\\n                json.dump(cache, f)\\\\\\\\\\\\\\\\n        except Exception as e:\\\\\\\\\\\\\\\\n            logger.error(f\\\\\\\\\\\"Failed to save cache: {e}\\\\\\\\\\\")\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    return response_text\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nif __name__ == \\\\\\\\\\\"__main__\\\\\\\\\\\":\\\\\\\\\\\\\\\\n    test_prompt = \\\\\\\\\\\"Hello, how are you?\\\\\\\\\\\"\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    # First call - should hit the API\\\\\\\\\\\\\\\\n    print(\\\\\\\\\\\"Making call...\\\\\\\\\\\")\\\\\\\\\\\\\\\\n    response1 = call_llm(test_prompt, use_cache=False)\\\\\\\\\\\\\\\\n    print(f\\\\\\\\\\\"Response: {response1}\\\\\\\\\\\")\\\\\\\\\\\\\\\\n\\\\', \\\\'main.py\\\\': \\\\'import argparse\\\\\\\\\\\\\\\\nimport os\\\\\\\\\\\\\\\\nimport sys\\\\\\\\\\\\\\\\nimport typing\\\\\\\\\\\\\\\\nimport logging\\\\\\\\\\\\\\\\nfrom pocketflow import Node\\\\\\\\\\\\\\\\nfrom call_llm import call_llm\\\\\\\\\\\\\\\\nfrom settings import (\\\\\\\\\\\\\\\\n    DEFAULT_EXCLUDE_SUFFIXES,\\\\\\\\\\\\\\\\n    DEFAULT_INCLUDE_SUFFIXES,\\\\\\\\\\\\\\\\n    DEFAULT_EXCLUDE_DIRS,\\\\\\\\\\\\\\\\n)\\\\\\\\\\\\\\\\nfrom settings import parse_args\\\\\\\\\\\\\\\\nimport fetch_files\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nclass Summarise(Node):\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\\\\\\n    This class identifies abstractions in the code.\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    def prep(self,shared):\\\\\\\\\\\\\\\\n        return file_contents[\\\\\\\\\\\"data\\\\\\\\\\\"]\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\n    def exec(self, text):\\\\\\\\\\\\\\\\n        prompt = f\\\\\\\\\\\"Summarize this text in 50 words: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n {text}\\\\\\\\\\\"\\\\\\\\\\\\\\\\n        return call_llm(prompt)\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\n    def post(self, shared, prep_res, exec_res):\\\\\\\\\\\\\\\\n        # Store the result in the shared settings\\\\\\\\\\\\\\\\n        file_contents[\\\\\\\\\\\"summaries\\\\\\\\\\\"] = exec_res\\\\\\\\\\\\\\\\n        return \\\\\\\\\\\"default\\\\\\\\\\\"\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\nfile_contents = {\\\\\\\\\\\\\\\\n\\\\\\\\\\\"data\\\\\\\\\\\": {},\\\\\\\\\\\\\\\\n\\\\\\\\\\\"summaries\\\\\\\\\\\": None\\\\\\\\\\\\\\\\n}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\ndef main() -> None:\\\\\\\\\\\\\\\\n    args = parse_args()\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    # create a dictionarty of settings to be used in the script\\\\\\\\\\\\\\\\n    shared_settings = {\\\\\\\\\\\\\\\\n        \\\\\\\\\\\"name\\\\\\\\\\\": args.name,\\\\\\\\\\\\\\\\n        \\\\\\\\\\\"directory\\\\\\\\\\\": args.directory,\\\\\\\\\\\\\\\\n        \\\\\\\\\\\"exclude_patterns\\\\\\\\\\\": DEFAULT_EXCLUDE_SUFFIXES,\\\\\\\\\\\\\\\\n        \\\\\\\\\\\"exclude_dirs\\\\\\\\\\\": DEFAULT_EXCLUDE_DIRS,\\\\\\\\\\\\\\\\n        \\\\\\\\\\\"include_suffixes\\\\\\\\\\\": DEFAULT_INCLUDE_SUFFIXES,\\\\\\\\\\\\\\\\n        \\\\\\\\\\\"additional_exclude_dirs\\\\\\\\\\\": args.exclude_dirs,\\\\\\\\\\\\\\\\n    }\\\\\\\\\\\\\\\\n    logging.debug(f\\\\\\\\\\\"Shared settings: {shared_settings}\\\\\\\\\\\")\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    # Get a list of files from the directory and its subdirectories and append to the shared settings\\\\\\\\\\\\\\\\n    shared_settings[\\\\\\\\\\\"files\\\\\\\\\\\"] = fetch_files.crawl_files(\\\\\\\\\\\\\\\\n        directory=shared_settings[\\\\\\\\\\\"directory\\\\\\\\\\\"],\\\\\\\\\\\\\\\\n        exclude_dirs=shared_settings[\\\\\\\\\\\"exclude_dirs\\\\\\\\\\\"].union(\\\\\\\\\\\\\\\\n            shared_settings[\\\\\\\\\\\"additional_exclude_dirs\\\\\\\\\\\"]\\\\\\\\\\\\\\\\n        ),\\\\\\\\\\\\\\\\n    )\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    for file in shared_settings[\\\\\\\\\\\"files\\\\\\\\\\\"]:\\\\\\\\\\\\\\\\n        with open(file, \\\\\\\\\\\"r\\\\\\\\\\\") as f:\\\\\\\\\\\\\\\\n            file_contents[\\\\\\\\\\\"data\\\\\\\\\\\"][file] = f.read()\\\\\\\\\\\\\\\\n    print(f\\\\\\\\\\\"File contents: {file_contents}\\\\\\\\\\\")\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\n    summarise_node = Summarise()\\\\\\\\\\\\\\\\n    summarise_node.run(file_contents[\\\\\\\\\\\"data\\\\\\\\\\\"])\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    print(f\\\\\\\\\\\"Summary: {file_contents[\\\\\\\\\\\\\\\\\\\\'summaries\\\\\\\\\\\\\\\\\\\\']}\\\\\\\\\\\")\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    # print (shared_settings)\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nif __name__ == \\\\\\\\\\\"__main__\\\\\\\\\\\":\\\\\\\\\\\\\\\\n    main()\\\\\\\\\\\\\\\\n\\\\'}\\\": \\\"The code generates project documentation using AI. It defines default file types to include/exclude and directories to skip during scanning. Command-line arguments customize the repository name, scan directory, and excluded directories. It crawls files and utilizes a Gemini model via API calls to summarize the content, storing results in a cache.\\\"}'}\": \"This project generates documentation using AI. It crawls project files, excluding specified directories and file types. The code uses the Gemini model to summarize file contents, caching the results. Command-line arguments customize the project name, scan directory, and exclusion rules for processing files.\\n\"}": "This Python script generates project documentation using AI. It defines settings for file inclusion/exclusion and directory skipping. It uses command-line arguments, crawls files, and leverages the Gemini model via API calls to summarize files, storing the results in a cache for subsequent use.\n", "Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\n\nCode:\n```python\n{\"Summarize this text in 50 words: \\n\\n import typing\\nimport argparse\\nimport os\\n\\n\\nDEFAULT_INCLUDE_SUFFIXES = {\\n    \\\".js\\\",\\n    \\\".json\\\",\\n    \\\".jsx\\\",\\n    \\\".md\\\",\\n    \\\".py\\\",\\n    \\\".pyi\\\",\\n    \\\".pyx\\\",\\n    \\\".rst\\\",\\n    \\\".tf\\\",\\n    \\\".tfvars\\\",\\n    \\\".ts\\\",\\n    \\\".tsx\\\",\\n    \\\".yaml\\\",\\n    \\\".yml\\\",\\n    \\\"Dockerfile\\\",\\n    \\\"Jenkinsfil\\\",\\n}\\n\\nDEFAULT_EXCLUDE_SUFFIXES = {}\\n\\nDEFAULT_EXCLUDE_DIRS = {\\n    \\\".git\\\",\\n    \\\".github\\\",\\n    \\\".terraform\\\",\\n    \\\"assets\\\",\\n    \\\"build\\\",\\n    \\\"dist\\\",\\n    \\\"env\\\",\\n    \\\"docs\\\",\\n    \\\"env\\\",\\n    \\\"images\\\",\\n    \\\"temp\\\",\\n    \\\"tests\\\",\\n    \\\"tmp\\\",\\n    \\\"venv\\\",\\n}\\n\\n\\ndef parse_args() -> typing.Any:\\n    \\\"\\\"\\\"\\n    Parse command line arguments.\\n    :return: Parsed arguments\\n    \\\"\\\"\\\"\\n    parser = argparse.ArgumentParser(\\n        description=\\\"Generate documentation for a project using AI.\\\",\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\n    )\\n\\n    parser.add_argument(\\n        \\\"-n\\\",\\n        \\\"--name\\\",\\n        help=\\\"local Repository name\\\",\\n        default=\\\"repo_name\\\",\\n        )\\n\\n    parser.add_argument(\\n        \\\"-d\\\",\\n        \\\"--directory\\\",\\n        help=\\\"top level directory to scan, default is current working directory\\\",\\n        default=os.getcwd(),\\n        )\\n\\n    parser.add_argument(\\n        \\\"-e\\\",\\n        \\\"--exclude_dirs\\\",\\n        nargs='*',\\n        help=\\\"comma separated list of directories to exclude\\\",\\n        default=\\\",\\\"\\n        )\\n\\n    return parser.parse_args()\\n\\nmodel = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.5-pro-exp-03-25\\\")\\nproject=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"sre-ai-dev\\\")\": \"This code defines default file suffixes to include/exclude for documentation generation. It also specifies directories to exclude from the scan, like \\\".git\\\" and \\\"venv.\\\"  The `parse_args` function handles command-line arguments for repository name, directory, and excluded directories.  Finally, it fetches environment variables for the Gemini model and project ID.\\n\", \"Summarize this text in 50 words: \\n\\n {'settings.py': 'import typing\\\\nimport argparse\\\\nimport os\\\\n\\\\n\\\\nDEFAULT_INCLUDE_SUFFIXES = {\\\\n    \\\".js\\\",\\\\n    \\\".json\\\",\\\\n    \\\".jsx\\\",\\\\n    \\\".md\\\",\\\\n    \\\".py\\\",\\\\n    \\\".pyi\\\",\\\\n    \\\".pyx\\\",\\\\n    \\\".rst\\\",\\\\n    \\\".tf\\\",\\\\n    \\\".tfvars\\\",\\\\n    \\\".ts\\\",\\\\n    \\\".tsx\\\",\\\\n    \\\".yaml\\\",\\\\n    \\\".yml\\\",\\\\n    \\\"Dockerfile\\\",\\\\n    \\\"Jenkinsfil\\\",\\\\n}\\\\n\\\\nDEFAULT_EXCLUDE_SUFFIXES = {}\\\\n\\\\nDEFAULT_EXCLUDE_DIRS = {\\\\n    \\\".git\\\",\\\\n    \\\".github\\\",\\\\n    \\\".terraform\\\",\\\\n    \\\"assets\\\",\\\\n    \\\"build\\\",\\\\n    \\\"dist\\\",\\\\n    \\\"env\\\",\\\\n    \\\"docs\\\",\\\\n    \\\"env\\\",\\\\n    \\\"images\\\",\\\\n    \\\"temp\\\",\\\\n    \\\"tests\\\",\\\\n    \\\"tmp\\\",\\\\n    \\\"venv\\\",\\\\n}\\\\n\\\\n\\\\ndef parse_args() -> typing.Any:\\\\n    \\\"\\\"\\\"\\\\n    Parse command line arguments.\\\\n    :return: Parsed arguments\\\\n    \\\"\\\"\\\"\\\\n    parser = argparse.ArgumentParser(\\\\n        description=\\\"Generate documentation for a project using AI.\\\",\\\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\\\n    )\\\\n\\\\n    parser.add_argument(\\\\n        \\\"-n\\\",\\\\n        \\\"--name\\\",\\\\n        help=\\\"local Repository name\\\",\\\\n        default=\\\"repo_name\\\",\\\\n        )\\\\n\\\\n    parser.add_argument(\\\\n        \\\"-d\\\",\\\\n        \\\"--directory\\\",\\\\n        help=\\\"top level directory to scan, default is current working directory\\\",\\\\n        default=os.getcwd(),\\\\n        )\\\\n\\\\n    parser.add_argument(\\\\n        \\\"-e\\\",\\\\n        \\\"--exclude_dirs\\\",\\\\n        nargs=\\\\'*\\\\',\\\\n        help=\\\"comma separated list of directories to exclude\\\",\\\\n        default=\\\",\\\"\\\\n        )\\\\n\\\\n    return parser.parse_args()\\\\n\\\\nmodel = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.5-pro-exp-03-25\\\")\\\\nproject=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"sre-ai-dev\\\")', 'llm_cache.json': '{\\\"Summarize this text in 50 words: \\\\\\\\n\\\\\\\\n import typing\\\\\\\\nimport argparse\\\\\\\\nimport os\\\\\\\\n\\\\\\\\n\\\\\\\\nDEFAULT_INCLUDE_SUFFIXES = {\\\\\\\\n    \\\\\\\\\\\".js\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".json\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".jsx\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".md\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".py\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".pyi\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".pyx\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".rst\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".tf\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".tfvars\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".ts\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".tsx\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".yaml\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".yml\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"Dockerfile\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"Jenkinsfil\\\\\\\\\\\",\\\\\\\\n}\\\\\\\\n\\\\\\\\nDEFAULT_EXCLUDE_SUFFIXES = {}\\\\\\\\n\\\\\\\\nDEFAULT_EXCLUDE_DIRS = {\\\\\\\\n    \\\\\\\\\\\".git\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".github\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".terraform\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"assets\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"build\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"dist\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"env\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"docs\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"env\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"images\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"temp\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"tests\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"tmp\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"venv\\\\\\\\\\\",\\\\\\\\n}\\\\\\\\n\\\\\\\\n\\\\\\\\ndef parse_args() -> typing.Any:\\\\\\\\n    \\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\n    Parse command line arguments.\\\\\\\\n    :return: Parsed arguments\\\\\\\\n    \\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\n    parser = argparse.ArgumentParser(\\\\\\\\n        description=\\\\\\\\\\\"Generate documentation for a project using AI.\\\\\\\\\\\",\\\\\\\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\\\\\\\n    )\\\\\\\\n\\\\\\\\n    parser.add_argument(\\\\\\\\n        \\\\\\\\\\\"-n\\\\\\\\\\\",\\\\\\\\n        \\\\\\\\\\\"--name\\\\\\\\\\\",\\\\\\\\n        help=\\\\\\\\\\\"local Repository name\\\\\\\\\\\",\\\\\\\\n        default=\\\\\\\\\\\"repo_name\\\\\\\\\\\",\\\\\\\\n        )\\\\\\\\n\\\\\\\\n    parser.add_argument(\\\\\\\\n        \\\\\\\\\\\"-d\\\\\\\\\\\",\\\\\\\\n        \\\\\\\\\\\"--directory\\\\\\\\\\\",\\\\\\\\n        help=\\\\\\\\\\\"top level directory to scan, default is current working directory\\\\\\\\\\\",\\\\\\\\n        default=os.getcwd(),\\\\\\\\n        )\\\\\\\\n\\\\\\\\n    parser.add_argument(\\\\\\\\n        \\\\\\\\\\\"-e\\\\\\\\\\\",\\\\\\\\n        \\\\\\\\\\\"--exclude_dirs\\\\\\\\\\\",\\\\\\\\n        nargs=\\\\'*\\\\',\\\\\\\\n        help=\\\\\\\\\\\"comma separated list of directories to exclude\\\\\\\\\\\",\\\\\\\\n        default=\\\\\\\\\\\",\\\\\\\\\\\"\\\\\\\\n        )\\\\\\\\n\\\\\\\\n    return parser.parse_args()\\\\\\\\n\\\\\\\\nmodel = os.getenv(\\\\\\\\\\\"GEMINI_MODEL\\\\\\\\\\\", \\\\\\\\\\\"gemini-2.5-pro-exp-03-25\\\\\\\\\\\")\\\\\\\\nproject=os.getenv(\\\\\\\\\\\"GEMINI_PROJECT_ID\\\\\\\\\\\", \\\\\\\\\\\"sre-ai-dev\\\\\\\\\\\")\\\": \\\"This code defines default file suffixes to include/exclude for documentation generation. It also specifies directories to exclude from the scan, like \\\\\\\\\\\".git\\\\\\\\\\\" and \\\\\\\\\\\"venv.\\\\\\\\\\\"  The `parse_args` function handles command-line arguments for repository name, directory, and excluded directories.  Finally, it fetches environment variables for the Gemini model and project ID.\\\\\\\\n\\\"}', 'fetch_files.py': '# A script to crawl files from a given directory and its subdirectories.\\\\n# It uses the `os` module to walk through the directory structure and collects file paths based on specified include and exclude patterns.\\\\n# files that are not in the exclude patterns and are in the include patterns are collected.\\\\nimport os\\\\nimport typing\\\\nimport logging\\\\nfrom pathlib import Path\\\\nfrom settings import DEFAULT_EXCLUDE_SUFFIXES, DEFAULT_INCLUDE_SUFFIXES, DEFAULT_EXCLUDE_DIRS\\\\n\\\\n\\\\n\\\\ndef crawl_files(\\\\n    directory: str,\\\\n    include_patterns: typing.Set[str] = DEFAULT_INCLUDE_SUFFIXES,\\\\n    exclude_suffixes: typing.Set[str] = DEFAULT_EXCLUDE_SUFFIXES,\\\\n    exclude_dirs: typing.Set[str] = DEFAULT_EXCLUDE_DIRS,\\\\n    ) -> typing.List[str]:\\\\n    try:\\\\n        files = {\\\\n            str(item.relative_to(directory))  # Store the relative path string\\\\n            for item in Path(directory).rglob(\\\"*\\\") # Iterate through all items recursively\\\\n            if item.is_file() # Consider only files\\\\n            if item.suffix in include_patterns # Check 0: Suffix is included\\\\n            and item.suffix not in exclude_suffixes # Check 1: Suffix is not excluded\\\\n            and not any(part in exclude_dirs for part in item.relative_to(directory).parts) # Check 2: No path part is excluded\\\\n        }\\\\n\\\\n    except ValueError as e:\\\\n        logging.error(f\\\"Error calculating relative paths: {e}\\\")\\\\n        logging.error(f\\\"Ensure the search directory \\\\'{directory.resolve()}\\\\' is within or is the \\\"\\\\n            f\\\"current working directory \\\\'{directory.resolve()}\\\\'\\\")\\\\n\\\\n    logging.info(f\\\"Found {len(files)} files in {directory}\\\")\\\\n    logging.debug(f\\\"Files found: {files}\\\")\\\\n\\\\n    return files\\\\n\\\\n\\\\ndef main() -> None:\\\\n    print(\\\"Crawling files...\\\")\\\\n    directory = os.getcwd()  \\\\n    print(f\\\"Files crawled from {directory}:\\\")\\\\n    print (crawl_files(directory))\\\\n\\\\nif __name__ == \\\"__main__\\\":\\\\n    main()', 'nodes.py': 'from pocketflow import Node\\\\nfrom call_llm import call_llm\\\\n\\\\n\\\\nclass Summarise(Node):\\\\n    \\\"\\\"\\\"\\\\n    This class identifies abstractions in the code.\\\\n    \\\"\\\"\\\"\\\\n\\\\n    def prep(self,shared):\\\\n        return shared_settings[\\\"data\\\"][\\\"settings.py\\\"]\\\\n    \\\\n    def exec(self, text):\\\\n        prompt = f\\\"Summarize this text in 50 words: \\\\\\\\n\\\\\\\\n {text}\\\"\\\\n        return call_llm(prompt)\\\\n    \\\\n    def post(self, shared, prep_res, exec_res):\\\\n        # Store the result in the shared settings\\\\n        shared_settings[\\\"summaries\\\"] = exec_res\\\\n        return \\\"default\\\"\\\\n    \\\\nfile_contents = {\\\\n    \\\"data\\\": {},\\\\n    \\\"summaries\\\": None\\\\n}\\\\n\\\\n# with open(\\\"settings.py\\\", \\\"r\\\") as f:\\\\n#     shared_settings[\\\"data\\\"][\\\"settings.py\\\"] = f.read()\\\\n\\\\n# summarise_node = Summarise()\\\\n# summarise_node.run(shared_settings)\\\\n\\\\n# print(f\\\"Summary: {shared_settings[\\\\'summaries\\\\']}\\\")', '__init__.py': '', 'call_llm.py': 'from google import genai\\\\nimport os\\\\nimport logging\\\\nimport json\\\\nfrom datetime import datetime\\\\nimport requests\\\\n\\\\n# Configure logging\\\\nlog_directory = os.getenv(\\\"LOG_DIR\\\", \\\"logs\\\")\\\\nos.makedirs(log_directory, exist_ok=True)\\\\nlog_file = os.path.join(\\\\n    log_directory, f\\\"llm_calls_{datetime.now().strftime(\\\\'%Y%m%d\\\\')}.log\\\"\\\\n)\\\\n\\\\n# Set up logger\\\\nlogger = logging.getLogger(\\\"llm_logger\\\")\\\\nlogger.setLevel(logging.INFO)\\\\nlogger.propagate = False  # Prevent propagation to root logger\\\\nfile_handler = logging.FileHandler(log_file)\\\\nfile_handler.setFormatter(\\\\n    logging.Formatter(\\\"%(asctime)s - %(levelname)s - %(message)s\\\")\\\\n)\\\\nlogger.addHandler(file_handler)\\\\n\\\\n# Simple cache configuration\\\\ncache_file = \\\"llm_cache.json\\\"\\\\n\\\\n\\\\n# By default, we Google Gemini 2.5 pro, as it shows great performance for code understanding\\\\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\\\\n    # Log the prompt\\\\n    logger.info(f\\\"PROMPT: {prompt}\\\")\\\\n\\\\n    # Check cache if enabled\\\\n    if use_cache:\\\\n        # Load cache from disk\\\\n        cache = {}\\\\n        if os.path.exists(cache_file):\\\\n            try:\\\\n                with open(cache_file, \\\"r\\\") as f:\\\\n                    cache = json.load(f)\\\\n            except:\\\\n                logger.warning(f\\\"Failed to load cache, starting with empty cache\\\")\\\\n\\\\n        # Return from cache if exists\\\\n        if prompt in cache:\\\\n            logger.info(f\\\"RESPONSE: {cache[prompt]}\\\")\\\\n            return cache[prompt]\\\\n\\\\n    # Call the LLM if not in cache or cache disabled\\\\n    client = genai.Client(\\\\n        vertexai=True,\\\\n        # TODO: change to your own project id and location\\\\n        project=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"ai-sre-dev-84b7\\\"),\\\\n        location=os.getenv(\\\"GEMINI_LOCATION\\\", \\\"us-central1\\\")\\\\n    )\\\\n\\\\n    model = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.0-flash-exp\\\")\\\\n    \\\\n    response = client.models.generate_content(model=model, contents=[prompt])\\\\n    response_text = response.text\\\\n\\\\n    # Log the response\\\\n    logger.info(f\\\"RESPONSE: {response_text}\\\")\\\\n\\\\n    # Update cache if enabled\\\\n    if use_cache:\\\\n        # Load cache again to avoid overwrites\\\\n        cache = {}\\\\n        if os.path.exists(cache_file):\\\\n            try:\\\\n                with open(cache_file, \\\"r\\\") as f:\\\\n                    cache = json.load(f)\\\\n            except:\\\\n                pass\\\\n\\\\n        # Add to cache and save\\\\n        cache[prompt] = response_text\\\\n        try:\\\\n            with open(cache_file, \\\"w\\\") as f:\\\\n                json.dump(cache, f)\\\\n        except Exception as e:\\\\n            logger.error(f\\\"Failed to save cache: {e}\\\")\\\\n\\\\n    return response_text\\\\n\\\\n\\\\nif __name__ == \\\"__main__\\\":\\\\n    test_prompt = \\\"Hello, how are you?\\\"\\\\n\\\\n    # First call - should hit the API\\\\n    print(\\\"Making call...\\\")\\\\n    response1 = call_llm(test_prompt, use_cache=False)\\\\n    print(f\\\"Response: {response1}\\\")\\\\n', 'main.py': 'import argparse\\\\nimport os\\\\nimport sys\\\\nimport typing\\\\nimport logging\\\\nfrom pocketflow import Node\\\\nfrom call_llm import call_llm\\\\nfrom settings import (\\\\n    DEFAULT_EXCLUDE_SUFFIXES,\\\\n    DEFAULT_INCLUDE_SUFFIXES,\\\\n    DEFAULT_EXCLUDE_DIRS,\\\\n)\\\\nfrom settings import parse_args\\\\nimport fetch_files\\\\n\\\\n\\\\nclass Summarise(Node):\\\\n    \\\"\\\"\\\"\\\\n    This class identifies abstractions in the code.\\\\n    \\\"\\\"\\\"\\\\n\\\\n    def prep(self,shared):\\\\n        return file_contents[\\\"data\\\"]\\\\n    \\\\n    def exec(self, text):\\\\n        prompt = f\\\"Summarize this text in 50 words: \\\\\\\\n\\\\\\\\n {text}\\\"\\\\n        return call_llm(prompt)\\\\n    \\\\n    def post(self, shared, prep_res, exec_res):\\\\n        # Store the result in the shared settings\\\\n        file_contents[\\\"summaries\\\"] = exec_res\\\\n        return \\\"default\\\"\\\\n    \\\\nfile_contents = {\\\\n\\\"data\\\": {},\\\\n\\\"summaries\\\": None\\\\n}\\\\n\\\\n\\\\ndef main() -> None:\\\\n    args = parse_args()\\\\n\\\\n    # create a dictionarty of settings to be used in the script\\\\n    shared_settings = {\\\\n        \\\"name\\\": args.name,\\\\n        \\\"directory\\\": args.directory,\\\\n        \\\"exclude_patterns\\\": DEFAULT_EXCLUDE_SUFFIXES,\\\\n        \\\"exclude_dirs\\\": DEFAULT_EXCLUDE_DIRS,\\\\n        \\\"include_suffixes\\\": DEFAULT_INCLUDE_SUFFIXES,\\\\n        \\\"additional_exclude_dirs\\\": args.exclude_dirs,\\\\n    }\\\\n    logging.debug(f\\\"Shared settings: {shared_settings}\\\")\\\\n\\\\n    # Get a list of files from the directory and its subdirectories and append to the shared settings\\\\n    shared_settings[\\\"files\\\"] = fetch_files.crawl_files(\\\\n        directory=shared_settings[\\\"directory\\\"],\\\\n        exclude_dirs=shared_settings[\\\"exclude_dirs\\\"].union(\\\\n            shared_settings[\\\"additional_exclude_dirs\\\"]\\\\n        ),\\\\n    )\\\\n\\\\n\\\\n\\\\n    for file in shared_settings[\\\"files\\\"]:\\\\n        with open(file, \\\"r\\\") as f:\\\\n            file_contents[\\\"data\\\"][file] = f.read()\\\\n    print(f\\\"File contents: {file_contents}\\\")\\\\n    \\\\n    summarise_node = Summarise()\\\\n    summarise_node.run(file_contents[\\\"data\\\"])\\\\n\\\\n    print(f\\\"Summary: {file_contents[\\\\'summaries\\\\']}\\\")\\\\n\\\\n    # print (shared_settings)\\\\n\\\\n\\\\nif __name__ == \\\"__main__\\\":\\\\n    main()\\\\n'}\": \"The code generates project documentation using AI. It defines default file types to include/exclude and directories to skip during scanning. Command-line arguments customize the repository name, scan directory, and excluded directories. It crawls files and utilizes a Gemini model via API calls to summarize the content, storing results in a cache.\", \"Summarize this text in 50 words: \\n\\n {'settings.py': 'import typing\\\\nimport argparse\\\\nimport os\\\\n\\\\n\\\\nDEFAULT_INCLUDE_SUFFIXES = {\\\\n    \\\".js\\\",\\\\n    \\\".json\\\",\\\\n    \\\".jsx\\\",\\\\n    \\\".md\\\",\\\\n    \\\".py\\\",\\\\n    \\\".pyi\\\",\\\\n    \\\".pyx\\\",\\\\n    \\\".rst\\\",\\\\n    \\\".tf\\\",\\\\n    \\\".tfvars\\\",\\\\n    \\\".ts\\\",\\\\n    \\\".tsx\\\",\\\\n    \\\".yaml\\\",\\\\n    \\\".yml\\\",\\\\n    \\\"Dockerfile\\\",\\\\n    \\\"Jenkinsfil\\\",\\\\n}\\\\n\\\\nDEFAULT_EXCLUDE_SUFFIXES = {}\\\\n\\\\nDEFAULT_EXCLUDE_DIRS = {\\\\n    \\\".git\\\",\\\\n    \\\".github\\\",\\\\n    \\\".terraform\\\",\\\\n    \\\"assets\\\",\\\\n    \\\"build\\\",\\\\n    \\\"dist\\\",\\\\n    \\\"env\\\",\\\\n    \\\"docs\\\",\\\\n    \\\"env\\\",\\\\n    \\\"images\\\",\\\\n    \\\"temp\\\",\\\\n    \\\"tests\\\",\\\\n    \\\"tmp\\\",\\\\n    \\\"venv\\\",\\\\n}\\\\n\\\\n\\\\ndef parse_args() -> typing.Any:\\\\n    \\\"\\\"\\\"\\\\n    Parse command line arguments.\\\\n    :return: Parsed arguments\\\\n    \\\"\\\"\\\"\\\\n    parser = argparse.ArgumentParser(\\\\n        description=\\\"Generate documentation for a project using AI.\\\",\\\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\\\n    )\\\\n\\\\n    parser.add_argument(\\\\n        \\\"-n\\\",\\\\n        \\\"--name\\\",\\\\n        help=\\\"local Repository name\\\",\\\\n        default=\\\"repo_name\\\",\\\\n        )\\\\n\\\\n    parser.add_argument(\\\\n        \\\"-d\\\",\\\\n        \\\"--directory\\\",\\\\n        help=\\\"top level directory to scan, default is current working directory\\\",\\\\n        default=os.getcwd(),\\\\n        )\\\\n\\\\n    parser.add_argument(\\\\n        \\\"-e\\\",\\\\n        \\\"--exclude_dirs\\\",\\\\n        nargs=\\\\'*\\\\',\\\\n        help=\\\"comma separated list of directories to exclude\\\",\\\\n        default=\\\",\\\"\\\\n        )\\\\n\\\\n    return parser.parse_args()\\\\n\\\\nmodel = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.5-pro-exp-03-25\\\")\\\\nproject=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"sre-ai-dev\\\")', '__init__.py': '', 'main.py': 'import logging\\\\nfrom pocketflow import Node\\\\nfrom call_llm import call_llm\\\\nfrom settings import (\\\\n    DEFAULT_EXCLUDE_SUFFIXES,\\\\n    DEFAULT_INCLUDE_SUFFIXES,\\\\n    DEFAULT_EXCLUDE_DIRS,\\\\n)\\\\nfrom settings import parse_args\\\\nimport fetch_files\\\\n\\\\n\\\\nclass Summarise(Node):\\\\n    \\\"\\\"\\\"\\\\n    This class identifies abstractions in the code.\\\\n    \\\"\\\"\\\"\\\\n\\\\n    def prep(self,shared):\\\\n        return file_contents[\\\"data\\\"]\\\\n    \\\\n    def exec(self, text):\\\\n        prompt = f\\\"Summarize this text in 50 words: \\\\\\\\n\\\\\\\\n {text}\\\"\\\\n        return call_llm(prompt)\\\\n    \\\\n    def post(self, shared, prep_res, exec_res):\\\\n        # Store the result in the shared settings\\\\n        file_contents[\\\"summaries\\\"] = exec_res\\\\n        return \\\"default\\\"\\\\n\\\\n\\\\nfile_contents = {\\\\n\\\"data\\\": {},\\\\n\\\"summaries\\\": None\\\\n}\\\\n\\\\n\\\\ndef main() -> None:\\\\n    args = parse_args()\\\\n\\\\n    # create a dictionarty of settings to be used in the script\\\\n    shared_settings = {\\\\n        \\\"name\\\": args.name,\\\\n        \\\"directory\\\": args.directory,\\\\n        \\\"exclude_patterns\\\": DEFAULT_EXCLUDE_SUFFIXES,\\\\n        \\\"exclude_dirs\\\": DEFAULT_EXCLUDE_DIRS,\\\\n        \\\"include_suffixes\\\": DEFAULT_INCLUDE_SUFFIXES,\\\\n        \\\"additional_exclude_dirs\\\": args.exclude_dirs,\\\\n    }\\\\n    logging.debug(f\\\"Shared settings: {shared_settings}\\\")\\\\n\\\\n    # Get a list of files from the directory and its subdirectories and append to the shared settings\\\\n    shared_settings[\\\"files\\\"] = fetch_files.crawl_files(\\\\n        directory=shared_settings[\\\"directory\\\"],\\\\n        exclude_dirs=shared_settings[\\\"exclude_dirs\\\"].union(\\\\n            shared_settings[\\\"additional_exclude_dirs\\\"]\\\\n        ),\\\\n    )\\\\n\\\\n    for file in shared_settings[\\\"files\\\"]:\\\\n        with open(file, \\\"r\\\") as f:\\\\n            file_contents[\\\"data\\\"][file] = f.read()\\\\n    \\\\n    summarise_node = Summarise()\\\\n    summarise_node.run(file_contents[\\\"data\\\"])\\\\n\\\\n    print(f\\\"Summary: {file_contents[\\\\'summaries\\\\']}\\\")\\\\n\\\\n    # print (shared_settings)\\\\n\\\\n\\\\nif __name__ == \\\"__main__\\\":\\\\n    main()\\\\n', 'fetch_files.py': '# A script to crawl files from a given directory and its subdirectories.\\\\n# It uses the `os` module to walk through the directory structure and collects file paths based on specified include and exclude patterns.\\\\n# files that are not in the exclude patterns and are in the include patterns are collected.\\\\nimport os\\\\nimport typing\\\\nimport logging\\\\nfrom pathlib import Path\\\\nfrom settings import DEFAULT_EXCLUDE_SUFFIXES, DEFAULT_INCLUDE_SUFFIXES, DEFAULT_EXCLUDE_DIRS\\\\n\\\\n\\\\n\\\\ndef crawl_files(\\\\n    directory: str,\\\\n    include_patterns: typing.Set[str] = DEFAULT_INCLUDE_SUFFIXES,\\\\n    exclude_suffixes: typing.Set[str] = DEFAULT_EXCLUDE_SUFFIXES,\\\\n    exclude_dirs: typing.Set[str] = DEFAULT_EXCLUDE_DIRS,\\\\n    ) -> typing.List[str]:\\\\n    try:\\\\n        files = {\\\\n            str(item.relative_to(directory))  # Store the relative path string\\\\n            for item in Path(directory).rglob(\\\"*\\\") # Iterate through all items recursively\\\\n            if item.is_file() # Consider only files\\\\n            if item.suffix in include_patterns # Check 0: Suffix is included\\\\n            and item.suffix not in exclude_suffixes # Check 1: Suffix is not excluded\\\\n            and not any(part in exclude_dirs for part in item.relative_to(directory).parts) # Check 2: No path part is excluded\\\\n        }\\\\n\\\\n    except ValueError as e:\\\\n        logging.error(f\\\"Error calculating relative paths: {e}\\\")\\\\n        logging.error(f\\\"Ensure the search directory \\\\'{directory.resolve()}\\\\' is within or is the \\\"\\\\n            f\\\"current working directory \\\\'{directory.resolve()}\\\\'\\\")\\\\n\\\\n    logging.info(f\\\"Found {len(files)} files in {directory}\\\")\\\\n    logging.debug(f\\\"Files found: {files}\\\")\\\\n\\\\n    return files\\\\n\\\\n\\\\ndef main() -> None:\\\\n    print(\\\"Crawling files...\\\")\\\\n    directory = os.getcwd()  \\\\n    print(f\\\"Files crawled from {directory}:\\\")\\\\n    print (crawl_files(directory))\\\\n\\\\nif __name__ == \\\"__main__\\\":\\\\n    main()', 'call_llm.py': 'from google import genai\\\\nimport os\\\\nimport logging\\\\nimport json\\\\nfrom datetime import datetime\\\\n\\\\n\\\\n# Configure logging\\\\nlog_directory = os.getenv(\\\"LOG_DIR\\\", \\\"logs\\\")\\\\nos.makedirs(log_directory, exist_ok=True)\\\\nlog_file = os.path.join(\\\\n    log_directory, f\\\"llm_calls_{datetime.now().strftime(\\\\'%Y%m%d\\\\')}.log\\\"\\\\n)\\\\n\\\\n# Set up logger\\\\nlogger = logging.getLogger(\\\"llm_logger\\\")\\\\nlogger.setLevel(logging.INFO)\\\\nlogger.propagate = False  # Prevent propagation to root logger\\\\nfile_handler = logging.FileHandler(log_file)\\\\nfile_handler.setFormatter(\\\\n    logging.Formatter(\\\"%(asctime)s - %(levelname)s - %(message)s\\\")\\\\n)\\\\nlogger.addHandler(file_handler)\\\\n\\\\n# Simple cache configuration\\\\ncache_file = \\\"llm_cache.json\\\"\\\\n\\\\n\\\\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\\\\n    # Log the prompt\\\\n    logger.info(f\\\"PROMPT: {prompt}\\\")\\\\n\\\\n    # Check cache if enabled\\\\n    if use_cache:\\\\n        # Load cache from disk\\\\n        cache = {}\\\\n        if os.path.exists(cache_file):\\\\n            try:\\\\n                with open(cache_file, \\\"r\\\") as f:\\\\n                    cache = json.load(f)\\\\n            except:\\\\n                logger.warning(f\\\"Failed to load cache, starting with empty cache\\\")\\\\n\\\\n        # Return from cache if exists\\\\n        if prompt in cache:\\\\n            logger.info(f\\\"RESPONSE: {cache[prompt]}\\\")\\\\n            return cache[prompt]\\\\n\\\\n    # Call the LLM if not in cache or cache disabled\\\\n    client = genai.Client(\\\\n        vertexai=True,\\\\n        project=os.getenv(\\\"GEMINI_PROJECT_ID\\\", \\\"ai-sre-dev-84b7\\\"),\\\\n        location=os.getenv(\\\"GEMINI_LOCATION\\\", \\\"us-central1\\\")\\\\n    )\\\\n\\\\n    model = os.getenv(\\\"GEMINI_MODEL\\\", \\\"gemini-2.0-flash-exp\\\")\\\\n    \\\\n    response = client.models.generate_content(model=model, contents=[prompt])\\\\n    response_text = response.text\\\\n\\\\n    # Log the response\\\\n    logger.info(f\\\"RESPONSE: {response_text}\\\")\\\\n\\\\n    # Update cache if enabled\\\\n    if use_cache:\\\\n        # Load cache again to avoid overwrites\\\\n        cache = {}\\\\n        if os.path.exists(cache_file):\\\\n            try:\\\\n                with open(cache_file, \\\"r\\\") as f:\\\\n                    cache = json.load(f)\\\\n            except:\\\\n                pass\\\\n\\\\n        # Add to cache and save\\\\n        cache[prompt] = response_text\\\\n        try:\\\\n            with open(cache_file, \\\"w\\\") as f:\\\\n                json.dump(cache, f)\\\\n        except Exception as e:\\\\n            logger.error(f\\\"Failed to save cache: {e}\\\")\\\\n\\\\n    return response_text\\\\n\\\\n\\\\nif __name__ == \\\"__main__\\\":\\\\n    test_prompt = \\\"Hello, how are you?\\\"\\\\n\\\\n    # First call - should hit the API\\\\n    print(\\\"Making call...\\\")\\\\n    response1 = call_llm(test_prompt, use_cache=False)\\\\n    print(f\\\"Response: {response1}\\\")\\\\n', 'llm_cache.json': '{\\\"Summarize this text in 50 words: \\\\\\\\n\\\\\\\\n import typing\\\\\\\\nimport argparse\\\\\\\\nimport os\\\\\\\\n\\\\\\\\n\\\\\\\\nDEFAULT_INCLUDE_SUFFIXES = {\\\\\\\\n    \\\\\\\\\\\".js\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".json\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".jsx\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".md\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".py\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".pyi\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".pyx\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".rst\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".tf\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".tfvars\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".ts\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".tsx\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".yaml\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".yml\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"Dockerfile\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"Jenkinsfil\\\\\\\\\\\",\\\\\\\\n}\\\\\\\\n\\\\\\\\nDEFAULT_EXCLUDE_SUFFIXES = {}\\\\\\\\n\\\\\\\\nDEFAULT_EXCLUDE_DIRS = {\\\\\\\\n    \\\\\\\\\\\".git\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".github\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\".terraform\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"assets\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"build\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"dist\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"env\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"docs\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"env\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"images\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"temp\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"tests\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"tmp\\\\\\\\\\\",\\\\\\\\n    \\\\\\\\\\\"venv\\\\\\\\\\\",\\\\\\\\n}\\\\\\\\n\\\\\\\\n\\\\\\\\ndef parse_args() -> typing.Any:\\\\\\\\n    \\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\n    Parse command line arguments.\\\\\\\\n    :return: Parsed arguments\\\\\\\\n    \\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\n    parser = argparse.ArgumentParser(\\\\\\\\n        description=\\\\\\\\\\\"Generate documentation for a project using AI.\\\\\\\\\\\",\\\\\\\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\\\\\\\n    )\\\\\\\\n\\\\\\\\n    parser.add_argument(\\\\\\\\n        \\\\\\\\\\\"-n\\\\\\\\\\\",\\\\\\\\n        \\\\\\\\\\\"--name\\\\\\\\\\\",\\\\\\\\n        help=\\\\\\\\\\\"local Repository name\\\\\\\\\\\",\\\\\\\\n        default=\\\\\\\\\\\"repo_name\\\\\\\\\\\",\\\\\\\\n        )\\\\\\\\n\\\\\\\\n    parser.add_argument(\\\\\\\\n        \\\\\\\\\\\"-d\\\\\\\\\\\",\\\\\\\\n        \\\\\\\\\\\"--directory\\\\\\\\\\\",\\\\\\\\n        help=\\\\\\\\\\\"top level directory to scan, default is current working directory\\\\\\\\\\\",\\\\\\\\n        default=os.getcwd(),\\\\\\\\n        )\\\\\\\\n\\\\\\\\n    parser.add_argument(\\\\\\\\n        \\\\\\\\\\\"-e\\\\\\\\\\\",\\\\\\\\n        \\\\\\\\\\\"--exclude_dirs\\\\\\\\\\\",\\\\\\\\n        nargs=\\\\'*\\\\',\\\\\\\\n        help=\\\\\\\\\\\"comma separated list of directories to exclude\\\\\\\\\\\",\\\\\\\\n        default=\\\\\\\\\\\",\\\\\\\\\\\"\\\\\\\\n        )\\\\\\\\n\\\\\\\\n    return parser.parse_args()\\\\\\\\n\\\\\\\\nmodel = os.getenv(\\\\\\\\\\\"GEMINI_MODEL\\\\\\\\\\\", \\\\\\\\\\\"gemini-2.5-pro-exp-03-25\\\\\\\\\\\")\\\\\\\\nproject=os.getenv(\\\\\\\\\\\"GEMINI_PROJECT_ID\\\\\\\\\\\", \\\\\\\\\\\"sre-ai-dev\\\\\\\\\\\")\\\": \\\"This code defines default file suffixes to include/exclude for documentation generation. It also specifies directories to exclude from the scan, like \\\\\\\\\\\".git\\\\\\\\\\\" and \\\\\\\\\\\"venv.\\\\\\\\\\\"  The `parse_args` function handles command-line arguments for repository name, directory, and excluded directories.  Finally, it fetches environment variables for the Gemini model and project ID.\\\\\\\\n\\\", \\\"Summarize this text in 50 words: \\\\\\\\n\\\\\\\\n {\\\\'settings.py\\\\': \\\\'import typing\\\\\\\\\\\\\\\\nimport argparse\\\\\\\\\\\\\\\\nimport os\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nDEFAULT_INCLUDE_SUFFIXES = {\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".js\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".json\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".jsx\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".md\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".py\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".pyi\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".pyx\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".rst\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".tf\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".tfvars\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".ts\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".tsx\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".yaml\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".yml\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"Dockerfile\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"Jenkinsfil\\\\\\\\\\\",\\\\\\\\\\\\\\\\n}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nDEFAULT_EXCLUDE_SUFFIXES = {}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nDEFAULT_EXCLUDE_DIRS = {\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".git\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".github\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\".terraform\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"assets\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"build\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"dist\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"env\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"docs\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"env\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"images\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"temp\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"tests\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"tmp\\\\\\\\\\\",\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"venv\\\\\\\\\\\",\\\\\\\\\\\\\\\\n}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\ndef parse_args() -> typing.Any:\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\\\\\\n    Parse command line arguments.\\\\\\\\\\\\\\\\n    :return: Parsed arguments\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\\\\\\n    parser = argparse.ArgumentParser(\\\\\\\\\\\\\\\\n        description=\\\\\\\\\\\"Generate documentation for a project using AI.\\\\\\\\\\\",\\\\\\\\\\\\\\\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\\\\\\\\\\\\\\\n    )\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    parser.add_argument(\\\\\\\\\\\\\\\\n        \\\\\\\\\\\"-n\\\\\\\\\\\",\\\\\\\\\\\\\\\\n        \\\\\\\\\\\"--name\\\\\\\\\\\",\\\\\\\\\\\\\\\\n        help=\\\\\\\\\\\"local Repository name\\\\\\\\\\\",\\\\\\\\\\\\\\\\n        default=\\\\\\\\\\\"repo_name\\\\\\\\\\\",\\\\\\\\\\\\\\\\n        )\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    parser.add_argument(\\\\\\\\\\\\\\\\n        \\\\\\\\\\\"-d\\\\\\\\\\\",\\\\\\\\\\\\\\\\n        \\\\\\\\\\\"--directory\\\\\\\\\\\",\\\\\\\\\\\\\\\\n        help=\\\\\\\\\\\"top level directory to scan, default is current working directory\\\\\\\\\\\",\\\\\\\\\\\\\\\\n        default=os.getcwd(),\\\\\\\\\\\\\\\\n        )\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    parser.add_argument(\\\\\\\\\\\\\\\\n        \\\\\\\\\\\"-e\\\\\\\\\\\",\\\\\\\\\\\\\\\\n        \\\\\\\\\\\"--exclude_dirs\\\\\\\\\\\",\\\\\\\\\\\\\\\\n        nargs=\\\\\\\\\\\\\\\\\\\\'*\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n        help=\\\\\\\\\\\"comma separated list of directories to exclude\\\\\\\\\\\",\\\\\\\\\\\\\\\\n        default=\\\\\\\\\\\",\\\\\\\\\\\"\\\\\\\\\\\\\\\\n        )\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    return parser.parse_args()\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nmodel = os.getenv(\\\\\\\\\\\"GEMINI_MODEL\\\\\\\\\\\", \\\\\\\\\\\"gemini-2.5-pro-exp-03-25\\\\\\\\\\\")\\\\\\\\\\\\\\\\nproject=os.getenv(\\\\\\\\\\\"GEMINI_PROJECT_ID\\\\\\\\\\\", \\\\\\\\\\\"sre-ai-dev\\\\\\\\\\\")\\\\', \\\\'llm_cache.json\\\\': \\\\'{\\\\\\\\\\\"Summarize this text in 50 words: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import typing\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nimport argparse\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nimport os\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nDEFAULT_INCLUDE_SUFFIXES = {\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".js\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".json\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".jsx\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".md\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".pyi\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".pyx\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".rst\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".tf\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".tfvars\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".ts\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".tsx\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".yaml\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".yml\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"Dockerfile\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"Jenkinsfil\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nDEFAULT_EXCLUDE_SUFFIXES = {}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nDEFAULT_EXCLUDE_DIRS = {\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".git\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".github\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".terraform\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"assets\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"build\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"dist\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"env\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"docs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"env\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"images\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"temp\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"tests\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"tmp\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"venv\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndef parse_args() -> typing.Any:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    Parse command line arguments.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    :return: Parsed arguments\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    parser = argparse.ArgumentParser(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        description=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"Generate documentation for a project using AI.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    )\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"-n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"--name\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"local Repository name\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        default=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"repo_name\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        )\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"-d\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"--directory\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"top level directory to scan, default is current working directory\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        default=os.getcwd(),\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        )\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"-e\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"--exclude_dirs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        nargs=\\\\\\\\\\\\\\\\\\\\'*\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"comma separated list of directories to exclude\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        default=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        )\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    return parser.parse_args()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nmodel = os.getenv(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"GEMINI_MODEL\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"gemini-2.5-pro-exp-03-25\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nproject=os.getenv(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"GEMINI_PROJECT_ID\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"sre-ai-dev\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\\\\\\\\\\\": \\\\\\\\\\\"This code defines default file suffixes to include/exclude for documentation generation. It also specifies directories to exclude from the scan, like \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\".git\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\" and \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"venv.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"  The `parse_args` function handles command-line arguments for repository name, directory, and excluded directories.  Finally, it fetches environment variables for the Gemini model and project ID.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\"}\\\\', \\\\'fetch_files.py\\\\': \\\\'# A script to crawl files from a given directory and its subdirectories.\\\\\\\\\\\\\\\\n# It uses the `os` module to walk through the directory structure and collects file paths based on specified include and exclude patterns.\\\\\\\\\\\\\\\\n# files that are not in the exclude patterns and are in the include patterns are collected.\\\\\\\\\\\\\\\\nimport os\\\\\\\\\\\\\\\\nimport typing\\\\\\\\\\\\\\\\nimport logging\\\\\\\\\\\\\\\\nfrom pathlib import Path\\\\\\\\\\\\\\\\nfrom settings import DEFAULT_EXCLUDE_SUFFIXES, DEFAULT_INCLUDE_SUFFIXES, DEFAULT_EXCLUDE_DIRS\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\ndef crawl_files(\\\\\\\\\\\\\\\\n    directory: str,\\\\\\\\\\\\\\\\n    include_patterns: typing.Set[str] = DEFAULT_INCLUDE_SUFFIXES,\\\\\\\\\\\\\\\\n    exclude_suffixes: typing.Set[str] = DEFAULT_EXCLUDE_SUFFIXES,\\\\\\\\\\\\\\\\n    exclude_dirs: typing.Set[str] = DEFAULT_EXCLUDE_DIRS,\\\\\\\\\\\\\\\\n    ) -> typing.List[str]:\\\\\\\\\\\\\\\\n    try:\\\\\\\\\\\\\\\\n        files = {\\\\\\\\\\\\\\\\n            str(item.relative_to(directory))  # Store the relative path string\\\\\\\\\\\\\\\\n            for item in Path(directory).rglob(\\\\\\\\\\\"*\\\\\\\\\\\") # Iterate through all items recursively\\\\\\\\\\\\\\\\n            if item.is_file() # Consider only files\\\\\\\\\\\\\\\\n            if item.suffix in include_patterns # Check 0: Suffix is included\\\\\\\\\\\\\\\\n            and item.suffix not in exclude_suffixes # Check 1: Suffix is not excluded\\\\\\\\\\\\\\\\n            and not any(part in exclude_dirs for part in item.relative_to(directory).parts) # Check 2: No path part is excluded\\\\\\\\\\\\\\\\n        }\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    except ValueError as e:\\\\\\\\\\\\\\\\n        logging.error(f\\\\\\\\\\\"Error calculating relative paths: {e}\\\\\\\\\\\")\\\\\\\\\\\\\\\\n        logging.error(f\\\\\\\\\\\"Ensure the search directory \\\\\\\\\\\\\\\\\\\\'{directory.resolve()}\\\\\\\\\\\\\\\\\\\\' is within or is the \\\\\\\\\\\"\\\\\\\\\\\\\\\\n            f\\\\\\\\\\\"current working directory \\\\\\\\\\\\\\\\\\\\'{directory.resolve()}\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\")\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    logging.info(f\\\\\\\\\\\"Found {len(files)} files in {directory}\\\\\\\\\\\")\\\\\\\\\\\\\\\\n    logging.debug(f\\\\\\\\\\\"Files found: {files}\\\\\\\\\\\")\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    return files\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\ndef main() -> None:\\\\\\\\\\\\\\\\n    print(\\\\\\\\\\\"Crawling files...\\\\\\\\\\\")\\\\\\\\\\\\\\\\n    directory = os.getcwd()  \\\\\\\\\\\\\\\\n    print(f\\\\\\\\\\\"Files crawled from {directory}:\\\\\\\\\\\")\\\\\\\\\\\\\\\\n    print (crawl_files(directory))\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nif __name__ == \\\\\\\\\\\"__main__\\\\\\\\\\\":\\\\\\\\\\\\\\\\n    main()\\\\', \\\\'nodes.py\\\\': \\\\'from pocketflow import Node\\\\\\\\\\\\\\\\nfrom call_llm import call_llm\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nclass Summarise(Node):\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\\\\\\n    This class identifies abstractions in the code.\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    def prep(self,shared):\\\\\\\\\\\\\\\\n        return shared_settings[\\\\\\\\\\\"data\\\\\\\\\\\"][\\\\\\\\\\\"settings.py\\\\\\\\\\\"]\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\n    def exec(self, text):\\\\\\\\\\\\\\\\n        prompt = f\\\\\\\\\\\"Summarize this text in 50 words: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n {text}\\\\\\\\\\\"\\\\\\\\\\\\\\\\n        return call_llm(prompt)\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\n    def post(self, shared, prep_res, exec_res):\\\\\\\\\\\\\\\\n        # Store the result in the shared settings\\\\\\\\\\\\\\\\n        shared_settings[\\\\\\\\\\\"summaries\\\\\\\\\\\"] = exec_res\\\\\\\\\\\\\\\\n        return \\\\\\\\\\\"default\\\\\\\\\\\"\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\nfile_contents = {\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"data\\\\\\\\\\\": {},\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"summaries\\\\\\\\\\\": None\\\\\\\\\\\\\\\\n}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n# with open(\\\\\\\\\\\"settings.py\\\\\\\\\\\", \\\\\\\\\\\"r\\\\\\\\\\\") as f:\\\\\\\\\\\\\\\\n#     shared_settings[\\\\\\\\\\\"data\\\\\\\\\\\"][\\\\\\\\\\\"settings.py\\\\\\\\\\\"] = f.read()\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n# summarise_node = Summarise()\\\\\\\\\\\\\\\\n# summarise_node.run(shared_settings)\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n# print(f\\\\\\\\\\\"Summary: {shared_settings[\\\\\\\\\\\\\\\\\\\\'summaries\\\\\\\\\\\\\\\\\\\\']}\\\\\\\\\\\")\\\\', \\\\'__init__.py\\\\': \\\\'\\\\', \\\\'call_llm.py\\\\': \\\\'from google import genai\\\\\\\\\\\\\\\\nimport os\\\\\\\\\\\\\\\\nimport logging\\\\\\\\\\\\\\\\nimport json\\\\\\\\\\\\\\\\nfrom datetime import datetime\\\\\\\\\\\\\\\\nimport requests\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n# Configure logging\\\\\\\\\\\\\\\\nlog_directory = os.getenv(\\\\\\\\\\\"LOG_DIR\\\\\\\\\\\", \\\\\\\\\\\"logs\\\\\\\\\\\")\\\\\\\\\\\\\\\\nos.makedirs(log_directory, exist_ok=True)\\\\\\\\\\\\\\\\nlog_file = os.path.join(\\\\\\\\\\\\\\\\n    log_directory, f\\\\\\\\\\\"llm_calls_{datetime.now().strftime(\\\\\\\\\\\\\\\\\\\\'%Y%m%d\\\\\\\\\\\\\\\\\\\\')}.log\\\\\\\\\\\"\\\\\\\\\\\\\\\\n)\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n# Set up logger\\\\\\\\\\\\\\\\nlogger = logging.getLogger(\\\\\\\\\\\"llm_logger\\\\\\\\\\\")\\\\\\\\\\\\\\\\nlogger.setLevel(logging.INFO)\\\\\\\\\\\\\\\\nlogger.propagate = False  # Prevent propagation to root logger\\\\\\\\\\\\\\\\nfile_handler = logging.FileHandler(log_file)\\\\\\\\\\\\\\\\nfile_handler.setFormatter(\\\\\\\\\\\\\\\\n    logging.Formatter(\\\\\\\\\\\"%(asctime)s - %(levelname)s - %(message)s\\\\\\\\\\\")\\\\\\\\\\\\\\\\n)\\\\\\\\\\\\\\\\nlogger.addHandler(file_handler)\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n# Simple cache configuration\\\\\\\\\\\\\\\\ncache_file = \\\\\\\\\\\"llm_cache.json\\\\\\\\\\\"\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n# By default, we Google Gemini 2.5 pro, as it shows great performance for code understanding\\\\\\\\\\\\\\\\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\\\\\\\\\\\\\\\\n    # Log the prompt\\\\\\\\\\\\\\\\n    logger.info(f\\\\\\\\\\\"PROMPT: {prompt}\\\\\\\\\\\")\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    # Check cache if enabled\\\\\\\\\\\\\\\\n    if use_cache:\\\\\\\\\\\\\\\\n        # Load cache from disk\\\\\\\\\\\\\\\\n        cache = {}\\\\\\\\\\\\\\\\n        if os.path.exists(cache_file):\\\\\\\\\\\\\\\\n            try:\\\\\\\\\\\\\\\\n                with open(cache_file, \\\\\\\\\\\"r\\\\\\\\\\\") as f:\\\\\\\\\\\\\\\\n                    cache = json.load(f)\\\\\\\\\\\\\\\\n            except:\\\\\\\\\\\\\\\\n                logger.warning(f\\\\\\\\\\\"Failed to load cache, starting with empty cache\\\\\\\\\\\")\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n        # Return from cache if exists\\\\\\\\\\\\\\\\n        if prompt in cache:\\\\\\\\\\\\\\\\n            logger.info(f\\\\\\\\\\\"RESPONSE: {cache[prompt]}\\\\\\\\\\\")\\\\\\\\\\\\\\\\n            return cache[prompt]\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    # Call the LLM if not in cache or cache disabled\\\\\\\\\\\\\\\\n    client = genai.Client(\\\\\\\\\\\\\\\\n        vertexai=True,\\\\\\\\\\\\\\\\n        # TODO: change to your own project id and location\\\\\\\\\\\\\\\\n        project=os.getenv(\\\\\\\\\\\"GEMINI_PROJECT_ID\\\\\\\\\\\", \\\\\\\\\\\"ai-sre-dev-84b7\\\\\\\\\\\"),\\\\\\\\\\\\\\\\n        location=os.getenv(\\\\\\\\\\\"GEMINI_LOCATION\\\\\\\\\\\", \\\\\\\\\\\"us-central1\\\\\\\\\\\")\\\\\\\\\\\\\\\\n    )\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    model = os.getenv(\\\\\\\\\\\"GEMINI_MODEL\\\\\\\\\\\", \\\\\\\\\\\"gemini-2.0-flash-exp\\\\\\\\\\\")\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\n    response = client.models.generate_content(model=model, contents=[prompt])\\\\\\\\\\\\\\\\n    response_text = response.text\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    # Log the response\\\\\\\\\\\\\\\\n    logger.info(f\\\\\\\\\\\"RESPONSE: {response_text}\\\\\\\\\\\")\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    # Update cache if enabled\\\\\\\\\\\\\\\\n    if use_cache:\\\\\\\\\\\\\\\\n        # Load cache again to avoid overwrites\\\\\\\\\\\\\\\\n        cache = {}\\\\\\\\\\\\\\\\n        if os.path.exists(cache_file):\\\\\\\\\\\\\\\\n            try:\\\\\\\\\\\\\\\\n                with open(cache_file, \\\\\\\\\\\"r\\\\\\\\\\\") as f:\\\\\\\\\\\\\\\\n                    cache = json.load(f)\\\\\\\\\\\\\\\\n            except:\\\\\\\\\\\\\\\\n                pass\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n        # Add to cache and save\\\\\\\\\\\\\\\\n        cache[prompt] = response_text\\\\\\\\\\\\\\\\n        try:\\\\\\\\\\\\\\\\n            with open(cache_file, \\\\\\\\\\\"w\\\\\\\\\\\") as f:\\\\\\\\\\\\\\\\n                json.dump(cache, f)\\\\\\\\\\\\\\\\n        except Exception as e:\\\\\\\\\\\\\\\\n            logger.error(f\\\\\\\\\\\"Failed to save cache: {e}\\\\\\\\\\\")\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    return response_text\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nif __name__ == \\\\\\\\\\\"__main__\\\\\\\\\\\":\\\\\\\\\\\\\\\\n    test_prompt = \\\\\\\\\\\"Hello, how are you?\\\\\\\\\\\"\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    # First call - should hit the API\\\\\\\\\\\\\\\\n    print(\\\\\\\\\\\"Making call...\\\\\\\\\\\")\\\\\\\\\\\\\\\\n    response1 = call_llm(test_prompt, use_cache=False)\\\\\\\\\\\\\\\\n    print(f\\\\\\\\\\\"Response: {response1}\\\\\\\\\\\")\\\\\\\\\\\\\\\\n\\\\', \\\\'main.py\\\\': \\\\'import argparse\\\\\\\\\\\\\\\\nimport os\\\\\\\\\\\\\\\\nimport sys\\\\\\\\\\\\\\\\nimport typing\\\\\\\\\\\\\\\\nimport logging\\\\\\\\\\\\\\\\nfrom pocketflow import Node\\\\\\\\\\\\\\\\nfrom call_llm import call_llm\\\\\\\\\\\\\\\\nfrom settings import (\\\\\\\\\\\\\\\\n    DEFAULT_EXCLUDE_SUFFIXES,\\\\\\\\\\\\\\\\n    DEFAULT_INCLUDE_SUFFIXES,\\\\\\\\\\\\\\\\n    DEFAULT_EXCLUDE_DIRS,\\\\\\\\\\\\\\\\n)\\\\\\\\\\\\\\\\nfrom settings import parse_args\\\\\\\\\\\\\\\\nimport fetch_files\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nclass Summarise(Node):\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\\\\\\n    This class identifies abstractions in the code.\\\\\\\\\\\\\\\\n    \\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\"\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    def prep(self,shared):\\\\\\\\\\\\\\\\n        return file_contents[\\\\\\\\\\\"data\\\\\\\\\\\"]\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\n    def exec(self, text):\\\\\\\\\\\\\\\\n        prompt = f\\\\\\\\\\\"Summarize this text in 50 words: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n {text}\\\\\\\\\\\"\\\\\\\\\\\\\\\\n        return call_llm(prompt)\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\n    def post(self, shared, prep_res, exec_res):\\\\\\\\\\\\\\\\n        # Store the result in the shared settings\\\\\\\\\\\\\\\\n        file_contents[\\\\\\\\\\\"summaries\\\\\\\\\\\"] = exec_res\\\\\\\\\\\\\\\\n        return \\\\\\\\\\\"default\\\\\\\\\\\"\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\nfile_contents = {\\\\\\\\\\\\\\\\n\\\\\\\\\\\"data\\\\\\\\\\\": {},\\\\\\\\\\\\\\\\n\\\\\\\\\\\"summaries\\\\\\\\\\\": None\\\\\\\\\\\\\\\\n}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\ndef main() -> None:\\\\\\\\\\\\\\\\n    args = parse_args()\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    # create a dictionarty of settings to be used in the script\\\\\\\\\\\\\\\\n    shared_settings = {\\\\\\\\\\\\\\\\n        \\\\\\\\\\\"name\\\\\\\\\\\": args.name,\\\\\\\\\\\\\\\\n        \\\\\\\\\\\"directory\\\\\\\\\\\": args.directory,\\\\\\\\\\\\\\\\n        \\\\\\\\\\\"exclude_patterns\\\\\\\\\\\": DEFAULT_EXCLUDE_SUFFIXES,\\\\\\\\\\\\\\\\n        \\\\\\\\\\\"exclude_dirs\\\\\\\\\\\": DEFAULT_EXCLUDE_DIRS,\\\\\\\\\\\\\\\\n        \\\\\\\\\\\"include_suffixes\\\\\\\\\\\": DEFAULT_INCLUDE_SUFFIXES,\\\\\\\\\\\\\\\\n        \\\\\\\\\\\"additional_exclude_dirs\\\\\\\\\\\": args.exclude_dirs,\\\\\\\\\\\\\\\\n    }\\\\\\\\\\\\\\\\n    logging.debug(f\\\\\\\\\\\"Shared settings: {shared_settings}\\\\\\\\\\\")\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    # Get a list of files from the directory and its subdirectories and append to the shared settings\\\\\\\\\\\\\\\\n    shared_settings[\\\\\\\\\\\"files\\\\\\\\\\\"] = fetch_files.crawl_files(\\\\\\\\\\\\\\\\n        directory=shared_settings[\\\\\\\\\\\"directory\\\\\\\\\\\"],\\\\\\\\\\\\\\\\n        exclude_dirs=shared_settings[\\\\\\\\\\\"exclude_dirs\\\\\\\\\\\"].union(\\\\\\\\\\\\\\\\n            shared_settings[\\\\\\\\\\\"additional_exclude_dirs\\\\\\\\\\\"]\\\\\\\\\\\\\\\\n        ),\\\\\\\\\\\\\\\\n    )\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    for file in shared_settings[\\\\\\\\\\\"files\\\\\\\\\\\"]:\\\\\\\\\\\\\\\\n        with open(file, \\\\\\\\\\\"r\\\\\\\\\\\") as f:\\\\\\\\\\\\\\\\n            file_contents[\\\\\\\\\\\"data\\\\\\\\\\\"][file] = f.read()\\\\\\\\\\\\\\\\n    print(f\\\\\\\\\\\"File contents: {file_contents}\\\\\\\\\\\")\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\n    summarise_node = Summarise()\\\\\\\\\\\\\\\\n    summarise_node.run(file_contents[\\\\\\\\\\\"data\\\\\\\\\\\"])\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    print(f\\\\\\\\\\\"Summary: {file_contents[\\\\\\\\\\\\\\\\\\\\'summaries\\\\\\\\\\\\\\\\\\\\']}\\\\\\\\\\\")\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n    # print (shared_settings)\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nif __name__ == \\\\\\\\\\\"__main__\\\\\\\\\\\":\\\\\\\\\\\\\\\\n    main()\\\\\\\\\\\\\\\\n\\\\'}\\\": \\\"The code generates project documentation using AI. It defines default file types to include/exclude and directories to skip during scanning. Command-line arguments customize the repository name, scan directory, and excluded directories. It crawls files and utilizes a Gemini model via API calls to summarize the content, storing results in a cache.\\\"}'}\": \"This project generates documentation using AI. It crawls project files, excluding specified directories and file types. The code uses the Gemini model to summarize file contents, caching the results. Command-line arguments customize the project name, scan directory, and exclusion rules for processing files.\\n\"}\n```": "The Python code aims to automatically generate documentation for a software project using AI, specifically Google's Gemini model. Here's a breakdown of its logic:\n\n1.  **Configuration and Argument Parsing:**\n\n    *   The `settings.py` file defines default settings like file suffixes to include/exclude (`DEFAULT_INCLUDE_SUFFIXES`, `DEFAULT_EXCLUDE_SUFFIXES`) and directories to exclude from scanning (`DEFAULT_EXCLUDE_DIRS`).  These are defined as sets for efficient lookups.\n    *   The `parse_args()` function uses the `argparse` module to handle command-line arguments.  These arguments allow the user to specify the repository name (`-n`, `--name`), the top-level directory to scan (`-d`, `--directory`), and additional directories to exclude (`-e`, `--exclude_dirs`). The `os.getcwd()` function is used to default the directory to the current working directory if not specified via command line. The `nargs='*'` argument to `add_argument` lets users specify multiple arguments for `exclude_dirs`.\n    *   It retrieves the Gemini model name and project ID from environment variables (`GEMINI_MODEL`, `GEMINI_PROJECT_ID`), providing default values if these environment variables are not set.\n\n2.  **File Crawling (`fetch_files.py`):**\n\n    *   The `crawl_files()` function recursively scans a directory and its subdirectories for files.\n    *   It uses `pathlib.Path` for efficient path manipulation.\n    *   It filters files based on:\n        *   Inclusion: The file suffix must be in the `include_patterns` set.\n        *   Exclusion: The file suffix must *not* be in the `exclude_suffixes` set.\n        *   Directory Exclusion: No part of the file's relative path can be in the `exclude_dirs` set.  This prevents scanning of unwanted directories like `.git` or `venv`.  It uses `item.relative_to(directory).parts` to split the path into its components.\n    *   Error Handling: Contains a `try...except` block to catch `ValueError` exceptions that can occur when calculating relative paths (e.g., if the directory is not a subdirectory of the current working directory).\n    *   Logging:  Uses the `logging` module to log the number of files found and, at the debug level, the list of files.\n\n3.  **LLM Interaction (`call_llm.py`):**\n\n    *   The `call_llm()` function interacts with the Google Gemini API to generate text (in this case, summaries).\n    *   It implements a caching mechanism using a JSON file (`llm_cache.json`) to store prompts and their corresponding responses.  This speeds up the process if the same prompt is encountered again.\n    *   Logging: Logs both prompts and responses using the `logging` module.  The logs are stored in files named `llm_calls_YYYYMMDD.log` in the `logs` directory.\n    *   Cache Handling:  The caching logic attempts to load the cache from disk, uses it if the prompt is found, and saves the updated cache back to disk. It includes error handling to prevent crashes if the cache file is corrupted or cannot be saved.\n    *   API Call: Uses the `google.genai` library to make the API call to Gemini. It retrieves the project ID, location, and model name from environment variables.\n\n4.  **Main Logic (`main.py`):**\n\n    *   The `main()` function is the entry point of the program.\n    *   It calls `parse_args()` to retrieve command-line arguments.\n    *   It creates a `shared_settings` dictionary to store configuration data, including the arguments parsed, and the default include/exclude lists.\n    *   It calls `fetch_files.crawl_files()` to get a list of files to process. The excluded directories are a union of the default excluded directories and the directories specified in the command line arguments.\n    *   It iterates through the list of files. For each file:\n        *   It reads the file's contents into the `file_contents[\"data\"]` dictionary.\n    *   It creates an instance of the `Summarise` class (likely a part of the `pocketflow` framework, though the exact details are not clear from the snippet). This class is responsible for creating a prompt and calling the llm\n    *   It calls the `run` method of the `Summarise` node to summarise the extracted file content using an llm.\n    *   Finally, it prints the summary (stored in `file_contents['summaries']`).\n\n5. **Pocketflow Integration**\n\n   *   The code appears to be using a simple dataflow framework called \"pocketflow\". The nodes have a `prep`, `exec` and `post` method\n   *   The `Summarise` node is defined which will call the llm\n   *   The `prep` method creates the prompt and accesses the shared content by calling `shared_settings[\"data\"][\\\"settings.py\\\"]`\n   *   The `exec` method calls the llm with the prompt\n   *   The `post` method stores the summary in `shared_settings[\"summaries\"]`\n\n**Purpose:** The overall purpose of the code is to automate the generation of documentation for a project by scanning source files, filtering them based on specified criteria, and using an AI model to create summaries of their contents. The caching mechanism is used to speed up the process and reduce the number of API calls made to the language model.  The use of command-line arguments and environment variables provides flexibility in configuring the script's behavior.\n", "Summarize this text in 50 words: \n\n import typing\nimport argparse\nimport os\n\n\nDEFAULT_INCLUDE_SUFFIXES = {\n    \".js\",\n    \".json\",\n    \".jsx\",\n    \".md\",\n    \".py\",\n    \".pyi\",\n    \".pyx\",\n    \".rst\",\n    \".tf\",\n    \".tfvars\",\n    \".ts\",\n    \".tsx\",\n    \".yaml\",\n    \".yml\",\n    \"Dockerfile\",\n    \"Jenkinsfil\",\n}\n\nDEFAULT_EXCLUDE_SUFFIXES = {}\n\nDEFAULT_EXCLUDE_DIRS = {\n    \".git\",\n    \".github\",\n    \".terraform\",\n    \"assets\",\n    \"build\",\n    \"dist\",\n    \"env\",\n    \"docs\",\n    \"env\",\n    \"images\",\n    \"temp\",\n    \"tests\",\n    \"tmp\",\n    \"venv\",\n}\n\n\ndef parse_args() -> typing.Any:\n    \"\"\"\n    Parse command line arguments.\n    :return: Parsed arguments\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Generate documentation for a project using AI.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    parser.add_argument(\n        \"-n\",\n        \"--name\",\n        help=\"local Repository name\",\n        default=\"repo_name\",\n        )\n\n    parser.add_argument(\n        \"-d\",\n        \"--directory\",\n        help=\"top level directory to scan, default is current working directory\",\n        default=os.getcwd(),\n        )\n\n    parser.add_argument(\n        \"-e\",\n        \"--exclude_dirs\",\n        nargs='*',\n        help=\"comma separated list of directories to exclude\",\n        default=\",\"\n        )\n\n    return parser.parse_args()\n\nmodel = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\nproject=os.getenv(\"GEMINI_PROJECT_ID\", \"sre-ai-dev\")": "This code defines default file suffixes to include/exclude for documentation generation, along with directories to ignore.  It uses `argparse` to handle command-line arguments for repository name, target directory, and excluded directories. It also pulls the Gemini model and project ID from environment variables.\n", "Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\n\nCode:\n```python\nimport typing\nimport argparse\nimport os\n\n\nDEFAULT_INCLUDE_SUFFIXES = {\n    \".js\",\n    \".json\",\n    \".jsx\",\n    \".md\",\n    \".py\",\n    \".pyi\",\n    \".pyx\",\n    \".rst\",\n    \".tf\",\n    \".tfvars\",\n    \".ts\",\n    \".tsx\",\n    \".yaml\",\n    \".yml\",\n    \"Dockerfile\",\n    \"Jenkinsfil\",\n}\n\nDEFAULT_EXCLUDE_SUFFIXES = {}\n\nDEFAULT_EXCLUDE_DIRS = {\n    \".git\",\n    \".github\",\n    \".terraform\",\n    \"assets\",\n    \"build\",\n    \"dist\",\n    \"env\",\n    \"docs\",\n    \"env\",\n    \"images\",\n    \"temp\",\n    \"tests\",\n    \"tmp\",\n    \"venv\",\n}\n\n\ndef parse_args() -> typing.Any:\n    \"\"\"\n    Parse command line arguments.\n    :return: Parsed arguments\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Generate documentation for a project using AI.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    parser.add_argument(\n        \"-n\",\n        \"--name\",\n        help=\"local Repository name\",\n        default=\"repo_name\",\n        )\n\n    parser.add_argument(\n        \"-d\",\n        \"--directory\",\n        help=\"top level directory to scan, default is current working directory\",\n        default=os.getcwd(),\n        )\n\n    parser.add_argument(\n        \"-e\",\n        \"--exclude_dirs\",\n        nargs='*',\n        help=\"comma separated list of directories to exclude\",\n        default=\",\"\n        )\n\n    return parser.parse_args()\n\nmodel = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\nproject=os.getenv(\"GEMINI_PROJECT_ID\", \"sre-ai-dev\")\n```": "```python\nimport typing\nimport argparse\nimport os\n\n\nDEFAULT_INCLUDE_SUFFIXES = {\n    \".js\",\n    \".json\",\n    \".jsx\",\n    \".md\",\n    \".py\",\n    \".pyi\",\n    \".pyx\",\n    \".rst\",\n    \".tf\",\n    \".tfvars\",\n    \".ts\",\n    \".tsx\",\n    \".yaml\",\n    \".yml\",\n    \"Dockerfile\",\n    \"Jenkinsfil\",\n}\n\nDEFAULT_EXCLUDE_SUFFIXES = {}\n\nDEFAULT_EXCLUDE_DIRS = {\n    \".git\",\n    \".github\",\n    \".terraform\",\n    \"assets\",\n    \"build\",\n    \"dist\",\n    \"env\",\n    \"docs\",\n    \"env\",\n    \"images\",\n    \"temp\",\n    \"tests\",\n    \"tmp\",\n    \"venv\",\n}\n\n\ndef parse_args() -> typing.Any:\n    \"\"\"\n    Parse command line arguments.\n    :return: Parsed arguments\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Generate documentation for a project using AI.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    parser.add_argument(\n        \"-n\",\n        \"--name\",\n        help=\"local Repository name\",\n        default=\"repo_name\",\n        )\n\n    parser.add_argument(\n        \"-d\",\n        \"--directory\",\n        help=\"top level directory to scan, default is current working directory\",\n        default=os.getcwd(),\n        )\n\n    parser.add_argument(\n        \"-e\",\n        \"--exclude_dirs\",\n        nargs='*',\n        help=\"comma separated list of directories to exclude\",\n        default=\",\"\n        )\n\n    return parser.parse_args()\n\nmodel = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\nproject=os.getenv(\"GEMINI_PROJECT_ID\", \"sre-ai-dev\")\n```\n\n**Analysis of the Python Code:**\n\n**1. Imports:**\n\n*   `typing`:  Used for type hinting, making the code more readable and helping with static analysis.\n*   `argparse`:  A standard library module used for parsing command-line arguments. This allows users to specify options when running the script.\n*   `os`:  A standard library module that provides a way to interact with the operating system, such as getting the current working directory or accessing environment variables.\n\n**2. Constants:**\n\n*   `DEFAULT_INCLUDE_SUFFIXES`: A set containing file extensions that the script will likely process (e.g., `.js`, `.py`, `.md`). Sets are used for efficient membership checking.\n*   `DEFAULT_EXCLUDE_SUFFIXES`: An empty set, presumably meant to hold file extensions to exclude from processing. Currently, it excludes no file extensions.\n*   `DEFAULT_EXCLUDE_DIRS`:  A set of directory names that the script should skip during processing (e.g., `.git`, `venv`, `tests`).\n\n**3. `parse_args()` Function:**\n\n*   **Purpose:**  This function defines how the script accepts and interprets command-line arguments.  It creates an `ArgumentParser` object, defines the arguments, and then parses them.\n*   **Logic:**\n    *   `parser = argparse.ArgumentParser(...)`: Initializes an `ArgumentParser` object with a description of the script and sets the formatter to show default values in the help message.\n    *   `parser.add_argument(...)`:  Adds three command-line arguments:\n        *   `-n` or `--name`:  Allows the user to specify a \"repo name\". It has a default value of \"repo\\_name\".\n        *   `-d` or `--directory`:  Allows the user to specify the top-level directory to scan. It defaults to the current working directory obtained using `os.getcwd()`.\n        *   `-e` or `--exclude_dirs`: Allows the user to provide a list of comma-separated directories to exclude.  It defaults to a string containing only a comma. The `nargs='*'` part means that this argument can accept zero or more values, which are collected into a list.\n    *   `return parser.parse_args()`:  Parses the command-line arguments provided by the user when the script is run and returns an object containing the parsed values.\n\n**4. Environment Variables:**\n\n*   `model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")`: Retrieves the value of the environment variable `GEMINI_MODEL`. If this environment variable is not set, it defaults to the string `\"gemini-2.5-pro-exp-03-25\"`. This suggests the script interfaces with a Gemini AI model.\n*   `project=os.getenv(\"GEMINI_PROJECT_ID\", \"sre-ai-dev\")`: Retrieves the value of the environment variable `GEMINI_PROJECT_ID`. If not set, it defaults to `\"sre-ai-dev\"`. This likely represents a project ID for the Gemini AI service.\n\n**Overall Purpose of the Code (Inferred):**\n\nThis code snippet appears to be the initial setup and configuration part of a script designed to generate documentation for a project using AI, specifically Google's Gemini model.\n\n*   It takes user input for the project name, directory to scan, and directories to exclude.\n*   It defines default settings for which file types to include and directory names to exclude during processing.\n*   It configures the script to connect to the Gemini AI service using environment variables for the model and project ID.\n\nThe code sets the stage for a larger process that likely involves:\n\n1.  Scanning the specified directory (and its subdirectories).\n2.  Filtering files based on `DEFAULT_INCLUDE_SUFFIXES` and `DEFAULT_EXCLUDE_SUFFIXES`.\n3.  Skipping directories listed in `DEFAULT_EXCLUDE_DIRS` (and potentially those passed via the `--exclude_dirs` argument).\n4.  Extracting content from the files.\n5.  Using the Gemini AI model (defined by `GEMINI_MODEL` and `GEMINI_PROJECT_ID`) to generate documentation based on the extracted content.\n", "Summarize this text in 50 words: \n\n # A script to crawl files from a given directory and its subdirectories.\n# It uses the `os` module to walk through the directory structure and collects file paths based on specified include and exclude patterns.\n# files that are not in the exclude patterns and are in the include patterns are collected.\nimport os\nimport typing\nimport logging\nfrom pathlib import Path\nfrom settings import DEFAULT_EXCLUDE_SUFFIXES, DEFAULT_INCLUDE_SUFFIXES, DEFAULT_EXCLUDE_DIRS\n\n\n\ndef crawl_files(\n    directory: str,\n    include_patterns: typing.Set[str] = DEFAULT_INCLUDE_SUFFIXES,\n    exclude_suffixes: typing.Set[str] = DEFAULT_EXCLUDE_SUFFIXES,\n    exclude_dirs: typing.Set[str] = DEFAULT_EXCLUDE_DIRS,\n    ) -> typing.List[str]:\n    try:\n        files = {\n            str(item.relative_to(directory))  # Store the relative path string\n            for item in Path(directory).rglob(\"*\") # Iterate through all items recursively\n            if item.is_file() # Consider only files\n            if item.suffix in include_patterns # Check 0: Suffix is included\n            and item.suffix not in exclude_suffixes # Check 1: Suffix is not excluded\n            and not any(part in exclude_dirs for part in item.relative_to(directory).parts) # Check 2: No path part is excluded\n        }\n\n    except ValueError as e:\n        logging.error(f\"Error calculating relative paths: {e}\")\n        logging.error(f\"Ensure the search directory '{directory.resolve()}' is within or is the \"\n            f\"current working directory '{directory.resolve()}'\")\n\n    logging.info(f\"Found {len(files)} files in {directory}\")\n    logging.debug(f\"Files found: {files}\")\n\n    return files\n\n\ndef main() -> None:\n    print(\"Crawling files...\")\n    directory = os.getcwd()  \n    print(f\"Files crawled from {directory}:\")\n    print (crawl_files(directory))\n\nif __name__ == \"__main__\":\n    main()": "This Python script crawls a directory and its subdirectories to find files. It uses `os` and `pathlib` to recursively search, filtering files based on include and exclude patterns for suffixes and directories. It returns a list of relative file paths, logging errors and debugging information along the way.\n", "Analyze the following Python code and describe its main algorithm or logic flow. Focus on the sequence of operations, control structures (loops, conditionals), function calls, and data transformations. Explain the purpose of the code in terms of its logic.\n\nCode:\n```python\n# A script to crawl files from a given directory and its subdirectories.\n# It uses the `os` module to walk through the directory structure and collects file paths based on specified include and exclude patterns.\n# files that are not in the exclude patterns and are in the include patterns are collected.\nimport os\nimport typing\nimport logging\nfrom pathlib import Path\nfrom settings import DEFAULT_EXCLUDE_SUFFIXES, DEFAULT_INCLUDE_SUFFIXES, DEFAULT_EXCLUDE_DIRS\n\n\n\ndef crawl_files(\n    directory: str,\n    include_patterns: typing.Set[str] = DEFAULT_INCLUDE_SUFFIXES,\n    exclude_suffixes: typing.Set[str] = DEFAULT_EXCLUDE_SUFFIXES,\n    exclude_dirs: typing.Set[str] = DEFAULT_EXCLUDE_DIRS,\n    ) -> typing.List[str]:\n    try:\n        files = {\n            str(item.relative_to(directory))  # Store the relative path string\n            for item in Path(directory).rglob(\"*\") # Iterate through all items recursively\n            if item.is_file() # Consider only files\n            if item.suffix in include_patterns # Check 0: Suffix is included\n            and item.suffix not in exclude_suffixes # Check 1: Suffix is not excluded\n            and not any(part in exclude_dirs for part in item.relative_to(directory).parts) # Check 2: No path part is excluded\n        }\n\n    except ValueError as e:\n        logging.error(f\"Error calculating relative paths: {e}\")\n        logging.error(f\"Ensure the search directory '{directory.resolve()}' is within or is the \"\n            f\"current working directory '{directory.resolve()}'\")\n\n    logging.info(f\"Found {len(files)} files in {directory}\")\n    logging.debug(f\"Files found: {files}\")\n\n    return files\n\n\ndef main() -> None:\n    print(\"Crawling files...\")\n    directory = os.getcwd()  \n    print(f\"Files crawled from {directory}:\")\n    print (crawl_files(directory))\n\nif __name__ == \"__main__\":\n    main()\n```": "```python\n# A script to crawl files from a given directory and its subdirectories.\n# It uses the `os` module to walk through the directory structure and collects file paths based on specified include and exclude patterns.\n# files that are not in the exclude patterns and are in the include patterns are collected.\nimport os\nimport typing\nimport logging\nfrom pathlib import Path\nfrom settings import DEFAULT_EXCLUDE_SUFFIXES, DEFAULT_INCLUDE_SUFFIXES, DEFAULT_EXCLUDE_DIRS\n\n\n\ndef crawl_files(\n    directory: str,\n    include_patterns: typing.Set[str] = DEFAULT_INCLUDE_SUFFIXES,\n    exclude_suffixes: typing.Set[str] = DEFAULT_EXCLUDE_SUFFIXES,\n    exclude_dirs: typing.Set[str] = DEFAULT_EXCLUDE_DIRS,\n    ) -> typing.List[str]:\n    \"\"\"\n    Crawls files from a given directory and its subdirectories based on include/exclude patterns.\n\n    Args:\n        directory (str): The root directory to start crawling from.\n        include_patterns (typing.Set[str], optional): A set of file suffixes to include. Defaults to DEFAULT_INCLUDE_SUFFIXES.\n        exclude_suffixes (typing.Set[str], optional): A set of file suffixes to exclude. Defaults to DEFAULT_EXCLUDE_SUFFIXES.\n        exclude_dirs (typing.Set[str], optional): A set of directory names to exclude. Defaults to DEFAULT_EXCLUDE_DIRS.\n\n    Returns:\n        typing.List[str]: A list of file paths (relative to the input directory) that meet the inclusion/exclusion criteria.\n    \"\"\"\n    try:\n        files = {\n            str(item.relative_to(directory))  # Store the relative path string\n            for item in Path(directory).rglob(\"*\") # Iterate through all items recursively\n            if item.is_file() # Consider only files\n            if item.suffix in include_patterns # Check 0: Suffix is included\n            and item.suffix not in exclude_suffixes # Check 1: Suffix is not excluded\n            and not any(part in exclude_dirs for part in item.relative_to(directory).parts) # Check 2: No path part is excluded\n        }\n\n    except ValueError as e:\n        logging.error(f\"Error calculating relative paths: {e}\")\n        logging.error(f\"Ensure the search directory '{directory.resolve()}' is within or is the \"\n            f\"current working directory '{directory.resolve()}'\")\n\n    logging.info(f\"Found {len(files)} files in {directory}\")\n    logging.debug(f\"Files found: {files}\")\n\n    return files\n\n\ndef main() -> None:\n    \"\"\"\n    Main function to execute the file crawling process.\n    \"\"\"\n    print(\"Crawling files...\")\n    directory = os.getcwd()  \n    print(f\"Files crawled from {directory}:\")\n    print (crawl_files(directory))\n\nif __name__ == \"__main__\":\n    main()\n```\n\n**Algorithm and Logic Flow:**\n\n1.  **`crawl_files` Function:**\n    *   **Input:**\n        *   `directory` (str):  The starting directory for the crawl.\n        *   `include_patterns` (typing.Set\\[str]): A set of file extensions (suffixes) that should be included in the results. Defaults to `DEFAULT_INCLUDE_SUFFIXES` (defined in `settings.py`).\n        *   `exclude_suffixes` (typing.Set\\[str]): A set of file extensions (suffixes) that should be excluded from the results. Defaults to `DEFAULT_EXCLUDE_SUFFIXES` (defined in `settings.py`).\n        *   `exclude_dirs` (typing.Set\\[str]): A set of directory names that should be excluded from the results. Defaults to `DEFAULT_EXCLUDE_DIRS` (defined in `settings.py`).\n    *   **Recursive Globbing:**\n        *   `Path(directory).rglob(\"*\")`: This uses the `pathlib` module to recursively find all files and directories under the given `directory`.  `rglob(\"*\")` is equivalent to `os.walk` but returns `Path` objects.\n    *   **Filtering (List Comprehension with Conditions):** The core logic resides within a set comprehension that iterates through the results of `rglob(\"*\")` and applies several filters:\n        *   `item.is_file()`:  Ensures that only files (not directories) are considered.\n        *   `item.suffix in include_patterns`: Checks if the file's extension (suffix) is present in the `include_patterns` set.\n        *   `item.suffix not in exclude_suffixes`: Checks if the file's extension (suffix) is *not* present in the `exclude_suffixes` set.\n        *   `not any(part in exclude_dirs for part in item.relative_to(directory).parts)`: This is the most complex condition. It checks if any part of the relative path of the file (relative to the starting directory) is present in the `exclude_dirs` set. `item.relative_to(directory).parts` splits the relative path into its directory components.  `any(...)` efficiently checks if *any* of these components are in the `exclude_dirs`.  If *any* part of the path is in the `exclude_dirs`, the entire file is excluded.\n    *   **Relative Path Transformation:**\n        *   `str(item.relative_to(directory))`: If a file passes all the filter conditions, its path is converted to a relative path with respect to the initial `directory` and then converted to a string. This string representation of the relative path is added to the `files` set.\n    *   **Error Handling:**\n        *   A `try...except` block handles potential `ValueError` exceptions that can occur during the `relative_to` operation if the path isn't within the base directory.  If a `ValueError` occurs, an error message is logged.\n    *   **Logging:**\n        *   The function logs the number of files found at the INFO level.\n        *   The function logs the list of found files at the DEBUG level.\n    *   **Output:**\n        *   The function returns a `typing.List[str]` containing the relative file paths that satisfied all the conditions.\n\n2.  **`main` Function:**\n    *   Prints \"Crawling files...\" to the console.\n    *   Gets the current working directory using `os.getcwd()`.\n    *   Prints the directory being crawled.\n    *   Calls the `crawl_files` function, passing in the current working directory.\n    *   Prints the list of files returned by `crawl_files`.\n\n3.  **`if __name__ == \"__main__\":` Block:**\n    *   Ensures that the `main` function is only executed when the script is run directly (not when it's imported as a module).\n\n**Purpose of the Code:**\n\nThe code implements a flexible file crawler.  Its primary purpose is to efficiently find files within a directory structure that match specific inclusion and exclusion criteria based on file extensions and directory names. It uses the `pathlib` module for efficient path manipulation and recursive file searching. The use of sets for `include_patterns`, `exclude_suffixes`, and `exclude_dirs` makes the filtering operations efficient.  The code provides a structured way to manage file discovery within a project, enabling developers to easily locate relevant files while excluding irrelevant ones.  The use of relative paths makes the results portable and independent of the absolute location where the script is run.\n"}